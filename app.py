#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Video Style Analysis - Gradio Web Interface
SOTA Models: CLIP | CLAP | HuBERT | Whisper | YOLO11
"""

import sys
import json
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, Dict, Any, List

import gradio as gr
import numpy as np
import cv2

sys.path.insert(0, str(Path(__file__).parent / "src"))

from steps import (
    VisualAnalysisStep, AudioAnalysisStep, ASRAnalysisStep,
    YOLOAnalysisStep, ConsensusStep,
    VideoInput, AudioInput, ASRInput, YOLOInput, ConsensusInput,
    VideoMetrics, VisualOutput, AudioOutput, ASROutput, YOLOOutput, ConsensusOutput,
)
from report_word import generate_word_report

# =============================================================================
# Internationalization (i18n)
# =============================================================================
TRANSLATIONS = {
    "en": {
        # Header
        "title": "üé¨ Video Style Analysis",
        "subtitle": "SOTA 2025/2026 | PyTorch + HuggingFace",
        "models": "CLIP (Scene) ¬∑ CLAP (Audio) ¬∑ HuBERT (Emotion) ¬∑ Whisper (ASR) ¬∑ YOLO11 (Detection)",
        
        # Sections
        "upload_section": "üì§ Upload",
        "settings_section": "‚öôÔ∏è Settings",
        "preview_section": "üé¨ Preview",
        "control_section": "üöÄ Analysis",
        "results_section": "üìä Results",
        "export_section": "üì• Export",
        
        # Upload
        "select_video": "Select Video (mp4, avi, mov, mkv)",
        "status": "Status",
        "asr_language": "ASR Language",
        "audio_preview": "Extracted Audio",
        "keyframes": "Key Frames",
        
        # Buttons
        "analyze_all": "üéØ Analyze All",
        "btn_visual": "üìπ Visual",
        "btn_audio": "üéµ Audio", 
        "btn_asr": "üé§ ASR",
        "btn_yolo": "üîç YOLO",
        "btn_consensus": "üìä Summary",
        "gen_report": "üìÑ Generate Report",
        "export_json": "üíæ Export JSON",
        
        # Tabs
        "tab_visual": "üìπ Visual",
        "tab_audio": "üéµ Audio",
        "tab_asr": "üé§ ASR",
        "tab_yolo": "üîç YOLO",
        "tab_summary": "üìä Summary",
        
        # Export
        "report_status": "Report Status",
        "word_report": "Word Report (.docx)",
        "pdf_report": "PDF Report (.pdf)",
        "json_data": "JSON Data",
        "json_status": "JSON Status",
        "quick_summary": "Quick Summary",
        
        # Messages
        "upload_first": "Please upload a video first",
        "run_analysis_first": "Please run visual or audio analysis first",
        "uploaded": "‚úÖ Uploaded",
        "workdir": "Work Directory",
        "frames_extracted": "Extracted {n} frames",
        "audio_extracted": "‚úÖ Audio extracted",
        "audio_failed": "‚ö†Ô∏è Audio extraction failed",
        "analysis_failed": "Analysis failed",
        "report_generated": "‚úÖ Report generated",
        "pdf_needs_libreoffice": "‚ö†Ô∏è PDF conversion requires LibreOffice",
        "json_exported": "‚úÖ JSON exported",
        
        # Progress
        "loading_clip": "‚è≥ Loading CLIP model...",
        "analyzing_visual": "üîÑ Visual analysis...",
        "loading_clap": "‚è≥ Loading CLAP model...",
        "analyzing_audio": "üîÑ Audio analysis...",
        "loading_whisper": "‚è≥ Loading Whisper model...",
        "analyzing_asr": "üîÑ Speech recognition...",
        "loading_yolo": "‚è≥ Loading YOLO11 model...",
        "analyzing_yolo": "üîÑ Object detection...",
        "calculating_consensus": "üîÑ Calculating summary...",
        "generating_word": "üìÑ Generating Word report...",
        "converting_pdf": "üìï Converting to PDF...",
        "done": "‚úÖ Done",
        
        # Results - Visual
        "visual_results": "Visual Analysis Results",
        "basic_info": "Basic Info",
        "duration": "Duration",
        "fps": "FPS",
        "sampled": "Sampled",
        "frames": "frames",
        "camera": "Camera",
        "angle": "Angle",
        "focal": "Focal",
        "color": "Color",
        "hue": "Hue",
        "saturation": "Saturation",
        "brightness": "Brightness",
        "contrast": "Contrast",
        "cct": "CCT",
        "editing": "Editing",
        "cuts": "Cuts",
        "avg_shot": "Avg Shot",
        "transition": "Transition",
        "scene_clip": "Scene (CLIP)",
        
        # Results - Audio
        "audio_results": "Audio Analysis Results (CLAP)",
        "rhythm": "Rhythm",
        "bpm": "BPM",
        "beats": "Beats",
        "percussive": "Percussive",
        "bgm_style": "BGM Style",
        "mood": "Mood",
        "key": "Key",
        "speech_ratio": "Speech Ratio",
        "instruments": "Instruments",
        
        # Results - ASR
        "asr_results": "Speech Analysis Results (Whisper + HuBERT)",
        "statistics": "Statistics",
        "words": "Words",
        "wpm": "WPM",
        "pace": "Pace",
        "catchphrases": "Catchphrases",
        "prosody": "Prosody",
        "pitch": "Pitch",
        "style": "Style",
        "emotion": "Emotion",
        "transcript": "Transcript",
        
        # Results - YOLO
        "yolo_results": "Object Detection Results (YOLO11)",
        "environment": "Environment",
        "env_type": "Type",
        "cook_style": "Style",
        "detection_stats": "Detection Stats",
        "unique_objects": "Unique Objects",
        "total_detections": "Total Detections",
        "detected_objects": "Detected Objects",
        "confidence": "confidence",
        
        # Results - Summary
        "summary_results": "Cross-Video Summary",
        "na": "N/A",
        
        # Footer
        "footer": "Video Style Analysis | SOTA 2025/2026 | PyTorch + HuggingFace",
    },
    "zh": {
        # Header
        "title": "üé¨ ËßÜÈ¢ëÈ£éÊ†ºÂàÜÊûêÁ≥ªÁªü",
        "subtitle": "SOTA 2025/2026 | PyTorch + HuggingFace",
        "models": "CLIP (Âú∫ÊôØ) ¬∑ CLAP (Èü≥È¢ë) ¬∑ HuBERT (ÊÉÖÊÑü) ¬∑ Whisper (ËØ≠Èü≥) ¬∑ YOLO11 (Ê£ÄÊµã)",
        
        # Sections
        "upload_section": "üì§ ‰∏ä‰º†",
        "settings_section": "‚öôÔ∏è ËÆæÁΩÆ",
        "preview_section": "üé¨ È¢ÑËßà",
        "control_section": "üöÄ ÂàÜÊûê",
        "results_section": "üìä ÁªìÊûú",
        "export_section": "üì• ÂØºÂá∫",
        
        # Upload
        "select_video": "ÈÄâÊã©ËßÜÈ¢ë (mp4, avi, mov, mkv)",
        "status": "Áä∂ÊÄÅ",
        "asr_language": "ËØ≠Èü≥ËØÜÂà´ËØ≠Ë®Ä",
        "audio_preview": "ÊèêÂèñÁöÑÈü≥È¢ë",
        "keyframes": "ÂÖ≥ÈîÆÂ∏ß",
        
        # Buttons
        "analyze_all": "üéØ ‰∏ÄÈîÆÂàÜÊûêÂÖ®ÈÉ®",
        "btn_visual": "üìπ ËßÜËßâ",
        "btn_audio": "üéµ Èü≥È¢ë",
        "btn_asr": "üé§ ËØ≠Èü≥",
        "btn_yolo": "üîç Ê£ÄÊµã",
        "btn_consensus": "üìä Ê±áÊÄª",
        "gen_report": "üìÑ ÁîüÊàêÊä•Âëä",
        "export_json": "üíæ ÂØºÂá∫ JSON",
        
        # Tabs
        "tab_visual": "üìπ ËßÜËßâ",
        "tab_audio": "üéµ Èü≥È¢ë",
        "tab_asr": "üé§ ËØ≠Èü≥",
        "tab_yolo": "üîç Ê£ÄÊµã",
        "tab_summary": "üìä Ê±áÊÄª",
        
        # Export
        "report_status": "Êä•ÂëäÁä∂ÊÄÅ",
        "word_report": "Word Êä•Âëä (.docx)",
        "pdf_report": "PDF Êä•Âëä (.pdf)",
        "json_data": "JSON Êï∞ÊçÆ",
        "json_status": "JSON Áä∂ÊÄÅ",
        "quick_summary": "Âø´ÈÄüÊëòË¶Å",
        
        # Messages
        "upload_first": "ËØ∑ÂÖà‰∏ä‰º†ËßÜÈ¢ë",
        "run_analysis_first": "ËØ∑ÂÖàËøêË°åËßÜËßâÊàñÈü≥È¢ëÂàÜÊûê",
        "uploaded": "‚úÖ Â∑≤‰∏ä‰º†",
        "workdir": "Â∑•‰ΩúÁõÆÂΩï",
        "frames_extracted": "Â∑≤ÊèêÂèñ {n} Â∏ß",
        "audio_extracted": "‚úÖ Èü≥È¢ëÂ∑≤ÊèêÂèñ",
        "audio_failed": "‚ö†Ô∏è Èü≥È¢ëÊèêÂèñÂ§±Ë¥•",
        "analysis_failed": "ÂàÜÊûêÂ§±Ë¥•",
        "report_generated": "‚úÖ Êä•ÂëäÂ∑≤ÁîüÊàê",
        "pdf_needs_libreoffice": "‚ö†Ô∏è PDF ËΩ¨Êç¢ÈúÄË¶Å LibreOffice",
        "json_exported": "‚úÖ JSON Â∑≤ÂØºÂá∫",
        
        # Progress
        "loading_clip": "‚è≥ Âä†ËΩΩ CLIP Ê®°Âûã...",
        "analyzing_visual": "üîÑ ËßÜËßâÂàÜÊûê‰∏≠...",
        "loading_clap": "‚è≥ Âä†ËΩΩ CLAP Ê®°Âûã...",
        "analyzing_audio": "üîÑ Èü≥È¢ëÂàÜÊûê‰∏≠...",
        "loading_whisper": "‚è≥ Âä†ËΩΩ Whisper Ê®°Âûã...",
        "analyzing_asr": "üîÑ ËØ≠Èü≥ËØÜÂà´‰∏≠...",
        "loading_yolo": "‚è≥ Âä†ËΩΩ YOLO11 Ê®°Âûã...",
        "analyzing_yolo": "üîÑ ÁõÆÊ†áÊ£ÄÊµã‰∏≠...",
        "calculating_consensus": "üîÑ ËÆ°ÁÆóÊ±áÊÄª...",
        "generating_word": "üìÑ ÁîüÊàê Word Êä•Âëä...",
        "converting_pdf": "üìï ËΩ¨Êç¢‰∏∫ PDF...",
        "done": "‚úÖ ÂÆåÊàê",
        
        # Results - Visual
        "visual_results": "ËßÜËßâÂàÜÊûêÁªìÊûú",
        "basic_info": "Âü∫Êú¨‰ø°ÊÅØ",
        "duration": "Êó∂Èïø",
        "fps": "Â∏ßÁéá",
        "sampled": "ÈááÊ†∑",
        "frames": "Â∏ß",
        "camera": "ÈïúÂ§¥",
        "angle": "ËßíÂ∫¶",
        "focal": "ÁÑ¶Ë∑ù",
        "color": "Ëâ≤ÂΩ©",
        "hue": "Ëâ≤Ë∞É",
        "saturation": "È•±ÂíåÂ∫¶",
        "brightness": "‰∫ÆÂ∫¶",
        "contrast": "ÂØπÊØîÂ∫¶",
        "cct": "Ëâ≤Ê∏©",
        "editing": "Ââ™Ëæë",
        "cuts": "Ââ™ËæëÊ¨°Êï∞",
        "avg_shot": "Âπ≥ÂùáÈïúÂ§¥",
        "transition": "ËΩ¨Âú∫",
        "scene_clip": "Âú∫ÊôØ (CLIP)",
        
        # Results - Audio
        "audio_results": "Èü≥È¢ëÂàÜÊûêÁªìÊûú (CLAP)",
        "rhythm": "ËäÇÂ•è",
        "bpm": "BPM",
        "beats": "ËäÇÊãçÊï∞",
        "percussive": "ÊâìÂáª‰πêÊØî‰æã",
        "bgm_style": "BGM È£éÊ†º",
        "mood": "ÊÉÖÁª™",
        "key": "Ë∞ÉÂºè",
        "speech_ratio": "ËØ≠Èü≥ÊØî‰æã",
        "instruments": "‰πêÂô®",
        
        # Results - ASR
        "asr_results": "ËØ≠Èü≥ÂàÜÊûêÁªìÊûú (Whisper + HuBERT)",
        "statistics": "ÁªüËÆ°",
        "words": "ËØçÊï∞",
        "wpm": "ËØ≠ÈÄü",
        "pace": "ËäÇÂ•è",
        "catchphrases": "Âè£Â§¥Á¶Ö",
        "prosody": "ÈüµÂæã",
        "pitch": "Èü≥È´ò",
        "style": "È£éÊ†º",
        "emotion": "ÊÉÖÊÑü",
        "transcript": "ËΩ¨ÂΩïÊñáÊú¨",
        
        # Results - YOLO
        "yolo_results": "ÁõÆÊ†áÊ£ÄÊµãÁªìÊûú (YOLO11)",
        "environment": "ÁéØÂ¢É",
        "env_type": "Á±ªÂûã",
        "cook_style": "È£éÊ†º",
        "detection_stats": "Ê£ÄÊµãÁªüËÆ°",
        "unique_objects": "Áâ©‰ΩìÁßçÁ±ª",
        "total_detections": "Ê£ÄÊµãÊÄªÊï∞",
        "detected_objects": "Ê£ÄÊµãÂà∞ÁöÑÁâ©‰Ωì",
        "confidence": "ÁΩÆ‰ø°Â∫¶",
        
        # Results - Summary
        "summary_results": "Ë∑®ËßÜÈ¢ëÊ±áÊÄª",
        "na": "N/A",
        
        # Footer
        "footer": "ËßÜÈ¢ëÈ£éÊ†ºÂàÜÊûê | SOTA 2025/2026 | PyTorch + HuggingFace",
    }
}

# Current language state
LANG = "en"

def t(key: str) -> str:
    """Get translated string"""
    return TRANSLATIONS.get(LANG, TRANSLATIONS["en"]).get(key, key)

def set_language(lang: str):
    """Set current language"""
    global LANG
    LANG = lang if lang in TRANSLATIONS else "en"


# =============================================================================
# Global State
# =============================================================================
class AnalysisState:
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.video_path: Optional[Path] = None
        self.audio_path: Optional[Path] = None
        self.work_dir: Optional[Path] = None
        self.visual_output: Optional[VisualOutput] = None
        self.audio_output: Optional[AudioOutput] = None
        self.asr_output: Optional[ASROutput] = None
        self.yolo_output: Optional[YOLOOutput] = None
        self.consensus_output: Optional[ConsensusOutput] = None
        self.report_path: Optional[str] = None
        self.pdf_path: Optional[str] = None

STATE = AnalysisState()


# =============================================================================
# Utility Functions
# =============================================================================
def extract_audio_from_video(video_path: Path, output_dir: Path) -> Optional[Path]:
    output_path = output_dir / f"{video_path.stem}_audio.wav"
    if output_path.exists():
        return output_path
    try:
        cmd = ["ffmpeg", "-y", "-i", str(video_path), "-vn", "-acodec", "pcm_s16le",
               "-ar", "22050", "-ac", "1", str(output_path)]
        subprocess.run(cmd, capture_output=True, check=True)
        return output_path
    except Exception:
        return None


def extract_frames_for_gallery(video_path: Path, output_dir: Path, num_frames: int = 12) -> List[str]:
    frames_dir = output_dir / "frames"
    frames_dir.mkdir(exist_ok=True)
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    if total_frames == 0:
        cap.release()
        return []
    
    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    frame_paths = []
    
    for i, idx in enumerate(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            timestamp = idx / fps if fps > 0 else 0
            frame_path = frames_dir / f"frame_{i:03d}_{timestamp:.1f}s.jpg"
            cv2.imwrite(str(frame_path), frame)
            frame_paths.append(str(frame_path))
    
    cap.release()
    return frame_paths


def convert_docx_to_pdf(docx_path: str) -> Optional[str]:
    pdf_path = docx_path.replace('.docx', '.pdf')
    try:
        cmd = ["libreoffice", "--headless", "--convert-to", "pdf",
               "--outdir", str(Path(docx_path).parent), docx_path]
        subprocess.run(cmd, capture_output=True, check=True, timeout=60)
        if Path(pdf_path).exists():
            return pdf_path
    except Exception:
        pass
    return None


# =============================================================================
# Result Formatters
# =============================================================================
def format_visual(output: VisualOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    scenes = "\n".join([
        f"  ‚Ä¢ {s.get('label', '?')}: **{s.get('probability', 0):.1%}**" 
        for s in output.scene_categories[:3]
    ])
    
    return f"""## üìπ {t('visual_results')}

### üìä {t('basic_info')}
| {t('duration')} | {t('fps')} | {t('sampled')} |
|:---:|:---:|:---:|
| **{output.duration:.2f}s** | **{output.fps:.1f}** | **{output.sampled_frames}** {t('frames')} |

### üì∑ {t('camera')}
| {t('angle')} | {t('focal')} |
|:---:|:---:|
| **{output.camera_angle}** | **{output.focal_length_tendency}** |

### üé® {t('color')}
| {t('hue')} | {t('saturation')} | {t('brightness')} | {t('contrast')} |
|:---:|:---:|:---:|:---:|
| **{output.hue_family}** | **{output.saturation_band}** | **{output.brightness_band}** | **{output.contrast}** |

{t('cct')}: **{output.cct_mean:.0f}K** | {t('cuts')}: **{output.cuts}** | {t('avg_shot')}: **{output.avg_shot_length:.2f}s** | {t('transition')}: **{output.transition_type}**

### üè† {t('scene_clip')}
{scenes}
"""


def format_audio(output: AudioOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    instruments = output.instruments.get('detected_instruments', [])
    inst_str = ", ".join(instruments) if instruments else t('na')
    
    return f"""## üéµ {t('audio_results')}

### ü•Å {t('rhythm')}
| {t('bpm')} | {t('beats')} | {t('percussive')} |
|:---:|:---:|:---:|
| **{output.tempo_bpm:.1f}** | **{output.num_beats}** | **{output.percussive_ratio:.2f}** |

### üé∏ {t('bgm_style')}
**{output.bgm_style}** ({output.bgm_style_confidence:.1%})

### üòä {t('mood')}
**{output.mood}** ({output.mood_confidence:.1%})

### üéπ {t('key')}: **{output.key_signature}** | {t('speech_ratio')}: **{output.speech_ratio:.2f}**

### üé∫ {t('instruments')}
{inst_str}
"""


def format_asr(output: ASROutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    text_preview = output.text[:500] + '...' if len(output.text) > 500 else output.text
    
    emotion_str = ""
    if output.emotion:
        emotion_str = f"\n### üòä {t('emotion')}\n**{output.emotion.get('dominant_emotion', t('na'))}** ({output.emotion.get('confidence', 0):.1%})"
    
    prosody_str = ""
    if output.prosody:
        prosody_str = f"\n### üéº {t('prosody')}\n{t('pitch')}: **{output.prosody.get('mean_pitch_hz', 0):.1f}Hz** | {t('style')}: **{output.prosody.get('prosody_style', t('na'))}**"
    
    catchphrases_str = ""
    if output.catchphrases:
        catchphrases_str = f"\n### üîÅ {t('catchphrases')}\n" + " ¬∑ ".join([f'"{p}"' for p in output.catchphrases[:5]])
    
    return f"""## üé§ {t('asr_results')}

### üìä {t('statistics')}
| {t('words')} | {t('wpm')} | {t('pace')} |
|:---:|:---:|:---:|
| **{output.num_words}** | **{output.words_per_minute:.1f}** | **{output.pace}** |
{catchphrases_str}{prosody_str}{emotion_str}

### üìú {t('transcript')}
```
{text_preview}
```
"""


def format_yolo(output: YOLOOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    detection = output.detection
    environment = output.environment
    object_counts = detection.get('object_counts', {})
    avg_conf = detection.get('avg_confidence', {})
    
    objects_str = "\n".join([
        f"| {obj} | {cnt} | {avg_conf.get(obj, 0):.1%} |"
        for obj, cnt in sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    ])
    
    return f"""## üîç {t('yolo_results')}

### üè† {t('environment')}
| {t('env_type')} | {t('cook_style')} |
|:---:|:---:|
| **{environment.get('environment_type', t('na'))}** | **{environment.get('cooking_style', t('na'))}** |

### üìä {t('detection_stats')}
| {t('unique_objects')} | {t('total_detections')} |
|:---:|:---:|
| **{detection.get('unique_objects', 0)}** | **{detection.get('total_detections', 0)}** |

### üéØ {t('detected_objects')}
| Object | Count | {t('confidence')} |
|:---|:---:|:---:|
{objects_str}
"""


def format_consensus(output: ConsensusOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    cct_str = f"{output.cct:.0f}K" if output.cct else t('na')
    shot_str = f"{output.avg_shot_length:.2f}s" if output.avg_shot_length else t('na')
    bpm_str = f"{output.tempo_bpm:.1f}" if output.tempo_bpm else t('na')
    
    return f"""## üìä {t('summary_results')}

### üì∑ {t('camera')}
| {t('angle')} | {t('focal')} | Motion |
|:---:|:---:|:---:|
| **{output.camera_angle}** | **{output.focal_length_tendency}** | **{output.camera_motion}** |

### üé® {t('color')}
| {t('hue')} | {t('saturation')} | {t('brightness')} |
|:---:|:---:|:---:|
| **{output.hue_family}** | **{output.saturation}** | **{output.brightness}** |

{t('cct')}: **{cct_str}** | {t('avg_shot')}: **{shot_str}** | {t('transition')}: **{output.transition_type}**

### üéµ Audio
| {t('bgm_style')} | {t('mood')} | {t('bpm')} |
|:---:|:---:|:---:|
| **{output.bgm_style}** | **{output.bgm_mood}** | **{bpm_str}** |

### üè† Scene: **{output.scene_category}**
"""


# =============================================================================
# Processing Functions
# =============================================================================
def upload_video(video_file):
    if video_file is None:
        return t('upload_first'), None, []
    
    STATE.reset()
    STATE.work_dir = Path(tempfile.mkdtemp(prefix="video_analysis_"))
    
    video_path = Path(video_file)
    STATE.video_path = STATE.work_dir / video_path.name
    
    import shutil
    shutil.copy(video_file, STATE.video_path)
    
    STATE.audio_path = extract_audio_from_video(STATE.video_path, STATE.work_dir)
    frame_paths = extract_frames_for_gallery(STATE.video_path, STATE.work_dir, num_frames=12)
    
    status = f"{t('uploaded')}: {video_path.name}\n"
    status += f"{t('workdir')}: {STATE.work_dir}\n"
    status += t('frames_extracted').format(n=len(frame_paths)) + "\n"
    status += t('audio_extracted') if STATE.audio_path else t('audio_failed')
    
    audio_path = str(STATE.audio_path) if STATE.audio_path else None
    return status, audio_path, frame_paths


def run_visual(progress=gr.Progress()):
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}", None
    
    progress(0.1, desc=t('loading_clip'))
    step = VisualAnalysisStep()
    input_data = VideoInput(video_path=STATE.video_path, work_dir=STATE.work_dir, frame_mode="edge")
    
    progress(0.4, desc=t('analyzing_visual'))
    STATE.visual_output = step.run(input_data)
    
    progress(1.0, desc=t('done'))
    contact = STATE.visual_output.contact_sheet if STATE.visual_output else None
    return format_visual(STATE.visual_output), contact


def run_audio(progress=gr.Progress()):
    if STATE.audio_path is None:
        return f"‚ùå {t('upload_first')}"
    
    progress(0.1, desc=t('loading_clap'))
    step = AudioAnalysisStep()
    input_data = AudioInput(audio_path=STATE.audio_path)
    
    progress(0.4, desc=t('analyzing_audio'))
    STATE.audio_output = step.run(input_data)
    
    progress(1.0, desc=t('done'))
    return format_audio(STATE.audio_output)


def run_asr(language: str, progress=gr.Progress()):
    if STATE.audio_path is None:
        return f"‚ùå {t('upload_first')}"
    
    progress(0.1, desc=t('loading_whisper'))
    step = ASRAnalysisStep()
    input_data = ASRInput(audio_path=STATE.audio_path, language=language,
                          model_size="large-v3-turbo", enable_prosody=True, enable_emotion=True)
    
    progress(0.4, desc=t('analyzing_asr'))
    STATE.asr_output = step.run(input_data)
    
    progress(1.0, desc=t('done'))
    return format_asr(STATE.asr_output)


def run_yolo(progress=gr.Progress()):
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}"
    
    progress(0.1, desc=t('loading_yolo'))
    step = YOLOAnalysisStep()
    input_data = YOLOInput(video_path=STATE.video_path, target_frames=36,
                           enable_colors=True, enable_materials=True)
    
    progress(0.4, desc=t('analyzing_yolo'))
    STATE.yolo_output = step.run(input_data)
    
    progress(1.0, desc=t('done'))
    return format_yolo(STATE.yolo_output)


def run_consensus():
    if STATE.visual_output is None and STATE.audio_output is None:
        return f"‚ùå {t('run_analysis_first')}"
    
    metrics = VideoMetrics(path=str(STATE.video_path) if STATE.video_path else "")
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    step = ConsensusStep()
    input_data = ConsensusInput(video_metrics=[metrics])
    STATE.consensus_output = step.run(input_data)
    
    return format_consensus(STATE.consensus_output)


def run_all(language: str, progress=gr.Progress()):
    progress(0.1, desc=t('analyzing_visual'))
    visual_result, contact = run_visual()
    
    progress(0.3, desc=t('analyzing_audio'))
    audio_result = run_audio()
    
    progress(0.5, desc=t('analyzing_asr'))
    asr_result = run_asr(language)
    
    progress(0.7, desc=t('analyzing_yolo'))
    yolo_result = run_yolo()
    
    progress(0.9, desc=t('calculating_consensus'))
    consensus_result = run_consensus()
    
    progress(1.0, desc=t('done'))
    
    # Generate summary
    lines = ["=" * 30, t('quick_summary'), "=" * 30, ""]
    if STATE.visual_output:
        lines.append(f"üìπ {t('angle')}: {STATE.visual_output.camera_angle}")
        lines.append(f"üé® {t('hue')}: {STATE.visual_output.hue_family}")
        lines.append(f"‚úÇÔ∏è {t('cuts')}: {STATE.visual_output.cuts}")
    if STATE.audio_output:
        lines.append(f"üéµ {t('bpm')}: {STATE.audio_output.tempo_bpm:.1f}")
        lines.append(f"üé∏ {t('bgm_style')}: {STATE.audio_output.bgm_style}")
    if STATE.asr_output:
        lines.append(f"üé§ {t('wpm')}: {STATE.asr_output.words_per_minute:.1f}")
    if STATE.yolo_output:
        lines.append(f"üîç Objects: {STATE.yolo_output.detection.get('unique_objects', 0)}")
    
    summary = "\n".join(lines)
    return visual_result, contact, audio_result, asr_result, yolo_result, consensus_result, summary


def gen_report(progress=gr.Progress()):
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}", None, None
    
    if STATE.visual_output is None and STATE.audio_output is None:
        return f"‚ùå {t('run_analysis_first')}", None, None
    
    progress(0.2, desc=t('generating_word'))
    
    metrics = VideoMetrics(path=str(STATE.video_path))
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    if STATE.consensus_output is None:
        run_consensus()
    
    metrics_dict = metrics.to_dict()
    consensus_dict = STATE.consensus_output.to_dict() if STATE.consensus_output else {}
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = STATE.work_dir / f"report_{timestamp}.docx"
    
    generate_word_report(
        video_metrics=[metrics_dict],
        consensus=consensus_dict,
        output_path=str(report_path),
        show_screenshots=True
    )
    
    STATE.report_path = str(report_path)
    
    progress(0.7, desc=t('converting_pdf'))
    STATE.pdf_path = convert_docx_to_pdf(STATE.report_path)
    
    progress(1.0, desc=t('done'))
    
    status = f"{t('report_generated')}\nüìÑ {report_path.name}"
    if STATE.pdf_path:
        status += f"\nüìï {Path(STATE.pdf_path).name}"
    else:
        status += f"\n{t('pdf_needs_libreoffice')}"
    
    return status, STATE.report_path, STATE.pdf_path


def export_json():
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}", None
    
    data = {
        "timestamp": datetime.now().isoformat(),
        "video_path": str(STATE.video_path),
        "visual": STATE.visual_output.to_dict() if STATE.visual_output else None,
        "audio": STATE.audio_output.to_dict() if STATE.audio_output else None,
        "asr": STATE.asr_output.to_dict() if STATE.asr_output else None,
        "yolo": STATE.yolo_output.to_dict() if STATE.yolo_output else None,
        "consensus": STATE.consensus_output.to_dict() if STATE.consensus_output else None,
    }
    
    json_path = STATE.work_dir / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    return f"{t('json_exported')}: {json_path.name}", str(json_path)


def switch_language(lang: str):
    """Switch UI language and return updated labels"""
    set_language(lang)
    return (
        f"# {t('title')}\n**{t('subtitle')}**\n\n{t('models')}",
        t('analyze_all'),
        t('btn_visual'),
        t('btn_audio'),
        t('btn_asr'),
        t('btn_yolo'),
        t('btn_consensus'),
        t('gen_report'),
        t('export_json'),
        t('footer'),
    )


# =============================================================================
# Gradio UI
# =============================================================================
def create_ui():
    with gr.Blocks(
        title="Video Style Analysis",
        theme=gr.themes.Soft(primary_hue="blue", secondary_hue="slate"),
        css="""
        .markdown-text { font-size: 14px; }
        .result-markdown { min-height: 400px; }
        """
    ) as demo:
        
        # Header
        header_md = gr.Markdown(f"# {t('title')}\n**{t('subtitle')}**\n\n{t('models')}")
        
        # Language selector
        with gr.Row():
            lang_radio = gr.Radio(
                choices=[("English", "en"), ("‰∏≠Êñá", "zh")],
                value="en",
                label="Language / ËØ≠Ë®Ä",
                scale=1
            )
        
        gr.Markdown("---")
        
        with gr.Row():
            # ========== Left Column: Upload & Settings ==========
            with gr.Column(scale=1, min_width=300):
                gr.Markdown(f"### {t('upload_section')}")
                video_input = gr.Video(label=t('select_video'), height=200)
                upload_status = gr.Textbox(label=t('status'), lines=4, interactive=False)
                
                gr.Markdown(f"### {t('settings_section')}")
                language_select = gr.Dropdown(
                    choices=[("English", "en"), ("‰∏≠Êñá", "zh"), ("Êó•Êú¨Ë™û", "ja"), ("Auto", "auto")],
                    value="en",
                    label=t('asr_language')
                )
                
                gr.Markdown(f"### {t('preview_section')}")
                audio_player = gr.Audio(label=t('audio_preview'), type="filepath")
                frame_gallery = gr.Gallery(label=t('keyframes'), columns=3, height=150, object_fit="contain")
            
            # ========== Middle Column: Analysis & Results ==========
            with gr.Column(scale=2, min_width=500):
                gr.Markdown(f"### {t('control_section')}")
                
                with gr.Row():
                    run_all_btn = gr.Button(t('analyze_all'), variant="primary", size="lg", scale=2)
                
                with gr.Row():
                    run_visual_btn = gr.Button(t('btn_visual'), size="sm")
                    run_audio_btn = gr.Button(t('btn_audio'), size="sm")
                    run_asr_btn = gr.Button(t('btn_asr'), size="sm")
                    run_yolo_btn = gr.Button(t('btn_yolo'), size="sm")
                    run_consensus_btn = gr.Button(t('btn_consensus'), size="sm")
                
                gr.Markdown(f"### {t('results_section')}")
                
                with gr.Tabs():
                    with gr.Tab(t('tab_visual')):
                        visual_result = gr.Markdown(f"*{t('upload_first')}*", elem_classes="result-markdown")
                        contact_img = gr.Image(label="Contact Sheet", height=150)
                    
                    with gr.Tab(t('tab_audio')):
                        audio_result = gr.Markdown(f"*{t('upload_first')}*", elem_classes="result-markdown")
                    
                    with gr.Tab(t('tab_asr')):
                        asr_result = gr.Markdown(f"*{t('upload_first')}*", elem_classes="result-markdown")
                    
                    with gr.Tab(t('tab_yolo')):
                        yolo_result = gr.Markdown(f"*{t('upload_first')}*", elem_classes="result-markdown")
                    
                    with gr.Tab(t('tab_summary')):
                        consensus_result = gr.Markdown(f"*{t('run_analysis_first')}*", elem_classes="result-markdown")
            
            # ========== Right Column: Export ==========
            with gr.Column(scale=1, min_width=280):
                gr.Markdown(f"### {t('export_section')}")
                
                with gr.Row():
                    gen_report_btn = gr.Button(t('gen_report'), variant="secondary", size="sm")
                    export_json_btn = gr.Button(t('export_json'), size="sm")
                
                report_status = gr.Textbox(label=t('report_status'), lines=3, interactive=False)
                
                report_file = gr.File(label=t('word_report'))
                pdf_file = gr.File(label=t('pdf_report'))
                
                gr.Markdown("---")
                
                json_status = gr.Textbox(label=t('json_status'), lines=2, interactive=False)
                json_file = gr.File(label=t('json_data'))
                
                gr.Markdown("---")
                
                summary_box = gr.Textbox(label=t('quick_summary'), lines=10, interactive=False)
        
        # Footer
        footer_md = gr.Markdown(f"---\n{t('footer')}")
        
        # ========== Event Handlers ==========
        video_input.change(
            fn=upload_video,
            inputs=[video_input],
            outputs=[upload_status, audio_player, frame_gallery]
        )
        
        run_visual_btn.click(fn=run_visual, outputs=[visual_result, contact_img])
        run_audio_btn.click(fn=run_audio, outputs=[audio_result])
        run_asr_btn.click(fn=run_asr, inputs=[language_select], outputs=[asr_result])
        run_yolo_btn.click(fn=run_yolo, outputs=[yolo_result])
        run_consensus_btn.click(fn=run_consensus, outputs=[consensus_result])
        
        run_all_btn.click(
            fn=run_all,
            inputs=[language_select],
            outputs=[visual_result, contact_img, audio_result, asr_result, 
                     yolo_result, consensus_result, summary_box]
        )
        
        gen_report_btn.click(fn=gen_report, outputs=[report_status, report_file, pdf_file])
        export_json_btn.click(fn=export_json, outputs=[json_status, json_file])
        
        # Language switch
        lang_radio.change(
            fn=switch_language,
            inputs=[lang_radio],
            outputs=[
                header_md,
                run_all_btn,
                run_visual_btn,
                run_audio_btn,
                run_asr_btn,
                run_yolo_btn,
                run_consensus_btn,
                gen_report_btn,
                export_json_btn,
                footer_md,
            ]
        )
    
    return demo


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Video Style Analysis Web UI")
    parser.add_argument("--port", type=int, default=8088, help="Server port")
    parser.add_argument("--share", action="store_true", help="Create public link")
    parser.add_argument("--lang", type=str, default="en", choices=["en", "zh"], help="Default language")
    args = parser.parse_args()
    
    set_language(args.lang)
    
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share,
        show_error=True
    )
