#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Video Style Analysis - Gradio Web Interface
SOTA Models: CLIP | CLAP | HuBERT | Whisper | YOLO11 | Deep-Fake-Detector-v2
"""

import sys
import json
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, Dict, Any, List

import gradio as gr
import numpy as np
import cv2

sys.path.insert(0, str(Path(__file__).parent / "src"))

from config import PipelineConfig, get_default_config
from steps import (
    VisualAnalysisStep, AudioAnalysisStep, ASRAnalysisStep,
    YOLOAnalysisStep, ConsensusStep, AIDetectionStep,
    VideoInput, AudioInput, ASRInput, YOLOInput, ConsensusInput, AIDetectionInput,
    VideoMetrics, VisualOutput, AudioOutput, ASROutput, YOLOOutput, ConsensusOutput, AIDetectionOutput,
)
from report_word import generate_word_report

# =============================================================================
# Internationalization (i18n)
# =============================================================================
TRANSLATIONS = {
    "en": {
        "title": "üé¨ Video Style Analysis",
        "subtitle": "SOTA 2025/2026 | PyTorch + HuggingFace",
        "models": "CLIP ¬∑ CLAP ¬∑ HuBERT ¬∑ Whisper ¬∑ YOLO11 ¬∑ DeepFake-v2",
        "upload_section": "üì§ Upload",
        "settings_section": "‚öôÔ∏è Settings",
        "preview_section": "üé¨ Preview",
        "control_section": "üöÄ Analysis",
        "results_section": "üìä Results",
        "export_section": "üì• Export",
        "config_section": "üîß Configuration",
        "select_video": "Select Video (mp4, avi, mov, mkv)",
        "status": "Status",
        "asr_language": "ASR Language",
        "audio_preview": "Extracted Audio",
        "keyframes": "Key Frames",
        "analyze_all": "üéØ Analyze All",
        "btn_visual": "üìπ Camera & Color",
        "btn_audio": "üéµ BGM & Tempo",
        "btn_asr": "üé§ Speech & Emotion",
        "btn_yolo": "üîç Objects",
        "btn_consensus": "üìä Summary",
        "btn_ai_detect": "ü§ñ AI Detect",
        "gen_report": "üìÑ Report",
        "export_json": "üíæ JSON",
        "tab_upload": "üì§ Upload & Preview",
        "tab_analysis": "üöÄ Run Analysis",
        "tab_export": "üì• Export & Reports",
        "tab_config": "‚öôÔ∏è Settings & Weights",
        "tab_visual": "üìπ Camera & Color",
        "tab_audio": "üéµ BGM & Tempo",
        "tab_asr": "üé§ Speech & Emotion",
        "tab_yolo": "üîç Objects & Materials",
        "tab_summary": "üìä Cross-Video Summary",
        "tab_ai": "ü§ñ AI/Deepfake Detection",
        "report_status": "Report Status",
        "word_report": "Word Report",
        "pdf_report": "PDF Report",
        "json_data": "JSON Data",
        "json_status": "JSON Status",
        "quick_summary": "Quick Summary",
        "upload_first": "Please upload a video first",
        "run_analysis_first": "Please run analysis first",
        "uploaded": "‚úÖ Uploaded",
        "workdir": "Work Directory",
        "frames_extracted": "Extracted {n} frames",
        "audio_extracted": "‚úÖ Audio extracted",
        "audio_failed": "‚ö†Ô∏è Audio extraction failed",
        "analysis_failed": "Analysis failed",
        "report_generated": "‚úÖ Report generated",
        "json_exported": "‚úÖ JSON exported",
        "loading": "Loading",
        "analyzing": "Analyzing",
        "done": "‚úÖ Done",
        "footer": "Video Style Analysis | SOTA 2025/2026",
        # Config labels
        "visual_frames": "Visual: Target Frames",
        "visual_scene_threshold": "Visual: Scene Threshold",
        "audio_sample_rate": "Audio: Sample Rate",
        "asr_model": "ASR: Whisper Model",
        "asr_beam_size": "ASR: Beam Size",
        "yolo_model": "YOLO: Model",
        "yolo_conf": "YOLO: Confidence",
        "yolo_frames": "YOLO: Target Frames",
        "ai_enabled": "AI Detection: Enabled",
        "ai_video_model": "AI: Video Model",
        "ai_video_threshold": "AI: Video Threshold",
        "ai_frame_enabled": "AI: Frame Detection",
        "ai_frame_threshold": "AI: Frame Threshold",
        "ai_face_enabled": "AI: Face Detection",
        "ai_video_weight": "AI: Video Weight",
        "ai_frame_weight": "AI: Frame Weight",
        "ai_face_weight": "AI: Face Weight",
        # Results
        "visual_results": "Visual Analysis Results",
        "audio_results": "Audio Analysis Results",
        "asr_results": "Speech Analysis Results",
        "yolo_results": "Object Detection Results",
        "summary_results": "Summary Results",
        "ai_results": "AI Detection Results",
        "verdict": "Verdict",
        "confidence": "Confidence",
        "real": "Real",
        "deepfake": "Deepfake",
        "synthetic": "Synthetic",
        "suspicious": "Suspicious",
    },
    "zh": {
        "title": "üé¨ ËßÜÈ¢ëÈ£éÊ†ºÂàÜÊûêÁ≥ªÁªü",
        "subtitle": "SOTA 2025/2026 | PyTorch + HuggingFace",
        "models": "CLIP ¬∑ CLAP ¬∑ HuBERT ¬∑ Whisper ¬∑ YOLO11 ¬∑ DeepFake-v2",
        "upload_section": "üì§ ‰∏ä‰º†",
        "settings_section": "‚öôÔ∏è ËÆæÁΩÆ",
        "preview_section": "üé¨ È¢ÑËßà",
        "control_section": "üöÄ ÂàÜÊûê",
        "results_section": "üìä ÁªìÊûú",
        "export_section": "üì• ÂØºÂá∫",
        "config_section": "üîß ÂèÇÊï∞ÈÖçÁΩÆ",
        "select_video": "ÈÄâÊã©ËßÜÈ¢ë (mp4, avi, mov, mkv)",
        "status": "Áä∂ÊÄÅ",
        "asr_language": "ËØ≠Èü≥ËØÜÂà´ËØ≠Ë®Ä",
        "audio_preview": "ÊèêÂèñÁöÑÈü≥È¢ë",
        "keyframes": "ÂÖ≥ÈîÆÂ∏ß",
        "analyze_all": "üéØ ‰∏ÄÈîÆÂàÜÊûê",
        "btn_visual": "üìπ ÈïúÂ§¥Ëâ≤ÂΩ©",
        "btn_audio": "üéµ ËÉåÊôØÈü≥‰πê",
        "btn_asr": "üé§ ËØ≠Èü≥ÊÉÖÊÑü",
        "btn_yolo": "üîç Áâ©‰ΩìÊ£ÄÊµã",
        "btn_consensus": "üìä ÁªºÂêàÊ±áÊÄª",
        "btn_ai_detect": "ü§ñ AIÊ£ÄÊµã",
        "gen_report": "üìÑ ÁîüÊàêÊä•Âëä",
        "export_json": "üíæ ÂØºÂá∫JSON",
        "tab_upload": "üì§ ‰∏ä‰º†‰∏éÈ¢ÑËßà",
        "tab_analysis": "üöÄ ËøêË°åÂàÜÊûê",
        "tab_export": "üì• ÂØºÂá∫‰∏éÊä•Âëä",
        "tab_config": "‚öôÔ∏è ÂèÇÊï∞ËÆæÁΩÆ",
        "tab_visual": "üìπ ÈïúÂ§¥‰∏éËâ≤ÂΩ©",
        "tab_audio": "üéµ ËÉåÊôØÈü≥‰πê‰∏éËäÇÂ•è",
        "tab_asr": "üé§ ËØ≠Èü≥‰∏éÊÉÖÊÑü",
        "tab_yolo": "üîç Áâ©‰Ωì‰∏éÊùêË¥®",
        "tab_summary": "üìä ÁªºÂêàÊ±áÊÄª",
        "tab_ai": "ü§ñ AIÁîüÊàêÊ£ÄÊµã",
        "report_status": "Êä•ÂëäÁä∂ÊÄÅ",
        "word_report": "Word Êä•Âëä",
        "pdf_report": "PDF Êä•Âëä",
        "json_data": "JSON Êï∞ÊçÆ",
        "json_status": "JSON Áä∂ÊÄÅ",
        "quick_summary": "Âø´ÈÄüÊëòË¶Å",
        "upload_first": "ËØ∑ÂÖà‰∏ä‰º†ËßÜÈ¢ë",
        "run_analysis_first": "ËØ∑ÂÖàËøêË°åÂàÜÊûê",
        "uploaded": "‚úÖ Â∑≤‰∏ä‰º†",
        "workdir": "Â∑•‰ΩúÁõÆÂΩï",
        "frames_extracted": "Â∑≤ÊèêÂèñ {n} Â∏ß",
        "audio_extracted": "‚úÖ Èü≥È¢ëÂ∑≤ÊèêÂèñ",
        "audio_failed": "‚ö†Ô∏è Èü≥È¢ëÊèêÂèñÂ§±Ë¥•",
        "analysis_failed": "ÂàÜÊûêÂ§±Ë¥•",
        "report_generated": "‚úÖ Êä•ÂëäÂ∑≤ÁîüÊàê",
        "json_exported": "‚úÖ JSON Â∑≤ÂØºÂá∫",
        "loading": "Âä†ËΩΩ‰∏≠",
        "analyzing": "ÂàÜÊûê‰∏≠",
        "done": "‚úÖ ÂÆåÊàê",
        "footer": "ËßÜÈ¢ëÈ£éÊ†ºÂàÜÊûê | SOTA 2025/2026",
        # Config labels
        "visual_frames": "ËßÜËßâ: ÁõÆÊ†áÂ∏ßÊï∞",
        "visual_scene_threshold": "ËßÜËßâ: Âú∫ÊôØÈòàÂÄº",
        "audio_sample_rate": "Èü≥È¢ë: ÈááÊ†∑Áéá",
        "asr_model": "ASR: Whisper Ê®°Âûã",
        "asr_beam_size": "ASR: Beam Size",
        "yolo_model": "YOLO: Ê®°Âûã",
        "yolo_conf": "YOLO: ÁΩÆ‰ø°Â∫¶",
        "yolo_frames": "YOLO: ÁõÆÊ†áÂ∏ßÊï∞",
        "ai_enabled": "AIÊ£ÄÊµã: ÂêØÁî®",
        "ai_video_model": "AI: ËßÜÈ¢ëÊ®°Âûã",
        "ai_video_threshold": "AI: ËßÜÈ¢ëÈòàÂÄº",
        "ai_frame_enabled": "AI: Â∏ßÊ£ÄÊµã",
        "ai_frame_threshold": "AI: Â∏ßÈòàÂÄº",
        "ai_face_enabled": "AI: ‰∫∫ËÑ∏Ê£ÄÊµã",
        "ai_video_weight": "AI: ËßÜÈ¢ëÊùÉÈáç",
        "ai_frame_weight": "AI: Â∏ßÊùÉÈáç",
        "ai_face_weight": "AI: ‰∫∫ËÑ∏ÊùÉÈáç",
        # Results
        "visual_results": "ËßÜËßâÂàÜÊûêÁªìÊûú",
        "audio_results": "Èü≥È¢ëÂàÜÊûêÁªìÊûú",
        "asr_results": "ËØ≠Èü≥ÂàÜÊûêÁªìÊûú",
        "yolo_results": "ÁõÆÊ†áÊ£ÄÊµãÁªìÊûú",
        "summary_results": "Ê±áÊÄªÁªìÊûú",
        "ai_results": "AIÁîüÊàêÊ£ÄÊµãÁªìÊûú",
        "verdict": "Âà§ÂÆö",
        "confidence": "ÁΩÆ‰ø°Â∫¶",
        "real": "ÁúüÂÆû",
        "deepfake": "Ê∑±Â∫¶‰º™ÈÄ†",
        "synthetic": "ÂêàÊàê",
        "suspicious": "ÂèØÁñë",
    }
}

LANG = "en"

def t(key: str) -> str:
    return TRANSLATIONS.get(LANG, TRANSLATIONS["en"]).get(key, key)

def set_language(lang: str):
    global LANG
    LANG = lang if lang in TRANSLATIONS else "en"


# =============================================================================
# Global State
# =============================================================================
class AnalysisState:
    def __init__(self):
        self.reset()
        self.config = get_default_config()
    
    def reset(self):
        self.video_path: Optional[Path] = None
        self.audio_path: Optional[Path] = None
        self.work_dir: Optional[Path] = None
        self.visual_output: Optional[VisualOutput] = None
        self.audio_output: Optional[AudioOutput] = None
        self.asr_output: Optional[ASROutput] = None
        self.yolo_output: Optional[YOLOOutput] = None
        self.consensus_output: Optional[ConsensusOutput] = None
        self.ai_output: Optional[AIDetectionOutput] = None
        self.report_path: Optional[str] = None
        self.pdf_path: Optional[str] = None

STATE = AnalysisState()


# =============================================================================
# Utility Functions
# =============================================================================
def extract_audio_from_video(video_path: Path, output_dir: Path) -> Optional[Path]:
    output_path = output_dir / f"{video_path.stem}_audio.wav"
    if output_path.exists():
        return output_path
    try:
        sr = STATE.config.audio.sample_rate
        cmd = ["ffmpeg", "-y", "-i", str(video_path), "-vn", "-acodec", "pcm_s16le",
               "-ar", str(sr), "-ac", "1", str(output_path)]
        subprocess.run(cmd, capture_output=True, check=True)
        return output_path
    except Exception:
        return None


def extract_frames_for_gallery(video_path: Path, output_dir: Path, num_frames: int = 12) -> List[str]:
    frames_dir = output_dir / "frames"
    frames_dir.mkdir(exist_ok=True)
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    if total_frames == 0:
        cap.release()
        return []
    
    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    frame_paths = []
    
    for i, idx in enumerate(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            timestamp = idx / fps if fps > 0 else 0
            frame_path = frames_dir / f"frame_{i:03d}_{timestamp:.1f}s.jpg"
            cv2.imwrite(str(frame_path), frame)
            frame_paths.append(str(frame_path))
    
    cap.release()
    return frame_paths


def convert_docx_to_pdf(docx_path: str) -> Optional[str]:
    pdf_path = docx_path.replace('.docx', '.pdf')
    try:
        timeout = STATE.config.report.pdf_timeout
        cmd = ["libreoffice", "--headless", "--convert-to", "pdf",
               "--outdir", str(Path(docx_path).parent), docx_path]
        subprocess.run(cmd, capture_output=True, check=True, timeout=timeout)
        if Path(pdf_path).exists():
            return pdf_path
    except Exception:
        pass
    return None


# =============================================================================
# Result Formatters
# =============================================================================
def format_visual(output: VisualOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    scenes = "\n".join([
        f"  ‚Ä¢ {s.get('label', '?')}: **{s.get('probability', 0):.1%}**"
        for s in output.scene_categories[:5]
    ])
    
    # CCT interpretation
    cct = output.cct_mean
    if cct < 3500:
        cct_desc = "Warm (incandescent/sunset)"
    elif cct < 5500:
        cct_desc = "Neutral (daylight balanced)"
    else:
        cct_desc = "Cool (overcast/blue hour)"
    
    # Cut rate interpretation
    cut_rate = output.cuts / max(output.duration, 1) * 60  # cuts per minute
    if cut_rate > 30:
        pace_desc = "Very fast editing"
    elif cut_rate > 15:
        pace_desc = "Dynamic editing"
    elif cut_rate > 5:
        pace_desc = "Moderate pacing"
    else:
        pace_desc = "Slow, contemplative"
    
    return f"""## üìπ Visual Analysis Results

### üìä Video Info

| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Duration** | {output.duration:.2f}s | Total video length |
| **FPS** | {output.fps:.1f} | Frames per second |
| **Frames Analyzed** | {output.sampled_frames} | Sample size for analysis |

### üì∑ Camera & Composition

| Metric | Value | Description |
|:-------|:------|:------------|
| **Camera Angle** | {output.camera_angle} | Viewer perspective (eye-level, overhead, low) |
| **Focal Length** | {output.focal_length_tendency} | Wide-angle, normal, or telephoto |

### üé® Color Analysis

| Metric | Value | Description |
|:-------|:------|:------------|
| **Dominant Hue** | {output.hue_family} | Primary color family |
| **Saturation** | {output.saturation_band} | Color intensity (vivid/muted) |
| **Brightness** | {output.brightness_band} | Light/dark overall |
| **Contrast** | {output.contrast} | Dynamic range |
| **Color Temp (CCT)** | {output.cct_mean:.0f}K | {cct_desc} |

### ‚úÇÔ∏è Editing Pace

| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Total Cuts** | {output.cuts} | Scene transitions detected |
| **Cuts/Minute** | {cut_rate:.1f} | {pace_desc} |

### üè† Scene Classification (CLIP)
*Top detected scene types:*
{scenes}

---
*Scene classification powered by CLIP (openai/clip-vit-large-patch14)*
"""


def format_audio(output: AudioOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    instruments = output.instruments.get('detected_instruments', [])
    inst_str = ", ".join(instruments[:5]) if instruments else "N/A"
    
    # Add explanations based on values
    tempo_desc = "Fast-paced" if output.tempo_bpm > 120 else "Medium tempo" if output.tempo_bpm > 80 else "Slow, relaxed"
    percussive_desc = "Heavy drums" if output.percussive_ratio > 0.5 else "Moderate beats" if output.percussive_ratio > 0.2 else "Light rhythm"
    
    return f"""## üéµ Audio Analysis Results

### üíì Rhythm & Tempo

| Metric | Value | Interpretation |
|:-------|:-----:|:---------------|
| **BPM** | {output.tempo_bpm:.1f} | {tempo_desc} |
| **Beat Count** | {output.num_beats} | Total rhythmic beats detected |
| **Percussive Ratio** | {output.percussive_ratio:.2f} | {percussive_desc} |

### üé∏ Music Classification (CLAP Model)

| Metric | Value | Description |
|:-------|:------|:------------|
| **BGM Style** | {output.bgm_style} | Genre/style of background music |
| **Mood** | {output.mood} | Emotional tone of the audio |
| **Key** | {output.key_signature} | Musical key (if detected) |

### üéπ Instruments Detected
{inst_str}

---
*Analysis powered by CLAP (laion/larger_clap_music_and_speech)*
"""


def format_asr(output: ASROutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    text_preview = output.text[:500] + '...' if len(output.text) > 500 else output.text
    
    # WPM interpretation
    wpm = output.words_per_minute
    if wpm > 160:
        pace_desc = "Very fast (energetic/urgent)"
    elif wpm > 130:
        pace_desc = "Fast (conversational+)"
    elif wpm > 100:
        pace_desc = "Normal conversation"
    elif wpm > 60:
        pace_desc = "Slow (deliberate/clear)"
    else:
        pace_desc = "Very slow (emphatic)"
    
    # Emotion section
    emotion_section = ""
    if output.emotion:
        emo = output.emotion.get('dominant_emotion', 'N/A')
        conf = output.emotion.get('confidence', 0)
        emotion_section = f"""
### üé≠ Emotion Analysis (HuBERT Model)
| Detected Emotion | Confidence |
|:----------------:|:----------:|
| **{emo}** | {conf:.1%} |
"""
    
    # Prosody section
    prosody_section = ""
    if output.prosody:
        pitch = output.prosody.get('mean_pitch_hz', 0)
        style = output.prosody.get('prosody_style', 'N/A')
        intensity = output.prosody.get('mean_intensity', 0)
        prosody_section = f"""
### üìä Prosody Analysis (Librosa)
| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Pitch** | {pitch:.1f} Hz | Average fundamental frequency |
| **Style** | {style} | Speaking manner |
| **Intensity** | {intensity:.1f} dB | Volume level |
"""
    
    return f"""## üé§ Speech Analysis Results

### üó£Ô∏è Speech Rate

| Metric | Value | Interpretation |
|:-------|:-----:|:---------------|
| **Total Words** | {output.num_words} | Words transcribed |
| **Words/Min (WPM)** | {wpm:.1f} | {pace_desc} |
| **Pace Category** | {output.pace} | Overall speaking speed |

{emotion_section}
{prosody_section}

### üìù Transcript (Whisper large-v3-turbo)
```
{text_preview}
```

---
*ASR powered by faster-whisper, Emotion by HuBERT*
"""


def format_yolo(output: YOLOOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    detection = output.detection
    environment = output.environment
    object_counts = detection.get('object_counts', {})
    
    objects_str = "\n".join([
        f"| {obj} | {cnt} |"
        for obj, cnt in sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    ])
    
    # Colors section
    colors_section = ""
    if output.colors:
        colors = output.colors
        if isinstance(colors, dict):
            dom_colors = colors.get('dominant_colors', colors.get('all_colors', []))
            if dom_colors and isinstance(dom_colors, list):
                colors_str = ", ".join(dom_colors[:5]) if dom_colors else "N/A"
                colors_section = f"""
### üé® Object Colors
**Dominant Colors**: {colors_str}
"""
    
    # Materials section
    materials_section = ""
    if output.materials:
        mats = output.materials
        if isinstance(mats, dict):
            dom_mats = mats.get('dominant_materials', mats.get('all_materials', []))
            if dom_mats and isinstance(dom_mats, list):
                mats_str = ", ".join(dom_mats[:5]) if dom_mats else "N/A"
                materials_section = f"""
### üß± Materials Detected
**Dominant Materials**: {mats_str}
"""
    
    return f"""## üîç Object Detection Results

### üè† Environment Classification

| Metric | Value | Description |
|:-------|:------|:------------|
| **Environment Type** | {environment.get('environment_type', 'N/A')} | Primary scene category |
| **Activity Style** | {environment.get('cooking_style', 'N/A')} | Detected activity type |

### üì¶ Object Detection Statistics

| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Unique Objects** | {detection.get('unique_objects', 0)} | Different object types found |
| **Total Detections** | {detection.get('total_detections', 0)} | Total instances across frames |

### üìã Detected Objects (Top 10)

| Object | Count |
|:-------|:-----:|
{objects_str}

{colors_section}
{materials_section}

---
*Detection powered by YOLO11 (ultralytics)*
"""


def format_consensus(output: ConsensusOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    cct_str = f"{output.cct:.0f}K" if output.cct else "N/A"
    bpm_str = f"{output.tempo_bpm:.1f}" if output.tempo_bpm else "N/A"
    
    return f"""## üìä {t('summary_results')}

| Camera | Hue | Saturation | Brightness |
|:---:|:---:|:---:|:---:|
| **{output.camera_angle}** | **{output.hue_family}** | **{output.saturation}** | **{output.brightness}** |

| BGM | Mood | BPM | CCT |
|:---:|:---:|:---:|:---:|
| **{output.bgm_style}** | **{output.bgm_mood}** | **{bpm_str}** | **{cct_str}** |

**Scene**: {output.scene_category}
"""


def format_ai_detection(output: AIDetectionOutput) -> str:
    if not output or not output.success:
        return f"‚ùå {t('analysis_failed')}"
    
    verdict_emoji = {
        "Real": "‚úÖ", "Suspicious": "‚ö†Ô∏è", "Deepfake": "üé≠",
        "AIGC": "üé®", "Audio-Deepfake": "üîä",
        "Synthetic": "ü§ñ", "AI-Generated": "ü§ñ", "Unknown": "‚ùì"
    }
    emoji = verdict_emoji.get(output.verdict, "‚ùì")
    
    models_str = ", ".join(output.models_used) if output.models_used else "None"
    
    # Get scores and availability flags with fallback
    aigc_score = getattr(output, 'aigc_score', 0.0)
    aigc_available = getattr(output, 'aigc_available', False)
    audio_score = getattr(output, 'audio_deepfake_score', 0.0)
    audio_available = getattr(output, 'audio_deepfake_available', False)
    temporal_available = getattr(output, 'temporal_available', False)
    face_available = getattr(output, 'face_available', False)
    
    # Get weights from analysis details
    weights = output.analysis_details.get("weights", {})
    deepfake_w = weights.get("deepfake", 0.30)
    clip_w = weights.get("clip", 0.20)
    temporal_w = weights.get("temporal", 0.15)
    aigc_w = weights.get("aigc", 0.20)
    audio_w = weights.get("audio_deepfake", 0.10)
    face_w = weights.get("face", 0.05)
    
    # Calculate weighted contribution
    def weighted_contrib(score, weight, available):
        if not available:
            return "‚Äî"
        contrib = score * weight
        return f"{contrib:.1%}"
    
    return f"""## ü§ñ AI Detection Results

### {emoji} Verdict: **{output.verdict}**
### Confidence: **{output.confidence:.1%}** (weighted average)

---

### üìä Detection Models & Weights

| Model | Weight | Score | Contribution | Status | Description |
|:------|:------:|:-----:|:------------:|:------:|:------------|
| üé≠ **DeepFake-v2** | `{deepfake_w:.0%}` | {output.deepfake_score:.1%} | {weighted_contrib(output.deepfake_score, deepfake_w, output.deepfake_available)} | {'‚úÖ' if output.deepfake_available else '‚ùå'} | *HuggingFace ViT model (92% acc), detects face swaps* |
| üîç **CLIP Synthetic** | `{clip_w:.0%}` | {output.clip_synthetic_score:.1%} | {weighted_contrib(output.clip_synthetic_score, clip_w, output.clip_available)} | {'‚úÖ' if output.clip_available else '‚ùå'} | *Zero-shot detection using CLIP embeddings* |
| ‚è±Ô∏è **CLIP-Temporal** | `{temporal_w:.0%}` | {output.temporal_score:.1%} | {weighted_contrib(output.temporal_score, temporal_w, temporal_available)} | {'‚úÖ' if temporal_available else '‚ùå'} | *Semantic consistency between frames (CLIP-based)* |
| üé® **AIGC Detector** | `{aigc_w:.0%}` | {aigc_score:.1%} | {weighted_contrib(aigc_score, aigc_w, aigc_available)} | {'‚úÖ' if aigc_available else '‚ùå'} | *Detects Stable Diffusion, DALL-E, Midjourney* |
| üîä **Audio Deepfake** | `{audio_w:.0%}` | {audio_score:.1%} | {weighted_contrib(audio_score, audio_w, audio_available)} | {'‚úÖ' if audio_available else '‚ùå'} | *Detects voice cloning & TTS synthesis* |
| üë§ **Face Analysis** | `{face_w:.0%}` | {output.no_face_ratio:.1%} | ‚Äî | {'‚úÖ' if face_available else '‚ùå'} | *No-face ratio analysis (>90% suspicious)* |

---

### üë§ Face Detection Details

| Metric | Value | Explanation |
|:-------|:-----:|:------------|
| **Faces Detected** | {output.faces_detected} | Total faces found across all frames |
| **Frames with Faces** | {output.frames_with_faces}/{output.frames_analyzed} | Ratio of frames containing faces |
| **No-Face Ratio** | {output.no_face_ratio:.1%} | Higher = more suspicious for face videos |
| **Temporal Anomalies** | {output.temporal_anomalies} | Sudden changes in frame consistency |

---

### ‚ÑπÔ∏è How Scoring Works

- **Final Confidence** = Œ£ (Model Score √ó Weight) / Œ£ Weights
- Models with ‚≠ê use HuggingFace pretrained models (higher reliability)
- Models with ‚úÖ use computed features (good reliability)
- **Verdict Thresholds**: Real <40% | Suspicious 40-70% | AI-Generated ‚â•70%

**Active Models**: {models_str}
"""


# =============================================================================
# Processing Functions
# =============================================================================
def upload_video(video_file):
    if video_file is None:
        return t('upload_first'), None, []
    
    STATE.reset()
    STATE.work_dir = Path(tempfile.mkdtemp(prefix="video_analysis_"))
    
    video_path = Path(video_file)
    STATE.video_path = STATE.work_dir / video_path.name
    
    import shutil
    shutil.copy(video_file, STATE.video_path)
    
    STATE.audio_path = extract_audio_from_video(STATE.video_path, STATE.work_dir)
    num_frames = STATE.config.ui.gallery_frames
    frame_paths = extract_frames_for_gallery(STATE.video_path, STATE.work_dir, num_frames)
    
    status = f"{t('uploaded')}: {video_path.name}\n"
    status += f"{t('workdir')}: {STATE.work_dir}\n"
    status += t('frames_extracted').format(n=len(frame_paths)) + "\n"
    status += t('audio_extracted') if STATE.audio_path else t('audio_failed')
    
    audio_path = str(STATE.audio_path) if STATE.audio_path else None
    return status, audio_path, frame_paths


# Internal analysis functions (no progress tracking)
def _run_visual_internal():
    """Internal visual analysis without progress tracking"""
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}", None
    
    cfg = STATE.config.visual
    step = VisualAnalysisStep()
    input_data = VideoInput(
        video_path=STATE.video_path,
        work_dir=STATE.work_dir,
        frame_mode=cfg.frame_mode,
        target_frames=cfg.target_frames,
        scene_threshold=cfg.scene_threshold
    )
    
    STATE.visual_output = step.run(input_data)
    contact = STATE.visual_output.contact_sheet if STATE.visual_output else None
    return format_visual(STATE.visual_output), contact


def _run_audio_internal():
    """Internal audio analysis without progress tracking"""
    if STATE.audio_path is None:
        return f"‚ùå {t('upload_first')}"
    
    step = AudioAnalysisStep()
    input_data = AudioInput(audio_path=STATE.audio_path)
    STATE.audio_output = step.run(input_data)
    return format_audio(STATE.audio_output)


def _run_asr_internal(language: str):
    """Internal ASR analysis without progress tracking"""
    if STATE.audio_path is None:
        return f"‚ùå {t('upload_first')}"
    
    cfg = STATE.config.asr
    step = ASRAnalysisStep()
    input_data = ASRInput(
        audio_path=STATE.audio_path,
        language=language,
        model_size=cfg.whisper_model,
        beam_size=cfg.whisper_beam_size,
        enable_prosody=True,
        enable_emotion=True
    )
    STATE.asr_output = step.run(input_data)
    return format_asr(STATE.asr_output)


def _run_yolo_internal():
    """Internal YOLO analysis without progress tracking"""
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}"
    
    cfg = STATE.config.yolo
    step = YOLOAnalysisStep()
    input_data = YOLOInput(
        video_path=STATE.video_path,
        target_frames=cfg.target_frames,
        model_name=cfg.model_name,
        confidence_threshold=cfg.confidence_threshold,
        enable_colors=cfg.enable_colors,
        enable_materials=cfg.enable_materials
    )
    STATE.yolo_output = step.run(input_data)
    return format_yolo(STATE.yolo_output)


def _run_ai_detection_internal():
    """Internal AI detection without progress tracking"""
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}"
    
    cfg = STATE.config.ai_detection
    if not cfg.enabled:
        return "‚ùå AI Detection is disabled"
    
    step = AIDetectionStep()
    input_data = AIDetectionInput(
        video_path=STATE.video_path,
        audio_path=STATE.audio_path,
        use_deepfake=cfg.use_deepfake,
        use_clip=cfg.use_clip,
        use_temporal=cfg.use_temporal,
        use_face_detection=cfg.use_face_detection,
        use_aigc=cfg.use_aigc,
        use_audio_deepfake=cfg.use_audio_deepfake,
        num_frames=cfg.num_frames,
        temporal_frames=cfg.temporal_frames,
        fake_threshold=cfg.fake_threshold,
        no_face_threshold=cfg.no_face_threshold,
        deepfake_weight=cfg.deepfake_weight,
        clip_weight=cfg.clip_weight,
        temporal_weight=cfg.temporal_weight,
        aigc_weight=cfg.aigc_weight,
        audio_deepfake_weight=cfg.audio_deepfake_weight,
        face_weight=cfg.face_weight,
    )
    STATE.ai_output = step.run(input_data)
    return format_ai_detection(STATE.ai_output)


# Public analysis functions with progress tracking (for standalone button clicks)
def run_visual(progress=gr.Progress()):
    progress(0.1, desc="üìπ Loading CLIP...")
    progress(0.3, desc="üìπ Analyzing visual...")
    result = _run_visual_internal()
    progress(1.0, desc="‚úÖ Visual done")
    return result


def run_audio(progress=gr.Progress()):
    progress(0.1, desc="üéµ Loading CLAP...")
    progress(0.3, desc="üéµ Analyzing audio...")
    result = _run_audio_internal()
    progress(1.0, desc="‚úÖ Audio done")
    return result


def run_asr(language: str, progress=gr.Progress()):
    progress(0.1, desc="üé§ Loading Whisper...")
    progress(0.3, desc="üé§ Transcribing...")
    result = _run_asr_internal(language)
    progress(1.0, desc="‚úÖ ASR done")
    return result


def run_yolo(progress=gr.Progress()):
    progress(0.1, desc="üîç Loading YOLO...")
    progress(0.3, desc="üîç Detecting objects...")
    result = _run_yolo_internal()
    progress(1.0, desc="‚úÖ YOLO done")
    return result


def run_ai_detection(progress=gr.Progress()):
    progress(0.1, desc="ü§ñ Loading AI models...")
    progress(0.3, desc="ü§ñ Detecting AI content...")
    result = _run_ai_detection_internal()
    progress(1.0, desc="‚úÖ AI detection done")
    return result


def run_consensus():
    if STATE.visual_output is None and STATE.audio_output is None:
        return f"‚ùå {t('run_analysis_first')}"
    
    metrics = VideoMetrics(path=str(STATE.video_path) if STATE.video_path else "")
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    step = ConsensusStep()
    input_data = ConsensusInput(video_metrics=[metrics])
    STATE.consensus_output = step.run(input_data)
    
    return format_consensus(STATE.consensus_output)


def run_all(language: str, progress=gr.Progress()):
    # Use internal functions to avoid duplicate progress bars
    
    progress(0.05, desc="üìπ Step 1/6: Visual Analysis...")
    visual_result, contact = _run_visual_internal()
    
    progress(0.20, desc="üéµ Step 2/6: Audio Analysis...")
    audio_result = _run_audio_internal()
    
    progress(0.35, desc="üé§ Step 3/6: Speech Recognition...")
    asr_result = _run_asr_internal(language)
    
    progress(0.50, desc="üîç Step 4/6: Object Detection...")
    yolo_result = _run_yolo_internal()
    
    progress(0.65, desc="ü§ñ Step 5/6: AI Detection...")
    ai_result = _run_ai_detection_internal() if STATE.config.ai_detection.enabled else "*Disabled*"
    
    progress(0.85, desc="üìä Step 6/6: Generating Summary...")
    consensus_result = run_consensus()
    
    progress(1.0, desc=t('done'))
    
    # Generate summary
    lines = ["=" * 25, t('quick_summary'), "=" * 25, ""]
    if STATE.visual_output:
        lines.append(f"üìπ Camera: {STATE.visual_output.camera_angle}")
        lines.append(f"üé® Color: {STATE.visual_output.hue_family}")
    if STATE.audio_output:
        lines.append(f"üéµ BPM: {STATE.audio_output.tempo_bpm:.1f}")
        lines.append(f"üé∏ BGM: {STATE.audio_output.bgm_style}")
    if STATE.asr_output:
        lines.append(f"üé§ WPM: {STATE.asr_output.words_per_minute:.1f}")
    if STATE.yolo_output:
        lines.append(f"üîç Objects: {STATE.yolo_output.detection.get('unique_objects', 0)}")
    if STATE.ai_output:
        lines.append(f"ü§ñ AI: {STATE.ai_output.verdict} ({STATE.ai_output.confidence:.0%})")
    
    summary = "\n".join(lines)
    return visual_result, contact, audio_result, asr_result, yolo_result, ai_result, consensus_result, summary


def gen_report(progress=gr.Progress()):
    if STATE.video_path is None:
        error_html = "<div style='text-align:center; padding:40px; background:#fee; border-radius:8px;'><p>‚ùå Please upload a video first</p></div>"
        return f"‚ùå {t('upload_first')}", None, None, error_html
    
    if STATE.visual_output is None and STATE.audio_output is None:
        error_html = "<div style='text-align:center; padding:40px; background:#fee; border-radius:8px;'><p>‚ùå Please run analysis first</p></div>"
        return f"‚ùå {t('run_analysis_first')}", None, None, error_html
    
    progress(0.2, desc="üìÑ Generating Word...")
    
    metrics = VideoMetrics(path=str(STATE.video_path))
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    if STATE.consensus_output is None:
        run_consensus()
    
    metrics_dict = metrics.to_dict()
    consensus_dict = STATE.consensus_output.to_dict() if STATE.consensus_output else {}
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = STATE.work_dir / f"report_{timestamp}.docx"
    
    generate_word_report(
        video_metrics=[metrics_dict],
        consensus=consensus_dict,
        output_path=str(report_path),
        show_screenshots=STATE.config.report.include_screenshots
    )
    
    STATE.report_path = str(report_path)
    
    progress(0.7, desc="üìï Converting PDF...")
    STATE.pdf_path = convert_docx_to_pdf(STATE.report_path)
    
    progress(1.0, desc=t('done'))
    
    status = f"{t('report_generated')}\nüìÑ {report_path.name}"
    
    # Generate PDF preview HTML
    pdf_preview_html = "<div style='text-align:center; padding:40px; background:#f5f5f5; border-radius:8px;'><p>üìÑ PDF conversion not available (requires LibreOffice)</p></div>"
    if STATE.pdf_path:
        status += f"\nüìï {Path(STATE.pdf_path).name}"
        # Create embedded PDF viewer
        pdf_preview_html = f'''
        <div style="width:100%; height:500px; border:1px solid #ddd; border-radius:8px; overflow:hidden;">
            <iframe src="file://{STATE.pdf_path}" width="100%" height="100%" style="border:none;">
                <p>PDF preview not supported. <a href="file://{STATE.pdf_path}" download>Download PDF</a></p>
            </iframe>
        </div>
        <p style="text-align:center; margin-top:10px; color:#666;">
            ‚¨ÜÔ∏è If preview doesn't load, download the PDF file above
        </p>
        '''
    
    return status, STATE.report_path, STATE.pdf_path, pdf_preview_html


def export_json():
    if STATE.video_path is None:
        return f"‚ùå {t('upload_first')}", None, "// Please upload a video first"
    
    data = {
        "timestamp": datetime.now().isoformat(),
        "video_path": str(STATE.video_path),
        "config": STATE.config.to_dict(),
        "visual": STATE.visual_output.to_dict() if STATE.visual_output else None,
        "audio": STATE.audio_output.to_dict() if STATE.audio_output else None,
        "asr": STATE.asr_output.to_dict() if STATE.asr_output else None,
        "yolo": STATE.yolo_output.to_dict() if STATE.yolo_output else None,
        "ai_detection": STATE.ai_output.to_dict() if STATE.ai_output else None,
        "consensus": STATE.consensus_output.to_dict() if STATE.consensus_output else None,
    }
    
    indent = STATE.config.report.json_indent
    json_path = STATE.work_dir / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, ensure_ascii=False, default=str)
    
    # Generate preview (truncated for large data)
    preview_data = {
        "timestamp": data["timestamp"],
        "video_path": data["video_path"],
        "visual": {"...": "see full JSON"} if data["visual"] else None,
        "audio": {"...": "see full JSON"} if data["audio"] else None,
        "asr": {"text_preview": data["asr"]["text"][:200] + "..." if data["asr"] and data["asr"].get("text") else None} if data["asr"] else None,
        "yolo": {"...": "see full JSON"} if data["yolo"] else None,
        "ai_detection": data["ai_detection"] if data["ai_detection"] else None,
        "consensus": {"...": "see full JSON"} if data["consensus"] else None,
    }
    json_preview = json.dumps(preview_data, indent=2, ensure_ascii=False, default=str)
    
    return f"{t('json_exported')}: {json_path.name}", str(json_path), json_preview


def update_config(
    visual_frames, visual_threshold,
    yolo_model, yolo_conf, yolo_frames,
    asr_model, asr_beam,
    ai_enabled, ai_deepfake, ai_clip, ai_temporal, ai_aigc, ai_audio, ai_face,
    ai_threshold, ai_deepfake_weight, ai_clip_weight, ai_temporal_weight, ai_aigc_weight, ai_audio_weight, ai_face_weight
):
    """Update configuration from UI controls"""
    # Visual
    STATE.config.visual.target_frames = int(visual_frames)
    STATE.config.visual.scene_threshold = float(visual_threshold)
    
    # YOLO
    STATE.config.yolo.model_name = yolo_model
    STATE.config.yolo.confidence_threshold = float(yolo_conf)
    STATE.config.yolo.target_frames = int(yolo_frames)
    
    # ASR
    STATE.config.asr.whisper_model = asr_model
    STATE.config.asr.whisper_beam_size = int(asr_beam)
    
    # AI Detection (SOTA 2025/2026)
    STATE.config.ai_detection.enabled = ai_enabled
    STATE.config.ai_detection.use_deepfake = ai_deepfake
    STATE.config.ai_detection.use_clip = ai_clip
    STATE.config.ai_detection.use_temporal = ai_temporal
    STATE.config.ai_detection.use_aigc = ai_aigc
    STATE.config.ai_detection.use_audio_deepfake = ai_audio
    STATE.config.ai_detection.use_face_detection = ai_face
    STATE.config.ai_detection.fake_threshold = float(ai_threshold)
    STATE.config.ai_detection.deepfake_weight = float(ai_deepfake_weight)
    STATE.config.ai_detection.clip_weight = float(ai_clip_weight)
    STATE.config.ai_detection.temporal_weight = float(ai_temporal_weight)
    STATE.config.ai_detection.aigc_weight = float(ai_aigc_weight)
    STATE.config.ai_detection.audio_deepfake_weight = float(ai_audio_weight)
    STATE.config.ai_detection.face_weight = float(ai_face_weight)
    
    # Calculate total weight and show warning if not ~1.0
    total_weight = ai_deepfake_weight + ai_clip_weight + ai_temporal_weight + ai_aigc_weight + ai_audio_weight + ai_face_weight
    
    status = "‚úÖ Configuration updated"
    if abs(total_weight - 1.0) > 0.1:
        status += f"\n‚ö†Ô∏è Warning: Total weight = {total_weight:.2f} (recommended: ~1.0)"
    
    return status


def switch_language(lang: str):
    set_language(lang)
    # Note: Tab labels can't be dynamically updated in Gradio
    # Use --lang zh to start with Chinese interface
    footer_note = t('footer')
    if lang == "zh":
        footer_note += " | üîÑ Âà∑Êñ∞È°µÈù¢‰ª•Êõ¥Êñ∞ÊâÄÊúâÊ†áÁ≠æ"
    else:
        footer_note += " | üîÑ Refresh page for full language switch"
    
    return (
        f"# {t('title')}\n**{t('subtitle')}** | {t('models')}",
        t('analyze_all'),
        t('btn_visual'),
        t('btn_audio'),
        t('btn_asr'),
        t('btn_yolo'),
        t('btn_ai_detect'),
        t('btn_consensus'),
        t('gen_report'),
        t('export_json'),
        footer_note,
    )


# =============================================================================
# Gradio UI
# =============================================================================
def create_ui():
    cfg = STATE.config
    
    with gr.Blocks(title="Video Style Analysis") as demo:
        
        # Header
        header_md = gr.Markdown(f"# {t('title')}\n**{t('subtitle')}** | {t('models')}")
        
        with gr.Row():
            lang_radio = gr.Radio(
                choices=[("English", "en"), ("‰∏≠Êñá", "zh")],
                value="en", label="Language / ËØ≠Ë®Ä", scale=1
            )
        
        gr.Markdown("---")
        
        with gr.Tabs():
            # ========== Tab 1: Upload & Preview ==========
            with gr.Tab(t('tab_upload'), id="tab_upload"):
                with gr.Row():
                    # Left: Upload Section (wider)
                    with gr.Column(scale=2, min_width=400):
                        gr.Markdown("### üì§ Video Upload")
                        gr.Markdown("*Supports MP4, AVI, MOV, MKV. Max 500MB.*")
                        
                        video_input = gr.Video(
                            label="Select Video File",
                            height=320,
                            elem_classes=["video-preview"]
                        )
                        
                        upload_status = gr.Textbox(
                            label="Upload Status",
                            lines=3,
                            interactive=False
                        )
                        
                        gr.Markdown("### ‚öôÔ∏è Analysis Settings")
                        language_select = gr.Dropdown(
                            choices=[("English", "en"), ("‰∏≠Êñá", "zh"), ("Êó•Êú¨Ë™û", "ja"), ("ÌïúÍµ≠Ïñ¥", "ko"), ("Auto-detect", "auto")],
                            value="en",
                            label="Speech Recognition Language"
                        )
                    
                    # Right: Preview Section
                    with gr.Column(scale=3, min_width=500):
                        gr.Markdown("### üé¨ Media Preview")
                        
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("**üîä Extracted Audio**")
                                gr.Markdown("*Audio track separated from video*")
                                audio_player = gr.Audio(
                                    label="Audio Preview",
                                    type="filepath"
                                )
                        
                        gr.Markdown("### üñºÔ∏è Key Frames Gallery")
                        gr.Markdown("*Click any frame to view full size*")
                        frame_gallery = gr.Gallery(
                            label="Extracted Key Frames",
                            columns=4,
                            rows=3,
                            height=280,
                            object_fit="contain",
                            allow_preview=True,
                            preview=True
                        )
            
            # ========== Tab 2: Run Analysis ==========
            with gr.Tab(t('tab_analysis'), id="tab_analysis"):
                gr.Markdown("### üéØ Analysis Controls")
                gr.Markdown("*Click 'Analyze All' for complete analysis, or run individual modules*")
                
                with gr.Row():
                    run_all_btn = gr.Button(
                        "üéØ Analyze All (Recommended)",
                        variant="primary",
                        size="lg",
                        scale=2
                    )
                
                gr.Markdown("**Individual Analysis Modules:**")
                with gr.Row():
                    run_visual_btn = gr.Button(t('btn_visual'), size="sm")
                    run_audio_btn = gr.Button(t('btn_audio'), size="sm")
                    run_asr_btn = gr.Button(t('btn_asr'), size="sm")
                    run_yolo_btn = gr.Button(t('btn_yolo'), size="sm")
                    run_ai_btn = gr.Button(t('btn_ai_detect'), size="sm")
                    run_consensus_btn = gr.Button(t('btn_consensus'), size="sm")
                
                gr.Markdown("---")
                
                # Results Tabs with meaningful names
                with gr.Tabs():
                    with gr.Tab(t('tab_visual'), id="result_visual"):
                        visual_result = gr.Markdown(f"*{t('upload_first')}*")
                        contact_img = gr.Image(
                            label="Contact Sheet",
                            height=200
                        )
                    
                    with gr.Tab(t('tab_audio'), id="result_audio"):
                        audio_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_asr'), id="result_asr"):
                        asr_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_yolo'), id="result_yolo"):
                        yolo_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_ai'), id="result_ai"):
                        ai_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_summary'), id="result_summary"):
                        consensus_result = gr.Markdown(f"*{t('run_analysis_first')}*")
            
            # ========== Tab 3: Export & Reports ==========
            with gr.Tab(t('tab_export'), id="tab_export"):
                gr.Markdown("### üìÑ Export Analysis Results")
                
                with gr.Row():
                    # Left: PDF Report
                    with gr.Column(scale=1):
                        gr.Markdown("#### üìï PDF Report")
                        gen_report_btn = gr.Button("üìÑ Generate Report", variant="primary", size="lg")
                        report_status = gr.Textbox(
                            label="Status",
                            lines=1,
                            interactive=False
                        )
                        
                        gr.Markdown("**Downloads:**")
                        with gr.Row():
                            report_file = gr.File(label="Word (.docx)")
                            pdf_file = gr.File(label="PDF")
                        
                        gr.Markdown("**PDF Preview:**")
                        pdf_preview = gr.HTML(
                            value="<div style='text-align:center; padding:40px; background:#f5f5f5; border-radius:8px;'><p>üìÑ Generate report to preview PDF</p></div>",
                            label="PDF Preview"
                        )
                    
                    # Right: JSON Export
                    with gr.Column(scale=1):
                        gr.Markdown("#### üíæ JSON Data")
                        export_json_btn = gr.Button("üíæ Export JSON", variant="secondary", size="lg")
                        json_status = gr.Textbox(
                            label="Status",
                            lines=1,
                            interactive=False
                        )
                        
                        gr.Markdown("**Download:**")
                        json_file = gr.File(label="JSON Data")
                        
                        gr.Markdown("**JSON Preview:**")
                        json_preview = gr.Code(
                            value="// Generate JSON to preview data",
                            language="json",
                            label="JSON Preview",
                            lines=15
                        )
                
                gr.Markdown("---")
                gr.Markdown("### üìã Quick Summary")
                summary_box = gr.Textbox(
                    label="Analysis Overview",
                    lines=10,
                    interactive=False
                )
            
            # ========== Tab 4: Configuration ==========
            with gr.Tab(t('tab_config'), id="tab_config"):
                gr.Markdown("### ‚öôÔ∏è Analysis Configuration")
                gr.Markdown("*Adjust parameters for each analysis module. Changes apply to next analysis.*")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### üìπ Visual Analysis")
                        gr.Markdown("*More frames = slower but more accurate*")
                        cfg_visual_frames = gr.Slider(
                            10, 200, value=cfg.visual.target_frames, step=10,
                            label="Target Frames"
                        )
                        cfg_visual_threshold = gr.Slider(
                            10, 50, value=cfg.visual.scene_threshold, step=1,
                            label="Scene Threshold (lower = more sensitive)"
                        )
                        
                        gr.Markdown("#### üîç YOLO Object Detection")
                        gr.Markdown("*n=fastest, l=most accurate*")
                        cfg_yolo_model = gr.Dropdown(
                            choices=["yolo11n.pt", "yolo11s.pt", "yolo11m.pt", "yolo11l.pt"],
                            value=cfg.yolo.model_name,
                            label="YOLO Model"
                        )
                        cfg_yolo_conf = gr.Slider(
                            0.1, 0.9, value=cfg.yolo.confidence_threshold, step=0.05,
                            label="Confidence Threshold"
                        )
                        cfg_yolo_frames = gr.Slider(
                            10, 100, value=cfg.yolo.target_frames, step=5,
                            label="Frames to Analyze"
                        )
                        
                        gr.Markdown("#### üé§ Speech Recognition (ASR)")
                        gr.Markdown("*large-v3-turbo = best, tiny = fastest*")
                        cfg_asr_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large-v3", "large-v3-turbo"],
                            value=cfg.asr.whisper_model,
                            label="Whisper Model"
                        )
                        cfg_asr_beam = gr.Slider(
                            1, 10, value=cfg.asr.whisper_beam_size, step=1,
                            label="Beam Size (higher = better)"
                        )
                    
                    with gr.Column():
                        gr.Markdown("#### ü§ñ AI Detection Models")
                        gr.Markdown("*‚≠ê = HuggingFace pretrained (high reliability)*")
                        
                        cfg_ai_enabled = gr.Checkbox(
                            value=cfg.ai_detection.enabled,
                            label="Enable AI Detection"
                        )
                        
                        gr.Markdown("**Detection Models:**")
                        cfg_ai_deepfake = gr.Checkbox(
                            value=cfg.ai_detection.use_deepfake,
                            label="‚≠ê DeepFake-v2 (ViT, 92% acc)"
                        )
                        cfg_ai_clip = gr.Checkbox(
                            value=cfg.ai_detection.use_clip,
                            label="‚≠ê CLIP Zero-Shot"
                        )
                        cfg_ai_temporal = gr.Checkbox(
                            value=cfg.ai_detection.use_temporal,
                            label="‚úÖ CLIP-Temporal Analysis"
                        )
                        cfg_ai_aigc = gr.Checkbox(
                            value=cfg.ai_detection.use_aigc,
                            label="‚≠ê AIGC Detector (SD/DALL-E/MJ)"
                        )
                        cfg_ai_audio = gr.Checkbox(
                            value=cfg.ai_detection.use_audio_deepfake,
                            label="‚≠ê Audio Deepfake"
                        )
                        cfg_ai_face = gr.Checkbox(
                            value=cfg.ai_detection.use_face_detection,
                            label="Face Analysis"
                        )
                        
                        cfg_ai_threshold = gr.Slider(
                            0.1, 0.9, value=cfg.ai_detection.fake_threshold, step=0.05,
                            label="AI Threshold (above = flagged)"
                        )
                        
                        gr.Markdown("#### ‚öñÔ∏è Ensemble Weights")
                        gr.Markdown("*Higher weight = more influence on final score. Total should ‚âà 1.0*")
                        
                        cfg_ai_deepfake_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.deepfake_weight, step=0.05,
                            label="DeepFake-v2 Weight (‚≠ê HuggingFace)"
                        )
                        cfg_ai_clip_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.clip_weight, step=0.05,
                            label="CLIP Synthetic Weight (‚≠ê HuggingFace)"
                        )
                        cfg_ai_temporal_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.temporal_weight, step=0.05,
                            label="CLIP-Temporal Weight (‚úÖ Computed)"
                        )
                        cfg_ai_aigc_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.aigc_weight, step=0.05,
                            label="AIGC Detector Weight (‚≠ê HuggingFace)"
                        )
                        cfg_ai_audio_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.audio_deepfake_weight, step=0.05,
                            label="Audio Deepfake Weight (‚≠ê HuggingFace)"
                        )
                        cfg_ai_face_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.face_weight, step=0.05,
                            label="Face Analysis Weight"
                        )
                
                gr.Markdown("---")
                config_status = gr.Textbox(label="Status", interactive=False)
                save_config_btn = gr.Button("üíæ Save Configuration", variant="primary", size="lg")
                
                save_config_btn.click(
                    fn=update_config,
                    inputs=[
                        cfg_visual_frames, cfg_visual_threshold,
                        cfg_yolo_model, cfg_yolo_conf, cfg_yolo_frames,
                        cfg_asr_model, cfg_asr_beam,
                        cfg_ai_enabled, cfg_ai_deepfake, cfg_ai_clip, cfg_ai_temporal, cfg_ai_aigc, cfg_ai_audio, cfg_ai_face,
                        cfg_ai_threshold, cfg_ai_deepfake_weight, cfg_ai_clip_weight, cfg_ai_temporal_weight, cfg_ai_aigc_weight, cfg_ai_audio_weight, cfg_ai_face_weight
                    ],
                    outputs=[config_status]
                )
        
        # Footer
        footer_md = gr.Markdown(f"---\n{t('footer')}")
        
        # ========== Event Handlers ==========
        video_input.change(fn=upload_video, inputs=[video_input],
                          outputs=[upload_status, audio_player, frame_gallery])
        
        run_visual_btn.click(fn=run_visual, outputs=[visual_result, contact_img])
        run_audio_btn.click(fn=run_audio, outputs=[audio_result])
        run_asr_btn.click(fn=run_asr, inputs=[language_select], outputs=[asr_result])
        run_yolo_btn.click(fn=run_yolo, outputs=[yolo_result])
        run_ai_btn.click(fn=run_ai_detection, outputs=[ai_result])
        run_consensus_btn.click(fn=run_consensus, outputs=[consensus_result])
        
        run_all_btn.click(
            fn=run_all,
            inputs=[language_select],
            outputs=[visual_result, contact_img, audio_result, asr_result,
                     yolo_result, ai_result, consensus_result, summary_box]
        )
        
        gen_report_btn.click(fn=gen_report, outputs=[report_status, report_file, pdf_file, pdf_preview])
        export_json_btn.click(fn=export_json, outputs=[json_status, json_file, json_preview])
        
        lang_radio.change(
            fn=switch_language,
            inputs=[lang_radio],
            outputs=[header_md, run_all_btn, run_visual_btn, run_audio_btn,
                     run_asr_btn, run_yolo_btn, run_ai_btn, run_consensus_btn,
                     gen_report_btn, export_json_btn, footer_md]
        )
    
    return demo


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Video Style Analysis Web UI")
    parser.add_argument("--port", type=int, default=8088, help="Server port")
    parser.add_argument("--share", action="store_true", help="Create public link")
    parser.add_argument("--lang", type=str, default="en", choices=["en", "zh"], help="Default language")
    args = parser.parse_args()
    
    set_language(args.lang)
    
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share,
        show_error=True
    )
