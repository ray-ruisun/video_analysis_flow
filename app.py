#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Video Style Analysis - Gradio Web Interface
SOTA Models: CLIP | CLAP | HuBERT | Whisper | YOLO11 | Deep-Fake-Detector-v2
"""

import sys
import json
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, Dict, Any, List
from dataclasses import dataclass, field

import gradio as gr
import numpy as np
import cv2

sys.path.insert(0, str(Path(__file__).parent / "src"))

from config import PipelineConfig, get_default_config
from steps import (
    VisualAnalysisStep, AudioAnalysisStep, ASRAnalysisStep,
    YOLOAnalysisStep, ConsensusStep, AIDetectionStep,
    VideoInput, AudioInput, ASRInput, YOLOInput, ConsensusInput, AIDetectionInput,
    VideoMetrics, VisualOutput, AudioOutput, ASROutput, YOLOOutput, ConsensusOutput, AIDetectionOutput,
)
from report_word import generate_word_report

# =============================================================================
# Internationalization (i18n)
# =============================================================================
TRANSLATIONS = {
    "en": {
        "title": "ğŸ¬ Video Style Analysis",
        "subtitle": "SOTA 2025/2026 | PyTorch + HuggingFace",
        "models": "CLIP Â· CLAP Â· HuBERT Â· Whisper Â· YOLO11 Â· DeepFake-v2",
        "upload_section": "ğŸ“¤ Upload",
        "settings_section": "âš™ï¸ Settings",
        "preview_section": "ğŸ¬ Preview",
        "control_section": "ğŸš€ Analysis",
        "results_section": "ğŸ“Š Results",
        "export_section": "ğŸ“¥ Export",
        "config_section": "ğŸ”§ Configuration",
        "select_video": "Select Video (mp4, avi, mov, mkv)",
        "status": "Status",
        "asr_language": "ASR Language",
        "audio_preview": "Extracted Audio",
        "keyframes": "Key Frames",
        "analyze_all": "ğŸ¯ Analyze All",
        "analyze_current": "ğŸ¯ Analyze Current Video",
        "analyze_batch": "ğŸ“ˆ Analyze All Videos (Cross-Video)",
        "batch_hint": "*Use 'Analyze Current' for single video, 'Analyze All' for multi-video comparison*",
        "btn_visual": "ğŸ“¹ Camera & Color",
        "btn_audio": "ğŸµ BGM & Tempo",
        "btn_asr": "ğŸ¤ Speech & Emotion",
        "btn_yolo": "ğŸ” Objects",
        "btn_consensus": "ğŸ“Š Summary",
        "btn_ai_detect": "ğŸ¤– AI Detect",
        "gen_report": "ğŸ“„ Report",
        "export_json": "ğŸ’¾ JSON",
        "tab_upload": "ğŸ“¤ Upload & Preview",
        "tab_analysis": "ğŸš€ Run Analysis",
        "tab_export": "ğŸ“¥ Export & Reports",
        "tab_config": "âš™ï¸ Settings & Weights",
        "tab_visual": "ğŸ“¹ Camera & Color",
        "tab_audio": "ğŸµ BGM & Tempo",
        "tab_asr": "ğŸ¤ Speech & Emotion",
        "tab_yolo": "ğŸ” Objects & Materials",
        "tab_summary": "ğŸ“Š Analysis Summary",
        "video_list": "Video List",
        "add_video": "â• Add Video",
        "clear_all": "ğŸ—‘ï¸ Clear All",
        "single_video_mode": "Single Video Analysis",
        "multi_video_mode": "Cross-Video Comparison ({n} videos)",
        "no_videos": "No videos added",
        "video_n": "Video {n}",
        "select_video": "Select video to view",
        "comparison_chart": "Comparison Chart",
        "tab_ai": "ğŸ¤– AI/Deepfake Detection",
        "report_status": "Report Status",
        "word_report": "Word Report",
        "pdf_report": "PDF Report",
        "json_data": "JSON Data",
        "json_status": "JSON Status",
        "quick_summary": "Quick Summary",
        "upload_first": "Please upload a video first",
        "run_analysis_first": "Please run analysis first",
        "uploaded": "âœ… Uploaded",
        "workdir": "Work Directory",
        "frames_extracted": "Extracted {n} frames",
        "audio_extracted": "âœ… Audio extracted",
        "audio_failed": "âš ï¸ Audio extraction failed",
        "analysis_failed": "Analysis failed",
        "report_generated": "âœ… Report generated",
        "json_exported": "âœ… JSON exported",
        "loading": "Loading",
        "analyzing": "Analyzing",
        "done": "âœ… Done",
        "footer": "Video Style Analysis | SOTA 2025/2026",
        # Config labels
        "visual_frames": "Visual: Target Frames",
        "visual_scene_threshold": "Visual: Scene Threshold",
        "audio_sample_rate": "Audio: Sample Rate",
        "asr_model": "ASR: Whisper Model",
        "asr_beam_size": "ASR: Beam Size",
        "yolo_model": "YOLO: Model",
        "yolo_conf": "YOLO: Confidence",
        "yolo_frames": "YOLO: Target Frames",
        "ai_enabled": "AI Detection: Enabled",
        "ai_video_model": "AI: Video Model",
        "ai_video_threshold": "AI: Video Threshold",
        "ai_frame_enabled": "AI: Frame Detection",
        "ai_frame_threshold": "AI: Frame Threshold",
        "ai_face_enabled": "AI: Face Detection",
        "ai_video_weight": "AI: Video Weight",
        "ai_frame_weight": "AI: Frame Weight",
        "ai_face_weight": "AI: Face Weight",
        # Results
        "visual_results": "Visual Analysis Results",
        "audio_results": "Audio Analysis Results",
        "asr_results": "Speech Analysis Results",
        "yolo_results": "Object Detection Results",
        "summary_results": "Summary Results",
        "ai_results": "AI Detection Results",
        "verdict": "Verdict",
        "confidence": "Confidence",
        "real": "Real",
        "deepfake": "Deepfake",
        "synthetic": "Synthetic",
        "suspicious": "Suspicious",
    },
    "zh": {
        "title": "ğŸ¬ è§†é¢‘é£æ ¼åˆ†æç³»ç»Ÿ",
        "subtitle": "SOTA 2025/2026 | PyTorch + HuggingFace",
        "models": "CLIP Â· CLAP Â· HuBERT Â· Whisper Â· YOLO11 Â· DeepFake-v2",
        "upload_section": "ğŸ“¤ ä¸Šä¼ ",
        "settings_section": "âš™ï¸ è®¾ç½®",
        "preview_section": "ğŸ¬ é¢„è§ˆ",
        "control_section": "ğŸš€ åˆ†æ",
        "results_section": "ğŸ“Š ç»“æœ",
        "export_section": "ğŸ“¥ å¯¼å‡º",
        "config_section": "ğŸ”§ å‚æ•°é…ç½®",
        "select_video": "é€‰æ‹©è§†é¢‘ (mp4, avi, mov, mkv)",
        "status": "çŠ¶æ€",
        "asr_language": "è¯­éŸ³è¯†åˆ«è¯­è¨€",
        "audio_preview": "æå–çš„éŸ³é¢‘",
        "keyframes": "å…³é”®å¸§",
        "analyze_all": "ğŸ¯ ä¸€é”®åˆ†æ",
        "analyze_current": "ğŸ¯ åˆ†æå½“å‰è§†é¢‘",
        "analyze_batch": "ğŸ“ˆ åˆ†æå…¨éƒ¨è§†é¢‘ (è·¨è§†é¢‘å¯¹æ¯”)",
        "batch_hint": "*å•è§†é¢‘åˆ†æä½¿ç”¨'åˆ†æå½“å‰'ï¼Œå¤šè§†é¢‘å¯¹æ¯”ä½¿ç”¨'åˆ†æå…¨éƒ¨'*",
        "btn_visual": "ğŸ“¹ é•œå¤´è‰²å½©",
        "btn_audio": "ğŸµ èƒŒæ™¯éŸ³ä¹",
        "btn_asr": "ğŸ¤ è¯­éŸ³æƒ…æ„Ÿ",
        "btn_yolo": "ğŸ” ç‰©ä½“æ£€æµ‹",
        "btn_consensus": "ğŸ“Š ç»¼åˆæ±‡æ€»",
        "btn_ai_detect": "ğŸ¤– AIæ£€æµ‹",
        "gen_report": "ğŸ“„ ç”ŸæˆæŠ¥å‘Š",
        "export_json": "ğŸ’¾ å¯¼å‡ºJSON",
        "tab_upload": "ğŸ“¤ ä¸Šä¼ ä¸é¢„è§ˆ",
        "tab_analysis": "ğŸš€ è¿è¡Œåˆ†æ",
        "tab_export": "ğŸ“¥ å¯¼å‡ºä¸æŠ¥å‘Š",
        "tab_config": "âš™ï¸ å‚æ•°è®¾ç½®",
        "tab_visual": "ğŸ“¹ é•œå¤´ä¸è‰²å½©",
        "tab_audio": "ğŸµ èƒŒæ™¯éŸ³ä¹ä¸èŠ‚å¥",
        "tab_asr": "ğŸ¤ è¯­éŸ³ä¸æƒ…æ„Ÿ",
        "tab_yolo": "ğŸ” ç‰©ä½“ä¸æè´¨",
        "tab_summary": "ğŸ“Š ç»¼åˆæ±‡æ€»",
        "video_list": "è§†é¢‘åˆ—è¡¨",
        "add_video": "â• æ·»åŠ è§†é¢‘",
        "clear_all": "ğŸ—‘ï¸ æ¸…ç©ºå…¨éƒ¨",
        "single_video_mode": "å•è§†é¢‘åˆ†æ",
        "multi_video_mode": "è·¨è§†é¢‘å¯¹æ¯” ({n} ä¸ªè§†é¢‘)",
        "no_videos": "æœªæ·»åŠ è§†é¢‘",
        "video_n": "è§†é¢‘ {n}",
        "select_video": "é€‰æ‹©è§†é¢‘æŸ¥çœ‹",
        "comparison_chart": "å¯¹æ¯”å›¾è¡¨",
        "tab_ai": "ğŸ¤– AIç”Ÿæˆæ£€æµ‹",
        "report_status": "æŠ¥å‘ŠçŠ¶æ€",
        "word_report": "Word æŠ¥å‘Š",
        "pdf_report": "PDF æŠ¥å‘Š",
        "json_data": "JSON æ•°æ®",
        "json_status": "JSON çŠ¶æ€",
        "quick_summary": "å¿«é€Ÿæ‘˜è¦",
        "upload_first": "è¯·å…ˆä¸Šä¼ è§†é¢‘",
        "run_analysis_first": "è¯·å…ˆè¿è¡Œåˆ†æ",
        "uploaded": "âœ… å·²ä¸Šä¼ ",
        "workdir": "å·¥ä½œç›®å½•",
        "frames_extracted": "å·²æå– {n} å¸§",
        "audio_extracted": "âœ… éŸ³é¢‘å·²æå–",
        "audio_failed": "âš ï¸ éŸ³é¢‘æå–å¤±è´¥",
        "analysis_failed": "åˆ†æå¤±è´¥",
        "report_generated": "âœ… æŠ¥å‘Šå·²ç”Ÿæˆ",
        "json_exported": "âœ… JSON å·²å¯¼å‡º",
        "loading": "åŠ è½½ä¸­",
        "analyzing": "åˆ†æä¸­",
        "done": "âœ… å®Œæˆ",
        "footer": "è§†é¢‘é£æ ¼åˆ†æ | SOTA 2025/2026",
        # Config labels
        "visual_frames": "è§†è§‰: ç›®æ ‡å¸§æ•°",
        "visual_scene_threshold": "è§†è§‰: åœºæ™¯é˜ˆå€¼",
        "audio_sample_rate": "éŸ³é¢‘: é‡‡æ ·ç‡",
        "asr_model": "ASR: Whisper æ¨¡å‹",
        "asr_beam_size": "ASR: Beam Size",
        "yolo_model": "YOLO: æ¨¡å‹",
        "yolo_conf": "YOLO: ç½®ä¿¡åº¦",
        "yolo_frames": "YOLO: ç›®æ ‡å¸§æ•°",
        "ai_enabled": "AIæ£€æµ‹: å¯ç”¨",
        "ai_video_model": "AI: è§†é¢‘æ¨¡å‹",
        "ai_video_threshold": "AI: è§†é¢‘é˜ˆå€¼",
        "ai_frame_enabled": "AI: å¸§æ£€æµ‹",
        "ai_frame_threshold": "AI: å¸§é˜ˆå€¼",
        "ai_face_enabled": "AI: äººè„¸æ£€æµ‹",
        "ai_video_weight": "AI: è§†é¢‘æƒé‡",
        "ai_frame_weight": "AI: å¸§æƒé‡",
        "ai_face_weight": "AI: äººè„¸æƒé‡",
        # Results
        "visual_results": "è§†è§‰åˆ†æç»“æœ",
        "audio_results": "éŸ³é¢‘åˆ†æç»“æœ",
        "asr_results": "è¯­éŸ³åˆ†æç»“æœ",
        "yolo_results": "ç›®æ ‡æ£€æµ‹ç»“æœ",
        "summary_results": "æ±‡æ€»ç»“æœ",
        "ai_results": "AIç”Ÿæˆæ£€æµ‹ç»“æœ",
        "verdict": "åˆ¤å®š",
        "confidence": "ç½®ä¿¡åº¦",
        "real": "çœŸå®",
        "deepfake": "æ·±åº¦ä¼ªé€ ",
        "synthetic": "åˆæˆ",
        "suspicious": "å¯ç–‘",
    }
}

LANG = "en"

def t(key: str) -> str:
    return TRANSLATIONS.get(LANG, TRANSLATIONS["en"]).get(key, key)

def set_language(lang: str):
    global LANG
    LANG = lang if lang in TRANSLATIONS else "en"


# =============================================================================
# Global State
# =============================================================================
@dataclass
class VideoAnalysis:
    """Single video analysis result container"""
    video_path: Optional[Path] = None
    audio_path: Optional[Path] = None
    work_dir: Optional[Path] = None
    visual_output: Optional[VisualOutput] = None
    audio_output: Optional[AudioOutput] = None
    asr_output: Optional[ASROutput] = None
    yolo_output: Optional[YOLOOutput] = None
    ai_output: Optional[AIDetectionOutput] = None
    
    def to_metrics(self) -> VideoMetrics:
        """Convert to VideoMetrics for consensus calculation"""
        metrics = VideoMetrics(path=str(self.video_path) if self.video_path else "")
        metrics.visual = self.visual_output
        metrics.audio = self.audio_output
        metrics.asr = self.asr_output
        metrics.yolo = self.yolo_output
        return metrics


class AnalysisState:
    def __init__(self):
        self.reset()
        self.config = get_default_config()
    
    def reset(self):
        # Multi-video support
        self.videos: List[VideoAnalysis] = []
        self.current_index: int = 0  # Currently selected video
        
        # Legacy single-video properties (for backward compatibility)
        self.video_path: Optional[Path] = None
        self.audio_path: Optional[Path] = None
        self.work_dir: Optional[Path] = None
        self.visual_output: Optional[VisualOutput] = None
        self.audio_output: Optional[AudioOutput] = None
        self.asr_output: Optional[ASROutput] = None
        self.yolo_output: Optional[YOLOOutput] = None
        self.consensus_output: Optional[ConsensusOutput] = None
        self.ai_output: Optional[AIDetectionOutput] = None
        self.report_path: Optional[str] = None
        self.pdf_path: Optional[str] = None
    
    def add_video(self, video_path: Path, work_dir: Path) -> int:
        """Add a new video for analysis, returns index"""
        analysis = VideoAnalysis(video_path=video_path, work_dir=work_dir)
        self.videos.append(analysis)
        return len(self.videos) - 1
    
    def get_current(self) -> Optional[VideoAnalysis]:
        """Get current video analysis"""
        if 0 <= self.current_index < len(self.videos):
            return self.videos[self.current_index]
        return None
    
    def sync_current_to_legacy(self):
        """Sync current video to legacy single-video properties"""
        current = self.get_current()
        if current:
            self.video_path = current.video_path
            self.audio_path = current.audio_path
            self.work_dir = current.work_dir
            self.visual_output = current.visual_output
            self.audio_output = current.audio_output
            self.asr_output = current.asr_output
            self.yolo_output = current.yolo_output
            self.ai_output = current.ai_output
    
    def sync_legacy_to_current(self):
        """Sync legacy properties back to current video"""
        current = self.get_current()
        if current:
            current.visual_output = self.visual_output
            current.audio_output = self.audio_output
            current.asr_output = self.asr_output
            current.yolo_output = self.yolo_output
            current.ai_output = self.ai_output
    
    def get_all_metrics(self) -> List[VideoMetrics]:
        """Get all video metrics for consensus"""
        return [v.to_metrics() for v in self.videos if v.visual_output or v.audio_output]
    
    def get_video_count(self) -> int:
        """Get number of videos"""
        return len(self.videos)
    
    def get_analyzed_count(self) -> int:
        """Get number of analyzed videos"""
        return sum(1 for v in self.videos if v.visual_output or v.audio_output)

STATE = AnalysisState()


# =============================================================================
# Utility Functions
# =============================================================================
def extract_audio_from_video(video_path: Path, output_dir: Path) -> Optional[Path]:
    output_path = output_dir / f"{video_path.stem}_audio.wav"
    if output_path.exists():
        return output_path
    try:
        sr = STATE.config.audio.sample_rate
        cmd = ["ffmpeg", "-y", "-i", str(video_path), "-vn", "-acodec", "pcm_s16le",
               "-ar", str(sr), "-ac", "1", str(output_path)]
        subprocess.run(cmd, capture_output=True, check=True)
        return output_path
    except Exception:
        return None


def extract_frames_for_gallery(video_path: Path, output_dir: Path, num_frames: int = 12) -> List[str]:
    frames_dir = output_dir / "frames"
    frames_dir.mkdir(exist_ok=True)
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    if total_frames == 0:
        cap.release()
        return []
    
    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    frame_paths = []
    
    for i, idx in enumerate(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            timestamp = idx / fps if fps > 0 else 0
            frame_path = frames_dir / f"frame_{i:03d}_{timestamp:.1f}s.jpg"
            cv2.imwrite(str(frame_path), frame)
            frame_paths.append(str(frame_path))
    
    cap.release()
    return frame_paths


def convert_docx_to_pdf(docx_path: str) -> Optional[str]:
    pdf_path = docx_path.replace('.docx', '.pdf')
    try:
        timeout = STATE.config.report.pdf_timeout
        cmd = ["libreoffice", "--headless", "--convert-to", "pdf",
               "--outdir", str(Path(docx_path).parent), docx_path]
        subprocess.run(cmd, capture_output=True, check=True, timeout=timeout)
        if Path(pdf_path).exists():
            return pdf_path
    except Exception:
        pass
    return None


# =============================================================================
# Result Formatters
# =============================================================================
def format_visual(output: VisualOutput) -> str:
    if not output or not output.success:
        return f"âŒ {t('analysis_failed')}"
    
    scenes = "\n".join([
        f"  â€¢ {s.get('label', '?')}: **{s.get('probability', 0):.1%}**"
        for s in output.scene_categories[:5]
    ])
    
    # CCT interpretation
    cct = output.cct_mean
    if cct < 3500:
        cct_desc = "Warm (incandescent/sunset)"
    elif cct < 5500:
        cct_desc = "Neutral (daylight balanced)"
    else:
        cct_desc = "Cool (overcast/blue hour)"
    
    # Cut rate interpretation
    cut_rate = output.cuts / max(output.duration, 1) * 60  # cuts per minute
    if cut_rate > 30:
        pace_desc = "Very fast editing"
    elif cut_rate > 15:
        pace_desc = "Dynamic editing"
    elif cut_rate > 5:
        pace_desc = "Moderate pacing"
    else:
        pace_desc = "Slow, contemplative"
    
    return f"""## ğŸ“¹ Visual Analysis Results

### ğŸ“Š Video Info

| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Duration** | {output.duration:.2f}s | Total video length |
| **FPS** | {output.fps:.1f} | Frames per second |
| **Frames Analyzed** | {output.sampled_frames} | Sample size for analysis |

### ğŸ“· Camera & Composition

| Metric | Value | Description |
|:-------|:------|:------------|
| **Camera Angle** | {output.camera_angle} | Viewer perspective (eye-level, overhead, low) |
| **Focal Length** | {output.focal_length_tendency} | Wide-angle, normal, or telephoto |

### ğŸ¨ Color Analysis

| Metric | Value | Description |
|:-------|:------|:------------|
| **Dominant Hue** | {output.hue_family} | Primary color family |
| **Saturation** | {output.saturation_band} | Color intensity (vivid/muted) |
| **Brightness** | {output.brightness_band} | Light/dark overall |
| **Contrast** | {output.contrast} | Dynamic range |
| **Color Temp (CCT)** | {output.cct_mean:.0f}K | {cct_desc} |

### âœ‚ï¸ Editing Pace

| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Total Cuts** | {output.cuts} | Scene transitions detected |
| **Cuts/Minute** | {cut_rate:.1f} | {pace_desc} |

### ğŸ  Scene Classification (CLIP)
*Top detected scene types:*
{scenes}

---
*Scene classification powered by CLIP (openai/clip-vit-large-patch14)*
"""


def format_audio(output: AudioOutput) -> str:
    if not output or not output.success:
        return f"âŒ {t('analysis_failed')}"
    
    instruments = output.instruments.get('detected_instruments', [])
    inst_str = ", ".join(instruments[:5]) if instruments else "N/A"
    
    # Add explanations based on values
    tempo_desc = "Fast-paced" if output.tempo_bpm > 120 else "Medium tempo" if output.tempo_bpm > 80 else "Slow, relaxed"
    percussive_desc = "Heavy drums" if output.percussive_ratio > 0.5 else "Moderate beats" if output.percussive_ratio > 0.2 else "Light rhythm"
    
    return f"""## ğŸµ Audio Analysis Results

### ğŸ’“ Rhythm & Tempo

| Metric | Value | Interpretation |
|:-------|:-----:|:---------------|
| **BPM** | {output.tempo_bpm:.1f} | {tempo_desc} |
| **Beat Count** | {output.num_beats} | Total rhythmic beats detected |
| **Percussive Ratio** | {output.percussive_ratio:.2f} | {percussive_desc} |

### ğŸ¸ Music Classification (CLAP Model)

| Metric | Value | Description |
|:-------|:------|:------------|
| **BGM Style** | {output.bgm_style} | Genre/style of background music |
| **Mood** | {output.mood} | Emotional tone of the audio |
| **Key** | {output.key_signature} | Musical key (if detected) |

### ğŸ¹ Instruments Detected
{inst_str}

---
*Analysis powered by CLAP (laion/larger_clap_music_and_speech)*
"""


def format_asr(output: ASROutput) -> str:
    if not output or not output.success:
        return f"âŒ {t('analysis_failed')}"
    
    text_preview = output.text[:500] + '...' if len(output.text) > 500 else output.text
    
    # WPM interpretation
    wpm = output.words_per_minute
    if wpm > 160:
        pace_desc = "Very fast (energetic/urgent)"
    elif wpm > 130:
        pace_desc = "Fast (conversational+)"
    elif wpm > 100:
        pace_desc = "Normal conversation"
    elif wpm > 60:
        pace_desc = "Slow (deliberate/clear)"
    else:
        pace_desc = "Very slow (emphatic)"
    
    # Emotion section
    emotion_section = ""
    if output.emotion:
        emo = output.emotion.get('dominant_emotion', 'N/A')
        conf = output.emotion.get('confidence', 0)
        emotion_section = f"""
### ğŸ­ Emotion Analysis (HuBERT Model)
| Detected Emotion | Confidence |
|:----------------:|:----------:|
| **{emo}** | {conf:.1%} |
"""
    
    # Prosody section
    prosody_section = ""
    if output.prosody:
        pitch = output.prosody.get('mean_pitch_hz', 0)
        style = output.prosody.get('prosody_style', 'N/A')
        intensity = output.prosody.get('mean_intensity', 0)
        prosody_section = f"""
### ğŸ“Š Prosody Analysis (Librosa)
| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Pitch** | {pitch:.1f} Hz | Average fundamental frequency |
| **Style** | {style} | Speaking manner |
| **Intensity** | {intensity:.1f} dB | Volume level |
"""
    
    return f"""## ğŸ¤ Speech Analysis Results

### ğŸ—£ï¸ Speech Rate

| Metric | Value | Interpretation |
|:-------|:-----:|:---------------|
| **Total Words** | {output.num_words} | Words transcribed |
| **Words/Min (WPM)** | {wpm:.1f} | {pace_desc} |
| **Pace Category** | {output.pace} | Overall speaking speed |

{emotion_section}
{prosody_section}

### ğŸ“ Transcript (Whisper large-v3-turbo)
```
{text_preview}
```

---
*ASR powered by faster-whisper, Emotion by HuBERT*
"""


def format_yolo(output: YOLOOutput) -> str:
    if not output or not output.success:
        return f"âŒ {t('analysis_failed')}"
    
    detection = output.detection
    environment = output.environment
    object_counts = detection.get('object_counts', {})
    
    objects_str = "\n".join([
        f"| {obj} | {cnt} |"
        for obj, cnt in sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    ])
    
    # Colors section
    colors_section = ""
    if output.colors:
        colors = output.colors
        if isinstance(colors, dict):
            dom_colors = colors.get('dominant_colors', colors.get('all_colors', []))
            if dom_colors and isinstance(dom_colors, list):
                colors_str = ", ".join(dom_colors[:5]) if dom_colors else "N/A"
                colors_section = f"""
### ğŸ¨ Object Colors
**Dominant Colors**: {colors_str}
"""
    
    # Materials section
    materials_section = ""
    if output.materials:
        mats = output.materials
        if isinstance(mats, dict):
            dom_mats = mats.get('dominant_materials', mats.get('all_materials', []))
            if dom_mats and isinstance(dom_mats, list):
                mats_str = ", ".join(dom_mats[:5]) if dom_mats else "N/A"
                materials_section = f"""
### ğŸ§± Materials Detected
**Dominant Materials**: {mats_str}
"""
    
    return f"""## ğŸ” Object Detection Results

### ğŸ  Environment Classification

| Metric | Value | Description |
|:-------|:------|:------------|
| **Environment Type** | {environment.get('environment_type', 'N/A')} | Primary scene category |
| **Activity Style** | {environment.get('cooking_style', 'N/A')} | Detected activity type |

### ğŸ“¦ Object Detection Statistics

| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Unique Objects** | {detection.get('unique_objects', 0)} | Different object types found |
| **Total Detections** | {detection.get('total_detections', 0)} | Total instances across frames |

### ğŸ“‹ Detected Objects (Top 10)

| Object | Count |
|:-------|:-----:|
{objects_str}

{colors_section}
{materials_section}

---
*Detection powered by YOLO11 (ultralytics)*
"""


def format_consensus(output: ConsensusOutput, video_count: int = 1) -> str:
    """Format consensus output - adapts based on single vs multi-video mode"""
    if not output or not output.success:
        return f"âŒ {t('analysis_failed')}"
    
    cct_str = f"{output.cct:.0f}K" if output.cct else "N/A"
    bpm_str = f"{output.tempo_bpm:.1f}" if output.tempo_bpm else "N/A"
    cuts_str = f"{output.cuts_per_minute:.1f}" if output.cuts_per_minute else "N/A"
    shot_str = f"{output.avg_shot_length:.2f}s" if output.avg_shot_length else "N/A"
    
    # Determine mode
    if video_count <= 1:
        # Single video mode - Analysis Summary
        mode_title = t('single_video_mode')
        mode_icon = "ğŸ“Š"
        comparison_note = ""
    else:
        # Multi-video mode - Cross-Video Comparison  
        mode_title = t('multi_video_mode').format(n=video_count)
        mode_icon = "ğŸ“ˆ"
        comparison_note = f"\n> ğŸ“Š *Aggregated from {video_count} videos using majority voting (categorical) and median (numerical)*\n"
    
    # Build distribution details for categorical metrics
    def format_distribution(detail: Dict) -> str:
        if not detail or not detail.get('distribution'):
            return ""
        dist = detail.get('distribution', [])
        if len(dist) <= 1:
            return ""
        items = [f"`{d['value']}` ({d['percentage']:.0f}%)" for d in dist[:3]]
        return " | ".join(items)
    
    # Camera distribution
    camera_dist = format_distribution(getattr(output, 'camera_angle_detail', None))
    camera_row = f"| **Camera Angle** | {output.camera_angle} | {camera_dist if camera_dist else 'â€”'} |"
    
    # Hue distribution
    hue_dist = format_distribution(getattr(output, 'hue_detail', None))
    hue_row = f"| **Hue Family** | {output.hue_family} | {hue_dist if hue_dist else 'â€”'} |"
    
    # Saturation distribution
    sat_dist = format_distribution(getattr(output, 'saturation_detail', None))
    sat_row = f"| **Saturation** | {output.saturation} | {sat_dist if sat_dist else 'â€”'} |"
    
    # Brightness distribution
    bright_dist = format_distribution(getattr(output, 'brightness_detail', None))
    bright_row = f"| **Brightness** | {output.brightness} | {bright_dist if bright_dist else 'â€”'} |"
    
    # Scene distribution
    scene_dist = format_distribution(getattr(output, 'scene_category_detail', None))
    scene_row = f"| **Scene** | {output.scene_category} | {scene_dist if scene_dist else 'â€”'} |"
    
    # BGM Style distribution
    bgm_dist = format_distribution(getattr(output, 'bgm_style_detail', None))
    bgm_row = f"| **BGM Style** | {output.bgm_style} | {bgm_dist if bgm_dist else 'â€”'} |"
    
    # Mood distribution
    mood_dist = format_distribution(getattr(output, 'bgm_mood_detail', None))
    mood_row = f"| **Mood** | {output.bgm_mood} | {mood_dist if mood_dist else 'â€”'} |"
    
    # YOLO section
    yolo_section = ""
    if getattr(output, 'yolo_available', False):
        yolo_env = getattr(output, 'yolo_environment', 'N/A')
        yolo_style = getattr(output, 'yolo_style', 'N/A')
        yolo_section = f"""
### ğŸ” Object Detection Summary
| Metric | Value |
|:-------|:------|
| Environment | {yolo_env} |
| Activity | {yolo_style} |
"""
    
    # Beat alignment
    beat_section = ""
    if output.beat_alignment is not None:
        beat_pct = output.beat_alignment * 100
        beat_icon = "ğŸ¯" if beat_pct > 50 else "ğŸ“"
        beat_section = f"""
### ğŸµ Audio-Visual Sync
| Metric | Value | Interpretation |
|:-------|:-----:|:---------------|
| **Beat Alignment** | {beat_pct:.1f}% | {beat_icon} {'Good' if beat_pct > 50 else 'Moderate'} sync between cuts & beats |
"""
    
    return f"""## {mode_icon} {mode_title}
{comparison_note}
### ğŸ¬ Visual Characteristics

| Metric | Dominant Value | Distribution |
|:-------|:-------------:|:-------------|
{camera_row}
{hue_row}
{sat_row}
{bright_row}
{scene_row}

### ğŸ“Š Technical Metrics

| Metric | Value | Description |
|:-------|:-----:|:------------|
| **Color Temperature** | {cct_str} | Average CCT |
| **Cuts per Minute** | {cuts_str} | Editing pace |
| **Avg Shot Length** | {shot_str} | Shot duration |
| **BPM** | {bpm_str} | Music tempo |

### ğŸµ Audio Characteristics

| Metric | Dominant Value | Distribution |
|:-------|:-------------:|:-------------|
{bgm_row}
{mood_row}
| **Key Signature** | {output.key_signature or 'N/A'} | â€” |
{yolo_section}{beat_section}
---
*Summary based on {'single video analysis' if video_count <= 1 else f'cross-video consensus from {video_count} videos'}*
"""


def format_ai_detection(output: AIDetectionOutput) -> str:
    if not output or not output.success:
        return f"âŒ {t('analysis_failed')}"
    
    verdict_emoji = {
        "Real": "âœ…", "Suspicious": "âš ï¸", "Deepfake": "ğŸ­",
        "AIGC": "ğŸ¨", "Audio-Deepfake": "ğŸ”Š",
        "Synthetic": "ğŸ¤–", "AI-Generated": "ğŸ¤–", "Unknown": "â“"
    }
    emoji = verdict_emoji.get(output.verdict, "â“")
    
    models_str = ", ".join(output.models_used) if output.models_used else "None"
    
    # Get scores and availability flags with fallback
    aigc_score = getattr(output, 'aigc_score', 0.0)
    aigc_available = getattr(output, 'aigc_available', False)
    audio_score = getattr(output, 'audio_deepfake_score', 0.0)
    audio_available = getattr(output, 'audio_deepfake_available', False)
    temporal_available = getattr(output, 'temporal_available', False)
    face_available = getattr(output, 'face_available', False)
    
    # Get weights from analysis details
    weights = output.analysis_details.get("weights", {})
    deepfake_w = weights.get("deepfake", 0.30)
    clip_w = weights.get("clip", 0.20)
    temporal_w = weights.get("temporal", 0.15)
    aigc_w = weights.get("aigc", 0.20)
    audio_w = weights.get("audio_deepfake", 0.10)
    face_w = weights.get("face", 0.05)
    
    # Calculate weighted contribution
    def weighted_contrib(score, weight, available):
        if not available:
            return "â€”"
        contrib = score * weight
        return f"{contrib:.1%}"
    
    return f"""## ğŸ¤– AI Detection Results

### {emoji} Verdict: **{output.verdict}**
### Confidence: **{output.confidence:.1%}** (weighted average)

---

### ğŸ“Š Detection Models & Weights

| Model | Weight | Score | Contribution | Status | Description |
|:------|:------:|:-----:|:------------:|:------:|:------------|
| ğŸ­ **DeepFake-v2** | `{deepfake_w:.0%}` | {output.deepfake_score:.1%} | {weighted_contrib(output.deepfake_score, deepfake_w, output.deepfake_available)} | {'âœ…' if output.deepfake_available else 'âŒ'} | *HuggingFace ViT model (92% acc), detects face swaps* |
| ğŸ” **CLIP Synthetic** | `{clip_w:.0%}` | {output.clip_synthetic_score:.1%} | {weighted_contrib(output.clip_synthetic_score, clip_w, output.clip_available)} | {'âœ…' if output.clip_available else 'âŒ'} | *Zero-shot detection using CLIP embeddings* |
| â±ï¸ **CLIP-Temporal** | `{temporal_w:.0%}` | {output.temporal_score:.1%} | {weighted_contrib(output.temporal_score, temporal_w, temporal_available)} | {'âœ…' if temporal_available else 'âŒ'} | *Semantic consistency between frames (CLIP-based)* |
| ğŸ¨ **AIGC Detector** | `{aigc_w:.0%}` | {aigc_score:.1%} | {weighted_contrib(aigc_score, aigc_w, aigc_available)} | {'âœ…' if aigc_available else 'âŒ'} | *Detects Stable Diffusion, DALL-E, Midjourney* |
| ğŸ”Š **Audio Deepfake** | `{audio_w:.0%}` | {audio_score:.1%} | {weighted_contrib(audio_score, audio_w, audio_available)} | {'âœ…' if audio_available else 'âŒ'} | *Detects voice cloning & TTS synthesis* |
| ğŸ‘¤ **Face Analysis** | `{face_w:.0%}` | {output.no_face_ratio:.1%} | â€” | {'âœ…' if face_available else 'âŒ'} | *No-face ratio analysis (>90% suspicious)* |

---

### ğŸ‘¤ Face Detection Details

| Metric | Value | Explanation |
|:-------|:-----:|:------------|
| **Faces Detected** | {output.faces_detected} | Total faces found across all frames |
| **Frames with Faces** | {output.frames_with_faces}/{output.frames_analyzed} | Ratio of frames containing faces |
| **No-Face Ratio** | {output.no_face_ratio:.1%} | Higher = more suspicious for face videos |
| **Temporal Anomalies** | {output.temporal_anomalies} | Sudden changes in frame consistency |

---

### â„¹ï¸ How Scoring Works

- **Final Confidence** = Î£ (Model Score Ã— Weight) / Î£ Weights
- Models with â­ use HuggingFace pretrained models (higher reliability)
- Models with âœ… use computed features (good reliability)
- **Verdict Thresholds**: Real <40% | Suspicious 40-70% | AI-Generated â‰¥70%

**Active Models**: {models_str}
"""


# =============================================================================
# Processing Functions
# =============================================================================
def upload_video(video_file):
    """Upload a single video (resets state for single-video mode)"""
    if video_file is None:
        return t('upload_first'), None, [], get_video_list_html()
    
    STATE.reset()
    STATE.work_dir = Path(tempfile.mkdtemp(prefix="video_analysis_"))
    
    video_path = Path(video_file)
    STATE.video_path = STATE.work_dir / video_path.name
    
    import shutil
    shutil.copy(video_file, STATE.video_path)
    
    # Also add to multi-video list
    STATE.add_video(STATE.video_path, STATE.work_dir)
    STATE.videos[0].audio_path = extract_audio_from_video(STATE.video_path, STATE.work_dir)
    STATE.audio_path = STATE.videos[0].audio_path
    
    num_frames = STATE.config.ui.gallery_frames
    frame_paths = extract_frames_for_gallery(STATE.video_path, STATE.work_dir, num_frames)
    
    status = f"{t('uploaded')}: {video_path.name}\n"
    status += f"{t('workdir')}: {STATE.work_dir}\n"
    status += t('frames_extracted').format(n=len(frame_paths)) + "\n"
    status += t('audio_extracted') if STATE.audio_path else t('audio_failed')
    
    audio_path = str(STATE.audio_path) if STATE.audio_path else None
    return status, audio_path, frame_paths, get_video_list_html()


def add_video(video_file):
    """Add an additional video for multi-video comparison"""
    if video_file is None:
        return get_video_list_html()
    
    video_path = Path(video_file)
    
    # Create work directory if not exists
    if STATE.work_dir is None:
        STATE.work_dir = Path(tempfile.mkdtemp(prefix="video_analysis_"))
    
    # Create unique subdirectory for this video
    video_work_dir = STATE.work_dir / f"video_{len(STATE.videos)}"
    video_work_dir.mkdir(exist_ok=True)
    
    # Copy video
    dest_path = video_work_dir / video_path.name
    import shutil
    shutil.copy(video_file, dest_path)
    
    # Add to video list
    idx = STATE.add_video(dest_path, video_work_dir)
    
    # Extract audio
    STATE.videos[idx].audio_path = extract_audio_from_video(dest_path, video_work_dir)
    
    # If this is the first video, set legacy state
    if len(STATE.videos) == 1:
        STATE.video_path = dest_path
        STATE.audio_path = STATE.videos[0].audio_path
    
    return get_video_list_html()


def clear_all_videos():
    """Clear all videos"""
    STATE.reset()
    return get_video_list_html(), t('upload_first'), None, []


def select_video(index: int):
    """Select a video from the list to view/analyze"""
    if 0 <= index < len(STATE.videos):
        STATE.current_index = index
        STATE.sync_current_to_legacy()
        
        video = STATE.videos[index]
        if video.video_path:
            # Extract frames for gallery
            num_frames = STATE.config.ui.gallery_frames
            frame_paths = extract_frames_for_gallery(video.video_path, video.work_dir, num_frames)
            audio_path = str(video.audio_path) if video.audio_path else None
            
            status = f"ğŸ“¹ {t('video_n').format(n=index+1)}: {video.video_path.name}"
            return status, audio_path, frame_paths
    
    return t('no_videos'), None, []


def get_video_list_html() -> str:
    """Generate HTML for video list display"""
    if not STATE.videos:
        return f"<div style='padding:20px;text-align:center;color:#888;'>{t('no_videos')}</div>"
    
    items = []
    for i, video in enumerate(STATE.videos):
        name = video.video_path.name if video.video_path else f"Video {i+1}"
        is_current = i == STATE.current_index
        
        # Status indicators
        status_icons = []
        if video.visual_output:
            status_icons.append("ğŸ“¹")
        if video.audio_output:
            status_icons.append("ğŸµ")
        if video.asr_output:
            status_icons.append("ğŸ¤")
        if video.yolo_output:
            status_icons.append("ğŸ”")
        if video.ai_output:
            status_icons.append("ğŸ¤–")
        
        status_str = " ".join(status_icons) if status_icons else "â³"
        
        bg_color = "#e3f2fd" if is_current else "#f5f5f5"
        border = "2px solid #2196f3" if is_current else "1px solid #ddd"
        
        items.append(f"""
        <div style='padding:10px;margin:5px 0;background:{bg_color};border:{border};border-radius:8px;'>
            <strong>{t('video_n').format(n=i+1)}</strong>: {name}<br/>
            <small style='color:#666;'>Status: {status_str}</small>
        </div>
        """)
    
    header = f"<div style='font-weight:bold;margin-bottom:10px;'>"
    if len(STATE.videos) == 1:
        header += f"ğŸ“Š {t('single_video_mode')}"
    else:
        header += f"ğŸ“ˆ {t('multi_video_mode').format(n=len(STATE.videos))}"
    header += "</div>"
    
    return header + "".join(items)


# Internal analysis functions (no progress tracking)
def _run_visual_internal():
    """Internal visual analysis without progress tracking"""
    if STATE.video_path is None:
        return f"âŒ {t('upload_first')}", None
    
    cfg = STATE.config.visual
    step = VisualAnalysisStep()
    input_data = VideoInput(
        video_path=STATE.video_path,
        work_dir=STATE.work_dir,
        frame_mode=cfg.frame_mode,
        target_frames=cfg.target_frames,
        scene_threshold=cfg.scene_threshold
    )
    
    STATE.visual_output = step.run(input_data)
    contact = STATE.visual_output.contact_sheet if STATE.visual_output else None
    return format_visual(STATE.visual_output), contact


def _run_audio_internal():
    """Internal audio analysis without progress tracking"""
    if STATE.audio_path is None:
        return f"âŒ {t('upload_first')}"
    
    step = AudioAnalysisStep()
    input_data = AudioInput(audio_path=STATE.audio_path)
    STATE.audio_output = step.run(input_data)
    return format_audio(STATE.audio_output)


def _run_asr_internal(language: str):
    """Internal ASR analysis without progress tracking"""
    if STATE.audio_path is None:
        return f"âŒ {t('upload_first')}"
    
    cfg = STATE.config.asr
    step = ASRAnalysisStep()
    input_data = ASRInput(
        audio_path=STATE.audio_path,
        language=language,
        model_size=cfg.whisper_model,
        beam_size=cfg.whisper_beam_size,
        enable_prosody=True,
        enable_emotion=True
    )
    STATE.asr_output = step.run(input_data)
    return format_asr(STATE.asr_output)


def _run_yolo_internal():
    """Internal YOLO analysis without progress tracking"""
    if STATE.video_path is None:
        return f"âŒ {t('upload_first')}"
    
    cfg = STATE.config.yolo
    step = YOLOAnalysisStep()
    input_data = YOLOInput(
        video_path=STATE.video_path,
        target_frames=cfg.target_frames,
        model_name=cfg.model_name,
        confidence_threshold=cfg.confidence_threshold,
        enable_colors=cfg.enable_colors,
        enable_materials=cfg.enable_materials
    )
    STATE.yolo_output = step.run(input_data)
    return format_yolo(STATE.yolo_output)


def _run_ai_detection_internal():
    """Internal AI detection without progress tracking"""
    if STATE.video_path is None:
        return f"âŒ {t('upload_first')}"
    
    cfg = STATE.config.ai_detection
    if not cfg.enabled:
        return "âŒ AI Detection is disabled"
    
    step = AIDetectionStep()
    input_data = AIDetectionInput(
        video_path=STATE.video_path,
        audio_path=STATE.audio_path,
        use_deepfake=cfg.use_deepfake,
        use_clip=cfg.use_clip,
        use_temporal=cfg.use_temporal,
        use_face_detection=cfg.use_face_detection,
        use_aigc=cfg.use_aigc,
        use_audio_deepfake=cfg.use_audio_deepfake,
        num_frames=cfg.num_frames,
        temporal_frames=cfg.temporal_frames,
        fake_threshold=cfg.fake_threshold,
        no_face_threshold=cfg.no_face_threshold,
        deepfake_weight=cfg.deepfake_weight,
        clip_weight=cfg.clip_weight,
        temporal_weight=cfg.temporal_weight,
        aigc_weight=cfg.aigc_weight,
        audio_deepfake_weight=cfg.audio_deepfake_weight,
        face_weight=cfg.face_weight,
    )
    STATE.ai_output = step.run(input_data)
    return format_ai_detection(STATE.ai_output)


# Public analysis functions with progress tracking (for standalone button clicks)
def run_visual(progress=gr.Progress()):
    progress(0.1, desc="ğŸ“¹ Loading CLIP...")
    progress(0.3, desc="ğŸ“¹ Analyzing visual...")
    result = _run_visual_internal()
    STATE.sync_legacy_to_current()  # Sync to multi-video state
    progress(1.0, desc="âœ… Visual done")
    return result


def run_audio(progress=gr.Progress()):
    progress(0.1, desc="ğŸµ Loading CLAP...")
    progress(0.3, desc="ğŸµ Analyzing audio...")
    result = _run_audio_internal()
    STATE.sync_legacy_to_current()  # Sync to multi-video state
    progress(1.0, desc="âœ… Audio done")
    return result


def run_asr(language: str, progress=gr.Progress()):
    progress(0.1, desc="ğŸ¤ Loading Whisper...")
    progress(0.3, desc="ğŸ¤ Transcribing...")
    result = _run_asr_internal(language)
    STATE.sync_legacy_to_current()  # Sync to multi-video state
    progress(1.0, desc="âœ… ASR done")
    return result


def run_yolo(progress=gr.Progress()):
    progress(0.1, desc="ğŸ” Loading YOLO...")
    progress(0.3, desc="ğŸ” Detecting objects...")
    result = _run_yolo_internal()
    STATE.sync_legacy_to_current()  # Sync to multi-video state
    progress(1.0, desc="âœ… YOLO done")
    return result


def run_ai_detection(progress=gr.Progress()):
    progress(0.1, desc="ğŸ¤– Loading AI models...")
    progress(0.3, desc="ğŸ¤– Detecting AI content...")
    result = _run_ai_detection_internal()
    STATE.sync_legacy_to_current()  # Sync to multi-video state
    progress(1.0, desc="âœ… AI detection done")
    return result


def run_batch_analysis(language: str, progress=gr.Progress()):
    """Analyze all videos in the list for cross-video comparison"""
    if not STATE.videos:
        return (f"âŒ {t('no_videos')}", None, "", "", "", "", "", "", get_video_list_html())
    
    total_videos = len(STATE.videos)
    results = []
    
    for i, video in enumerate(STATE.videos):
        # Switch to this video
        STATE.current_index = i
        STATE.sync_current_to_legacy()
        
        # Progress for this video
        base_progress = i / total_videos
        progress(base_progress + 0.02, desc=f"ğŸ“¹ Video {i+1}/{total_videos}: Visual...")
        
        _run_visual_internal()
        progress(base_progress + 0.04, desc=f"ğŸµ Video {i+1}/{total_videos}: Audio...")
        
        _run_audio_internal()
        progress(base_progress + 0.06, desc=f"ğŸ¤ Video {i+1}/{total_videos}: ASR...")
        
        _run_asr_internal(language)
        progress(base_progress + 0.08, desc=f"ğŸ” Video {i+1}/{total_videos}: YOLO...")
        
        _run_yolo_internal()
        
        # Sync back to video list
        STATE.sync_legacy_to_current()
        results.append(video.video_path.name if video.video_path else f"Video {i+1}")
    
    # Generate cross-video consensus
    progress(0.95, desc="ğŸ“Š Generating cross-video summary...")
    consensus_result = run_consensus()
    
    progress(1.0, desc=t('done'))
    
    # Summary
    summary = f"âœ… Analyzed {total_videos} videos:\n"
    for r in results:
        summary += f"  â€¢ {r}\n"
    summary += f"\nğŸ“Š Cross-video consensus generated"
    
    # Return the last video's results for display
    STATE.current_index = total_videos - 1
    STATE.sync_current_to_legacy()
    
    visual_result = format_visual(STATE.visual_output) if STATE.visual_output else ""
    audio_result = format_audio(STATE.audio_output) if STATE.audio_output else ""
    asr_result = format_asr(STATE.asr_output) if STATE.asr_output else ""
    yolo_result = format_yolo(STATE.yolo_output) if STATE.yolo_output else ""
    ai_result = format_ai_detection(STATE.ai_output) if STATE.ai_output else "*Not run*"
    contact = STATE.visual_output.contact_sheet if STATE.visual_output else None
    
    return (visual_result, contact, audio_result, asr_result, yolo_result, 
            ai_result, consensus_result, summary, get_video_list_html())


def run_consensus():
    """Run consensus analysis - supports both single and multi-video mode"""
    # Sync current analysis to legacy state
    STATE.sync_legacy_to_current()
    
    # Get all metrics from all videos
    all_metrics = STATE.get_all_metrics()
    
    # Fallback to legacy single-video mode if no multi-video data
    if not all_metrics:
        if STATE.visual_output is None and STATE.audio_output is None:
            return f"âŒ {t('run_analysis_first')}"
        
        # Use legacy single video
        metrics = VideoMetrics(path=str(STATE.video_path) if STATE.video_path else "")
        metrics.visual = STATE.visual_output
        metrics.audio = STATE.audio_output
        metrics.asr = STATE.asr_output
        metrics.yolo = STATE.yolo_output
        all_metrics = [metrics]
    
    step = ConsensusStep()
    input_data = ConsensusInput(video_metrics=all_metrics)
    STATE.consensus_output = step.run(input_data)
    
    video_count = len(all_metrics)
    return format_consensus(STATE.consensus_output, video_count)


def run_all(language: str, progress=gr.Progress()):
    # Use internal functions to avoid duplicate progress bars
    
    progress(0.05, desc="ğŸ“¹ Step 1/6: Visual Analysis...")
    visual_result, contact = _run_visual_internal()
    
    progress(0.20, desc="ğŸµ Step 2/6: Audio Analysis...")
    audio_result = _run_audio_internal()
    
    progress(0.35, desc="ğŸ¤ Step 3/6: Speech Recognition...")
    asr_result = _run_asr_internal(language)
    
    progress(0.50, desc="ğŸ” Step 4/6: Object Detection...")
    yolo_result = _run_yolo_internal()
    
    progress(0.65, desc="ğŸ¤– Step 5/6: AI Detection...")
    ai_result = _run_ai_detection_internal() if STATE.config.ai_detection.enabled else "*Disabled*"
    
    # Sync results back to multi-video state
    STATE.sync_legacy_to_current()
    
    progress(0.85, desc="ğŸ“Š Step 6/6: Generating Summary...")
    consensus_result = run_consensus()
    
    progress(1.0, desc=t('done'))
    
    # Generate summary
    video_count = STATE.get_video_count()
    lines = ["=" * 25, t('quick_summary'), "=" * 25, ""]
    
    if video_count > 1:
        lines.append(f"ğŸ“ˆ Mode: Cross-Video Comparison ({video_count} videos)")
        lines.append(f"âœ… Analyzed: {STATE.get_analyzed_count()} videos")
    else:
        lines.append(f"ğŸ“Š Mode: Single Video Analysis")
    
    lines.append("")
    
    if STATE.visual_output:
        lines.append(f"ğŸ“¹ Camera: {STATE.visual_output.camera_angle}")
        lines.append(f"ğŸ¨ Color: {STATE.visual_output.hue_family}")
    if STATE.audio_output:
        lines.append(f"ğŸµ BPM: {STATE.audio_output.tempo_bpm:.1f}")
        lines.append(f"ğŸ¸ BGM: {STATE.audio_output.bgm_style}")
    if STATE.asr_output:
        lines.append(f"ğŸ¤ WPM: {STATE.asr_output.words_per_minute:.1f}")
    if STATE.yolo_output:
        lines.append(f"ğŸ” Objects: {STATE.yolo_output.detection.get('unique_objects', 0)}")
    if STATE.ai_output:
        lines.append(f"ğŸ¤– AI: {STATE.ai_output.verdict} ({STATE.ai_output.confidence:.0%})")
    
    summary = "\n".join(lines)
    video_list = get_video_list_html()
    
    return visual_result, contact, audio_result, asr_result, yolo_result, ai_result, consensus_result, summary, video_list


def gen_report(progress=gr.Progress()):
    if STATE.video_path is None:
        error_html = "<div style='text-align:center; padding:40px; background:#fee; border-radius:8px;'><p>âŒ Please upload a video first</p></div>"
        return f"âŒ {t('upload_first')}", None, None, error_html
    
    if STATE.visual_output is None and STATE.audio_output is None:
        error_html = "<div style='text-align:center; padding:40px; background:#fee; border-radius:8px;'><p>âŒ Please run analysis first</p></div>"
        return f"âŒ {t('run_analysis_first')}", None, None, error_html
    
    progress(0.2, desc="ğŸ“„ Generating Word...")
    
    metrics = VideoMetrics(path=str(STATE.video_path))
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    if STATE.consensus_output is None:
        run_consensus()
    
    metrics_dict = metrics.to_dict()
    consensus_dict = STATE.consensus_output.to_dict() if STATE.consensus_output else {}
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = STATE.work_dir / f"report_{timestamp}.docx"
    
    generate_word_report(
        video_metrics=[metrics_dict],
        consensus=consensus_dict,
        output_path=str(report_path),
        show_screenshots=STATE.config.report.include_screenshots
    )
    
    STATE.report_path = str(report_path)
    
    progress(0.7, desc="ğŸ“• Converting PDF...")
    STATE.pdf_path = convert_docx_to_pdf(STATE.report_path)
    
    progress(1.0, desc=t('done'))
    
    status = f"{t('report_generated')}\nğŸ“„ {report_path.name}"
    
    # Generate PDF preview HTML
    pdf_preview_html = "<div style='text-align:center; padding:40px; background:#f5f5f5; border-radius:8px;'><p>ğŸ“„ PDF conversion not available (requires LibreOffice)</p></div>"
    if STATE.pdf_path:
        status += f"\nğŸ“• {Path(STATE.pdf_path).name}"
        # Create embedded PDF viewer
        pdf_preview_html = f'''
        <div style="width:100%; height:500px; border:1px solid #ddd; border-radius:8px; overflow:hidden;">
            <iframe src="file://{STATE.pdf_path}" width="100%" height="100%" style="border:none;">
                <p>PDF preview not supported. <a href="file://{STATE.pdf_path}" download>Download PDF</a></p>
            </iframe>
        </div>
        <p style="text-align:center; margin-top:10px; color:#666;">
            â¬†ï¸ If preview doesn't load, download the PDF file above
        </p>
        '''
    
    return status, STATE.report_path, STATE.pdf_path, pdf_preview_html


def export_json():
    if STATE.video_path is None:
        return f"âŒ {t('upload_first')}", None, "// Please upload a video first"
    
    data = {
        "timestamp": datetime.now().isoformat(),
        "video_path": str(STATE.video_path),
        "config": STATE.config.to_dict(),
        "visual": STATE.visual_output.to_dict() if STATE.visual_output else None,
        "audio": STATE.audio_output.to_dict() if STATE.audio_output else None,
        "asr": STATE.asr_output.to_dict() if STATE.asr_output else None,
        "yolo": STATE.yolo_output.to_dict() if STATE.yolo_output else None,
        "ai_detection": STATE.ai_output.to_dict() if STATE.ai_output else None,
        "consensus": STATE.consensus_output.to_dict() if STATE.consensus_output else None,
    }
    
    indent = STATE.config.report.json_indent
    json_path = STATE.work_dir / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=indent, ensure_ascii=False, default=str)
    
    # Generate preview (truncated for large data)
    preview_data = {
        "timestamp": data["timestamp"],
        "video_path": data["video_path"],
        "visual": {"...": "see full JSON"} if data["visual"] else None,
        "audio": {"...": "see full JSON"} if data["audio"] else None,
        "asr": {"text_preview": data["asr"]["text"][:200] + "..." if data["asr"] and data["asr"].get("text") else None} if data["asr"] else None,
        "yolo": {"...": "see full JSON"} if data["yolo"] else None,
        "ai_detection": data["ai_detection"] if data["ai_detection"] else None,
        "consensus": {"...": "see full JSON"} if data["consensus"] else None,
    }
    json_preview = json.dumps(preview_data, indent=2, ensure_ascii=False, default=str)
    
    return f"{t('json_exported')}: {json_path.name}", str(json_path), json_preview


def update_config(
    visual_frames, visual_threshold,
    yolo_model, yolo_conf, yolo_frames,
    asr_model, asr_beam,
    ai_enabled, ai_deepfake, ai_clip, ai_temporal, ai_aigc, ai_audio, ai_face,
    ai_threshold, ai_deepfake_weight, ai_clip_weight, ai_temporal_weight, ai_aigc_weight, ai_audio_weight, ai_face_weight
):
    """Update configuration from UI controls"""
    # Visual
    STATE.config.visual.target_frames = int(visual_frames)
    STATE.config.visual.scene_threshold = float(visual_threshold)
    
    # YOLO
    STATE.config.yolo.model_name = yolo_model
    STATE.config.yolo.confidence_threshold = float(yolo_conf)
    STATE.config.yolo.target_frames = int(yolo_frames)
    
    # ASR
    STATE.config.asr.whisper_model = asr_model
    STATE.config.asr.whisper_beam_size = int(asr_beam)
    
    # AI Detection (SOTA 2025/2026)
    STATE.config.ai_detection.enabled = ai_enabled
    STATE.config.ai_detection.use_deepfake = ai_deepfake
    STATE.config.ai_detection.use_clip = ai_clip
    STATE.config.ai_detection.use_temporal = ai_temporal
    STATE.config.ai_detection.use_aigc = ai_aigc
    STATE.config.ai_detection.use_audio_deepfake = ai_audio
    STATE.config.ai_detection.use_face_detection = ai_face
    STATE.config.ai_detection.fake_threshold = float(ai_threshold)
    STATE.config.ai_detection.deepfake_weight = float(ai_deepfake_weight)
    STATE.config.ai_detection.clip_weight = float(ai_clip_weight)
    STATE.config.ai_detection.temporal_weight = float(ai_temporal_weight)
    STATE.config.ai_detection.aigc_weight = float(ai_aigc_weight)
    STATE.config.ai_detection.audio_deepfake_weight = float(ai_audio_weight)
    STATE.config.ai_detection.face_weight = float(ai_face_weight)
    
    # Calculate total weight and show warning if not ~1.0
    total_weight = ai_deepfake_weight + ai_clip_weight + ai_temporal_weight + ai_aigc_weight + ai_audio_weight + ai_face_weight
    
    status = "âœ… Configuration updated"
    if abs(total_weight - 1.0) > 0.1:
        status += f"\nâš ï¸ Warning: Total weight = {total_weight:.2f} (recommended: ~1.0)"
    
    return status


def switch_language(lang: str):
    set_language(lang)
    # Note: Tab labels can't be dynamically updated in Gradio
    # Use --lang zh to start with Chinese interface
    footer_note = t('footer')
    if lang == "zh":
        footer_note += " | ğŸ”„ åˆ·æ–°é¡µé¢ä»¥æ›´æ–°æ‰€æœ‰æ ‡ç­¾"
    else:
        footer_note += " | ğŸ”„ Refresh page for full language switch"
    
    return (
        f"# {t('title')}\n**{t('subtitle')}** | {t('models')}",
        t('analyze_all'),
        t('btn_visual'),
        t('btn_audio'),
        t('btn_asr'),
        t('btn_yolo'),
        t('btn_ai_detect'),
        t('btn_consensus'),
        t('gen_report'),
        t('export_json'),
        footer_note,
    )


# =============================================================================
# Gradio UI
# =============================================================================
def create_ui():
    cfg = STATE.config
    
    with gr.Blocks(title="Video Style Analysis") as demo:
        
        # Header
        header_md = gr.Markdown(f"# {t('title')}\n**{t('subtitle')}** | {t('models')}")
        
        with gr.Row():
            lang_radio = gr.Radio(
                choices=[("English", "en"), ("ä¸­æ–‡", "zh")],
                value="en", label="Language / è¯­è¨€", scale=1
            )
        
        gr.Markdown("---")
        
        with gr.Tabs():
            # ========== Tab 1: Upload & Preview ==========
            with gr.Tab(t('tab_upload'), id="tab_upload"):
                with gr.Row():
                    # Left: Upload Section
                    with gr.Column(scale=2, min_width=400):
                        gr.Markdown("### ğŸ“¤ Video Upload")
                        gr.Markdown("*Supports MP4, AVI, MOV, MKV. Upload single or multiple videos.*")
                        
                        video_input = gr.Video(
                            label="Select Video File",
                            height=280,
                            elem_classes=["video-preview"]
                        )
                        
                        with gr.Row():
                            add_video_btn = gr.Button(t('add_video'), size="sm", scale=1)
                            clear_videos_btn = gr.Button(t('clear_all'), size="sm", variant="secondary", scale=1)
                        
                        upload_status = gr.Textbox(
                            label="Upload Status",
                            lines=2,
                            interactive=False
                        )
                        
                        # Video list for multi-video mode
                        gr.Markdown("### ğŸ“‹ " + t('video_list'))
                        video_list_html = gr.HTML(
                            value=get_video_list_html(),
                            label="Video List"
                        )
                        
                        video_selector = gr.Slider(
                            minimum=1, maximum=10, step=1, value=1,
                            label=t('select_video'),
                            visible=False  # Hidden until multiple videos
                        )
                        
                        gr.Markdown("### âš™ï¸ Analysis Settings")
                        language_select = gr.Dropdown(
                            choices=[("English", "en"), ("ä¸­æ–‡", "zh"), ("æ—¥æœ¬èª", "ja"), ("í•œêµ­ì–´", "ko"), ("Auto-detect", "auto")],
                            value="en",
                            label="Speech Recognition Language"
                        )
                    
                    # Right: Preview Section
                    with gr.Column(scale=3, min_width=500):
                        gr.Markdown("### ğŸ¬ Media Preview")
                        
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("**ğŸ”Š Extracted Audio**")
                                gr.Markdown("*Audio track separated from video*")
                                audio_player = gr.Audio(
                                    label="Audio Preview",
                                    type="filepath"
                                )
                        
                        gr.Markdown("### ğŸ–¼ï¸ Key Frames Gallery")
                        gr.Markdown("*Click any frame to view full size*")
                        frame_gallery = gr.Gallery(
                            label="Extracted Key Frames",
                            columns=4,
                            rows=3,
                            height=280,
                            object_fit="contain",
                            allow_preview=True,
                            preview=True
                        )
            
            # ========== Tab 2: Run Analysis ==========
            with gr.Tab(t('tab_analysis'), id="tab_analysis"):
                gr.Markdown("### ğŸ¯ Analysis Controls")
                gr.Markdown("*Click 'Analyze All' for complete analysis, or run individual modules*")
                
                with gr.Row():
                    run_all_btn = gr.Button(
                        "ğŸ¯ Analyze Current Video",
                        variant="primary",
                        size="lg",
                        scale=2
                    )
                    run_batch_btn = gr.Button(
                        "ğŸ“ˆ Analyze All Videos (Cross-Video)",
                        variant="secondary",
                        size="lg",
                        scale=2
                    )
                
                gr.Markdown("*Use 'Analyze Current' for single video, 'Analyze All' for multi-video comparison*")
                gr.Markdown("**Individual Analysis Modules:**")
                with gr.Row():
                    run_visual_btn = gr.Button(t('btn_visual'), size="sm")
                    run_audio_btn = gr.Button(t('btn_audio'), size="sm")
                    run_asr_btn = gr.Button(t('btn_asr'), size="sm")
                    run_yolo_btn = gr.Button(t('btn_yolo'), size="sm")
                    run_ai_btn = gr.Button(t('btn_ai_detect'), size="sm")
                    run_consensus_btn = gr.Button(t('btn_consensus'), size="sm")
                
                gr.Markdown("---")
                
                # Results Tabs with meaningful names
                with gr.Tabs():
                    with gr.Tab(t('tab_visual'), id="result_visual"):
                        visual_result = gr.Markdown(f"*{t('upload_first')}*")
                        contact_img = gr.Image(
                            label="Contact Sheet",
                            height=200
                        )
                    
                    with gr.Tab(t('tab_audio'), id="result_audio"):
                        audio_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_asr'), id="result_asr"):
                        asr_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_yolo'), id="result_yolo"):
                        yolo_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_ai'), id="result_ai"):
                        ai_result = gr.Markdown(f"*{t('upload_first')}*")
                    
                    with gr.Tab(t('tab_summary'), id="result_summary"):
                        consensus_result = gr.Markdown(f"*{t('run_analysis_first')}*")
            
            # ========== Tab 3: Export & Reports ==========
            with gr.Tab(t('tab_export'), id="tab_export"):
                gr.Markdown("### ğŸ“„ Export Analysis Results")
                
                with gr.Row():
                    # Left: PDF Report
                    with gr.Column(scale=1):
                        gr.Markdown("#### ğŸ“• PDF Report")
                        gen_report_btn = gr.Button("ğŸ“„ Generate Report", variant="primary", size="lg")
                        report_status = gr.Textbox(
                            label="Status",
                            lines=1,
                            interactive=False
                        )
                        
                        gr.Markdown("**Downloads:**")
                        with gr.Row():
                            report_file = gr.File(label="Word (.docx)")
                            pdf_file = gr.File(label="PDF")
                        
                        gr.Markdown("**PDF Preview:**")
                        pdf_preview = gr.HTML(
                            value="<div style='text-align:center; padding:40px; background:#f5f5f5; border-radius:8px;'><p>ğŸ“„ Generate report to preview PDF</p></div>",
                            label="PDF Preview"
                        )
                    
                    # Right: JSON Export
                    with gr.Column(scale=1):
                        gr.Markdown("#### ğŸ’¾ JSON Data")
                        export_json_btn = gr.Button("ğŸ’¾ Export JSON", variant="secondary", size="lg")
                        json_status = gr.Textbox(
                            label="Status",
                            lines=1,
                            interactive=False
                        )
                        
                        gr.Markdown("**Download:**")
                        json_file = gr.File(label="JSON Data")
                        
                        gr.Markdown("**JSON Preview:**")
                        json_preview = gr.Code(
                            value="// Generate JSON to preview data",
                            language="json",
                            label="JSON Preview",
                            lines=15
                        )
                
                gr.Markdown("---")
                gr.Markdown("### ğŸ“‹ Quick Summary")
                summary_box = gr.Textbox(
                    label="Analysis Overview",
                    lines=10,
                    interactive=False
                )
            
            # ========== Tab 4: Configuration ==========
            with gr.Tab(t('tab_config'), id="tab_config"):
                gr.Markdown("### âš™ï¸ Analysis Configuration")
                gr.Markdown("*Adjust parameters for each analysis module. Changes apply to next analysis.*")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### ğŸ“¹ Visual Analysis")
                        gr.Markdown("*More frames = slower but more accurate*")
                        cfg_visual_frames = gr.Slider(
                            10, 200, value=cfg.visual.target_frames, step=10,
                            label="Target Frames"
                        )
                        cfg_visual_threshold = gr.Slider(
                            10, 50, value=cfg.visual.scene_threshold, step=1,
                            label="Scene Threshold (lower = more sensitive)"
                        )
                        
                        gr.Markdown("#### ğŸ” YOLO Object Detection")
                        gr.Markdown("*n=fastest, l=most accurate*")
                        cfg_yolo_model = gr.Dropdown(
                            choices=["yolo11n.pt", "yolo11s.pt", "yolo11m.pt", "yolo11l.pt"],
                            value=cfg.yolo.model_name,
                            label="YOLO Model"
                        )
                        cfg_yolo_conf = gr.Slider(
                            0.1, 0.9, value=cfg.yolo.confidence_threshold, step=0.05,
                            label="Confidence Threshold"
                        )
                        cfg_yolo_frames = gr.Slider(
                            10, 100, value=cfg.yolo.target_frames, step=5,
                            label="Frames to Analyze"
                        )
                        
                        gr.Markdown("#### ğŸ¤ Speech Recognition (ASR)")
                        gr.Markdown("*large-v3-turbo = best, tiny = fastest*")
                        cfg_asr_model = gr.Dropdown(
                            choices=["tiny", "base", "small", "medium", "large-v3", "large-v3-turbo"],
                            value=cfg.asr.whisper_model,
                            label="Whisper Model"
                        )
                        cfg_asr_beam = gr.Slider(
                            1, 10, value=cfg.asr.whisper_beam_size, step=1,
                            label="Beam Size (higher = better)"
                        )
                    
                    with gr.Column():
                        gr.Markdown("#### ğŸ¤– AI Detection Models")
                        gr.Markdown("*â­ = HuggingFace pretrained (high reliability)*")
                        
                        cfg_ai_enabled = gr.Checkbox(
                            value=cfg.ai_detection.enabled,
                            label="Enable AI Detection"
                        )
                        
                        gr.Markdown("**Detection Models:**")
                        cfg_ai_deepfake = gr.Checkbox(
                            value=cfg.ai_detection.use_deepfake,
                            label="â­ DeepFake-v2 (ViT, 92% acc)"
                        )
                        cfg_ai_clip = gr.Checkbox(
                            value=cfg.ai_detection.use_clip,
                            label="â­ CLIP Zero-Shot"
                        )
                        cfg_ai_temporal = gr.Checkbox(
                            value=cfg.ai_detection.use_temporal,
                            label="âœ… CLIP-Temporal Analysis"
                        )
                        cfg_ai_aigc = gr.Checkbox(
                            value=cfg.ai_detection.use_aigc,
                            label="â­ AIGC Detector (SD/DALL-E/MJ)"
                        )
                        cfg_ai_audio = gr.Checkbox(
                            value=cfg.ai_detection.use_audio_deepfake,
                            label="â­ Audio Deepfake"
                        )
                        cfg_ai_face = gr.Checkbox(
                            value=cfg.ai_detection.use_face_detection,
                            label="Face Analysis"
                        )
                        
                        cfg_ai_threshold = gr.Slider(
                            0.1, 0.9, value=cfg.ai_detection.fake_threshold, step=0.05,
                            label="AI Threshold (above = flagged)"
                        )
                        
                        gr.Markdown("#### âš–ï¸ Ensemble Weights")
                        gr.Markdown("*Higher weight = more influence on final score. Total should â‰ˆ 1.0*")
                        
                        cfg_ai_deepfake_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.deepfake_weight, step=0.05,
                            label="DeepFake-v2 Weight (â­ HuggingFace)"
                        )
                        cfg_ai_clip_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.clip_weight, step=0.05,
                            label="CLIP Synthetic Weight (â­ HuggingFace)"
                        )
                        cfg_ai_temporal_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.temporal_weight, step=0.05,
                            label="CLIP-Temporal Weight (âœ… Computed)"
                        )
                        cfg_ai_aigc_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.aigc_weight, step=0.05,
                            label="AIGC Detector Weight (â­ HuggingFace)"
                        )
                        cfg_ai_audio_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.audio_deepfake_weight, step=0.05,
                            label="Audio Deepfake Weight (â­ HuggingFace)"
                        )
                        cfg_ai_face_weight = gr.Slider(
                            0, 1, value=cfg.ai_detection.face_weight, step=0.05,
                            label="Face Analysis Weight"
                        )
                
                gr.Markdown("---")
                config_status = gr.Textbox(label="Status", interactive=False)
                save_config_btn = gr.Button("ğŸ’¾ Save Configuration", variant="primary", size="lg")
                
                save_config_btn.click(
                    fn=update_config,
                    inputs=[
                        cfg_visual_frames, cfg_visual_threshold,
                        cfg_yolo_model, cfg_yolo_conf, cfg_yolo_frames,
                        cfg_asr_model, cfg_asr_beam,
                        cfg_ai_enabled, cfg_ai_deepfake, cfg_ai_clip, cfg_ai_temporal, cfg_ai_aigc, cfg_ai_audio, cfg_ai_face,
                        cfg_ai_threshold, cfg_ai_deepfake_weight, cfg_ai_clip_weight, cfg_ai_temporal_weight, cfg_ai_aigc_weight, cfg_ai_audio_weight, cfg_ai_face_weight
                    ],
                    outputs=[config_status]
                )
        
        # Footer
        footer_md = gr.Markdown(f"---\n{t('footer')}")
        
        # ========== Event Handlers ==========
        video_input.change(fn=upload_video, inputs=[video_input],
                          outputs=[upload_status, audio_player, frame_gallery, video_list_html])
        
        add_video_btn.click(fn=add_video, inputs=[video_input],
                           outputs=[video_list_html])
        
        clear_videos_btn.click(fn=clear_all_videos, inputs=[],
                              outputs=[video_list_html, upload_status, audio_player, frame_gallery])
        
        video_selector.change(fn=lambda x: select_video(int(x)-1), inputs=[video_selector],
                             outputs=[upload_status, audio_player, frame_gallery])
        
        run_visual_btn.click(fn=run_visual, outputs=[visual_result, contact_img])
        run_audio_btn.click(fn=run_audio, outputs=[audio_result])
        run_asr_btn.click(fn=run_asr, inputs=[language_select], outputs=[asr_result])
        run_yolo_btn.click(fn=run_yolo, outputs=[yolo_result])
        run_ai_btn.click(fn=run_ai_detection, outputs=[ai_result])
        run_consensus_btn.click(fn=run_consensus, outputs=[consensus_result])
        
        run_all_btn.click(
            fn=run_all,
            inputs=[language_select],
            outputs=[visual_result, contact_img, audio_result, asr_result,
                     yolo_result, ai_result, consensus_result, summary_box, video_list_html]
        )
        
        run_batch_btn.click(
            fn=run_batch_analysis,
            inputs=[language_select],
            outputs=[visual_result, contact_img, audio_result, asr_result,
                     yolo_result, ai_result, consensus_result, summary_box, video_list_html]
        )
        
        gen_report_btn.click(fn=gen_report, outputs=[report_status, report_file, pdf_file, pdf_preview])
        export_json_btn.click(fn=export_json, outputs=[json_status, json_file, json_preview])
        
        lang_radio.change(
            fn=switch_language,
            inputs=[lang_radio],
            outputs=[header_md, run_all_btn, run_visual_btn, run_audio_btn,
                     run_asr_btn, run_yolo_btn, run_ai_btn, run_consensus_btn,
                     gen_report_btn, export_json_btn, footer_md]
        )
    
    return demo


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Video Style Analysis Web UI")
    parser.add_argument("--port", type=int, default=8088, help="Server port")
    parser.add_argument("--share", action="store_true", help="Create public link")
    parser.add_argument("--lang", type=str, default="en", choices=["en", "zh"], help="Default language")
    args = parser.parse_args()
    
    set_language(args.lang)
    
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share,
        show_error=True
    )
