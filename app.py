#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Video Style Analysis - Gradio Web Interface
SOTA Models: CLIP | CLAP | HuBERT | Whisper | YOLO11
"""

import sys
import json
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, Dict, Any, List

import gradio as gr
import numpy as np
import cv2

sys.path.insert(0, str(Path(__file__).parent / "src"))

from steps import (
    VisualAnalysisStep, AudioAnalysisStep, ASRAnalysisStep,
    YOLOAnalysisStep, ConsensusStep,
    VideoInput, AudioInput, ASRInput, YOLOInput, ConsensusInput,
    VideoMetrics, VisualOutput, AudioOutput, ASROutput, YOLOOutput, ConsensusOutput,
)
from report_word import generate_word_report

# =============================================================================
# å…¨å±€çŠ¶æ€
# =============================================================================
class AnalysisState:
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.video_path: Optional[Path] = None
        self.audio_path: Optional[Path] = None
        self.work_dir: Optional[Path] = None
        self.visual_output: Optional[VisualOutput] = None
        self.audio_output: Optional[AudioOutput] = None
        self.asr_output: Optional[ASROutput] = None
        self.yolo_output: Optional[YOLOOutput] = None
        self.consensus_output: Optional[ConsensusOutput] = None
        self.report_path: Optional[str] = None
        self.pdf_path: Optional[str] = None

STATE = AnalysisState()

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================
def extract_audio_from_video(video_path: Path, output_dir: Path) -> Optional[Path]:
    output_path = output_dir / f"{video_path.stem}_audio.wav"
    if output_path.exists():
        return output_path
    try:
        cmd = ["ffmpeg", "-y", "-i", str(video_path), "-vn", "-acodec", "pcm_s16le",
               "-ar", "22050", "-ac", "1", str(output_path)]
        subprocess.run(cmd, capture_output=True, check=True)
        return output_path
    except Exception:
        return None


def extract_frames_for_gallery(video_path: Path, output_dir: Path, num_frames: int = 12) -> List[str]:
    frames_dir = output_dir / "frames"
    frames_dir.mkdir(exist_ok=True)
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    if total_frames == 0:
        cap.release()
        return []
    
    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    frame_paths = []
    
    for i, idx in enumerate(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            timestamp = idx / fps if fps > 0 else 0
            frame_path = frames_dir / f"frame_{i:03d}_{timestamp:.1f}s.jpg"
            cv2.imwrite(str(frame_path), frame)
            frame_paths.append(str(frame_path))
    
    cap.release()
    return frame_paths


def convert_docx_to_pdf(docx_path: str) -> Optional[str]:
    pdf_path = docx_path.replace('.docx', '.pdf')
    try:
        cmd = ["libreoffice", "--headless", "--convert-to", "pdf",
               "--outdir", str(Path(docx_path).parent), docx_path]
        subprocess.run(cmd, capture_output=True, check=True, timeout=60)
        if Path(pdf_path).exists():
            return pdf_path
    except Exception:
        pass
    return None


# =============================================================================
# ç»“æœæ ¼å¼åŒ–
# =============================================================================
def format_visual(output: VisualOutput) -> str:
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    return f"""### ğŸ“¹ è§†è§‰åˆ†æç»“æœ

**åŸºæœ¬ä¿¡æ¯**: æ—¶é•¿ {output.duration:.2f}s | FPS {output.fps:.1f} | é‡‡æ · {output.sampled_frames} å¸§

**é•œå¤´**: {output.camera_angle} | ç„¦è· {output.focal_length_tendency}

**è‰²å½©**: {output.hue_family} | é¥±å’Œåº¦ {output.saturation_band} | äº®åº¦ {output.brightness_band} | å¯¹æ¯”åº¦ {output.contrast}

**è‰²æ¸©**: {output.cct_mean:.0f}K

**å‰ªè¾‘**: {output.cuts} æ¬¡å‰ªè¾‘ | å¹³å‡é•œå¤´ {output.avg_shot_length:.2f}s | {output.transition_type}

**åœºæ™¯ (CLIP)**:
{chr(10).join([f"  â€¢ {s.get('label', '?')}: {s.get('probability', 0):.1%}" for s in output.scene_categories[:3]])}
"""


def format_audio(output: AudioOutput) -> str:
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    instruments = output.instruments.get('detected_instruments', [])
    return f"""### ğŸµ éŸ³é¢‘åˆ†æç»“æœ (CLAP)

**èŠ‚å¥**: BPM {output.tempo_bpm:.1f} | èŠ‚æ‹ {output.num_beats} | æ‰“å‡»ä¹æ¯”ä¾‹ {output.percussive_ratio:.2f}

**BGM é£æ ¼**: {output.bgm_style} ({output.bgm_style_confidence:.1%})

**æƒ…ç»ª**: {output.mood} ({output.mood_confidence:.1%})

**è°ƒå¼**: {output.key_signature} | è¯­éŸ³æ¯”ä¾‹ {output.speech_ratio:.2f}

**ä¹å™¨**: {', '.join(instruments) if instruments else 'N/A'}
"""


def format_asr(output: ASROutput) -> str:
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    text_preview = output.text[:300] + '...' if len(output.text) > 300 else output.text
    emotion_str = ""
    if output.emotion:
        emotion_str = f"\n**æƒ…æ„Ÿ**: {output.emotion.get('dominant_emotion', 'N/A')} ({output.emotion.get('confidence', 0):.1%})"
    
    prosody_str = ""
    if output.prosody:
        prosody_str = f"\n**éŸµå¾‹**: éŸ³é«˜ {output.prosody.get('mean_pitch_hz', 0):.1f}Hz | {output.prosody.get('prosody_style', 'N/A')}"
    
    return f"""### ğŸ¤ è¯­éŸ³åˆ†æç»“æœ (Whisper + HuBERT)

**ç»Ÿè®¡**: {output.num_words} è¯ | {output.words_per_minute:.1f} wpm | {output.pace}

**å£å¤´ç¦…**: {', '.join([f'"{p}"' for p in output.catchphrases[:5]]) if output.catchphrases else 'N/A'}
{prosody_str}{emotion_str}

**è½¬å½•**:
```
{text_preview}
```
"""


def format_yolo(output: YOLOOutput) -> str:
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    detection = output.detection
    environment = output.environment
    object_counts = detection.get('object_counts', {})
    
    objects_str = "\n".join([f"  â€¢ {obj}: {cnt}Ã—" for obj, cnt in sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:10]])
    
    return f"""### ğŸ” ç›®æ ‡æ£€æµ‹ç»“æœ (YOLO11)

**ç¯å¢ƒ**: {environment.get('environment_type', 'N/A')} | {environment.get('cooking_style', 'N/A')}

**ç»Ÿè®¡**: {detection.get('unique_objects', 0)} ç§ç‰©ä½“ | {detection.get('total_detections', 0)} æ¬¡æ£€æµ‹

**æ£€æµ‹åˆ°çš„ç‰©ä½“**:
{objects_str}
"""


def format_consensus(output: ConsensusOutput) -> str:
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    cct_str = f"{output.cct:.0f}K" if output.cct else "N/A"
    shot_str = f"{output.avg_shot_length:.2f}s" if output.avg_shot_length else "N/A"
    bpm_str = f"{output.tempo_bpm:.1f}" if output.tempo_bpm else "N/A"
    
    return f"""### ğŸ¯ å…±è¯†åˆ†æç»“æœ

**é•œå¤´**: {output.camera_angle} | ç„¦è· {output.focal_length_tendency} | è¿åŠ¨ {output.camera_motion}

**è‰²å½©**: {output.hue_family} | é¥±å’Œåº¦ {output.saturation} | äº®åº¦ {output.brightness}

**è‰²æ¸©**: {cct_str}

**å‰ªè¾‘**: {output.transition_type} | å¹³å‡ {shot_str}

**éŸ³é¢‘**: {output.bgm_style} | æƒ…ç»ª {output.bgm_mood} | BPM {bpm_str}

**åœºæ™¯**: {output.scene_category}
"""


# =============================================================================
# å¤„ç†å‡½æ•°
# =============================================================================
def upload_video(video_file):
    """å¤„ç†è§†é¢‘ä¸Šä¼ """
    if video_file is None:
        return "è¯·ä¸Šä¼ è§†é¢‘", None, []
    
    STATE.reset()
    STATE.work_dir = Path(tempfile.mkdtemp(prefix="video_analysis_"))
    
    video_path = Path(video_file)
    STATE.video_path = STATE.work_dir / video_path.name
    
    import shutil
    shutil.copy(video_file, STATE.video_path)
    
    STATE.audio_path = extract_audio_from_video(STATE.video_path, STATE.work_dir)
    frame_paths = extract_frames_for_gallery(STATE.video_path, STATE.work_dir, num_frames=12)
    
    status = f"âœ… å·²ä¸Šä¼ : {video_path.name}\n"
    status += f"ğŸ“ å·¥ä½œç›®å½•: {STATE.work_dir}\n"
    status += f"ğŸ–¼ï¸ æå– {len(frame_paths)} å¸§\n"
    status += "ğŸµ éŸ³é¢‘å·²æå–" if STATE.audio_path else "âš ï¸ éŸ³é¢‘æå–å¤±è´¥"
    
    audio_path = str(STATE.audio_path) if STATE.audio_path else None
    return status, audio_path, frame_paths


def run_visual(progress=gr.Progress()):
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘", None
    
    progress(0.1, desc="â³ åŠ è½½ CLIP...")
    step = VisualAnalysisStep()
    input_data = VideoInput(video_path=STATE.video_path, work_dir=STATE.work_dir, frame_mode="edge")
    
    progress(0.4, desc="ğŸ”„ è§†è§‰åˆ†æä¸­...")
    STATE.visual_output = step.run(input_data)
    
    progress(1.0, desc="âœ… å®Œæˆ")
    contact = STATE.visual_output.contact_sheet if STATE.visual_output else None
    return format_visual(STATE.visual_output), contact


def run_audio(progress=gr.Progress()):
    if STATE.audio_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘"
    
    progress(0.1, desc="â³ åŠ è½½ CLAP...")
    step = AudioAnalysisStep()
    input_data = AudioInput(audio_path=STATE.audio_path)
    
    progress(0.4, desc="ğŸ”„ éŸ³é¢‘åˆ†æä¸­...")
    STATE.audio_output = step.run(input_data)
    
    progress(1.0, desc="âœ… å®Œæˆ")
    return format_audio(STATE.audio_output)


def run_asr(language: str, progress=gr.Progress()):
    if STATE.audio_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘"
    
    progress(0.1, desc="â³ åŠ è½½ Whisper...")
    step = ASRAnalysisStep()
    input_data = ASRInput(audio_path=STATE.audio_path, language=language,
                          model_size="large-v3-turbo", enable_prosody=True, enable_emotion=True)
    
    progress(0.4, desc="ğŸ”„ è¯­éŸ³è¯†åˆ«ä¸­...")
    STATE.asr_output = step.run(input_data)
    
    progress(1.0, desc="âœ… å®Œæˆ")
    return format_asr(STATE.asr_output)


def run_yolo(progress=gr.Progress()):
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘"
    
    progress(0.1, desc="â³ åŠ è½½ YOLO11...")
    step = YOLOAnalysisStep()
    input_data = YOLOInput(video_path=STATE.video_path, target_frames=36,
                           enable_colors=True, enable_materials=True)
    
    progress(0.4, desc="ğŸ”„ ç›®æ ‡æ£€æµ‹ä¸­...")
    STATE.yolo_output = step.run(input_data)
    
    progress(1.0, desc="âœ… å®Œæˆ")
    return format_yolo(STATE.yolo_output)


def run_consensus():
    """è¿è¡Œå…±è¯†åˆ†æ - éœ€è¦å…ˆè¿è¡Œå…¶ä»–åˆ†æ"""
    if STATE.visual_output is None and STATE.audio_output is None:
        return "âŒ è¯·å…ˆè¿è¡Œè§†è§‰æˆ–éŸ³é¢‘åˆ†æ"
    
    metrics = VideoMetrics(path=str(STATE.video_path) if STATE.video_path else "")
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    step = ConsensusStep()
    input_data = ConsensusInput(video_metrics=[metrics])
    STATE.consensus_output = step.run(input_data)
    
    return format_consensus(STATE.consensus_output)


def run_all(language: str, progress=gr.Progress()):
    """ä¸€é”®åˆ†æå…¨éƒ¨"""
    progress(0.1, desc="ğŸ“¹ è§†è§‰åˆ†æ...")
    visual_result, contact = run_visual()
    
    progress(0.3, desc="ğŸµ éŸ³é¢‘åˆ†æ...")
    audio_result = run_audio()
    
    progress(0.5, desc="ğŸ¤ è¯­éŸ³åˆ†æ...")
    asr_result = run_asr(language)
    
    progress(0.7, desc="ğŸ” ç›®æ ‡æ£€æµ‹...")
    yolo_result = run_yolo()
    
    progress(0.9, desc="ğŸ¯ å…±è¯†è®¡ç®—...")
    consensus_result = run_consensus()
    
    progress(1.0, desc="âœ… å…¨éƒ¨å®Œæˆ")
    
    # ç”Ÿæˆæ‘˜è¦
    summary = "=== åˆ†ææ‘˜è¦ ===\n\n"
    if STATE.visual_output:
        summary += f"ğŸ“¹ é•œå¤´: {STATE.visual_output.camera_angle}\n"
        summary += f"ğŸ¨ è‰²å½©: {STATE.visual_output.hue_family}\n"
        summary += f"âœ‚ï¸ å‰ªè¾‘: {STATE.visual_output.cuts} æ¬¡\n"
    if STATE.audio_output:
        summary += f"ğŸµ BPM: {STATE.audio_output.tempo_bpm:.1f}\n"
        summary += f"ğŸ¸ BGM: {STATE.audio_output.bgm_style}\n"
    if STATE.asr_output:
        summary += f"ğŸ¤ è¯­é€Ÿ: {STATE.asr_output.words_per_minute:.1f} wpm\n"
    if STATE.yolo_output:
        summary += f"ğŸ” ç‰©ä½“: {STATE.yolo_output.detection.get('unique_objects', 0)} ç§\n"
    
    return visual_result, contact, audio_result, asr_result, yolo_result, consensus_result, summary


def gen_report(progress=gr.Progress()):
    """ç”ŸæˆæŠ¥å‘Š"""
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘å¹¶è¿è¡Œåˆ†æ", None, None
    
    if STATE.visual_output is None and STATE.audio_output is None:
        return "âŒ è¯·å…ˆè¿è¡Œåˆ†æ", None, None
    
    progress(0.2, desc="ğŸ“„ ç”Ÿæˆ Word æŠ¥å‘Š...")
    
    metrics = VideoMetrics(path=str(STATE.video_path))
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    if STATE.consensus_output is None:
        run_consensus()
    
    metrics_dict = metrics.to_dict()
    consensus_dict = STATE.consensus_output.to_dict() if STATE.consensus_output else {}
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = STATE.work_dir / f"report_{timestamp}.docx"
    
    generate_word_report(
        video_metrics=[metrics_dict],
        consensus=consensus_dict,
        output_path=str(report_path),
        show_screenshots=True
    )
    
    STATE.report_path = str(report_path)
    
    progress(0.7, desc="ğŸ“• è½¬æ¢ PDF...")
    STATE.pdf_path = convert_docx_to_pdf(STATE.report_path)
    
    progress(1.0, desc="âœ… å®Œæˆ")
    
    status = f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ\nğŸ“„ {report_path.name}"
    if STATE.pdf_path:
        status += f"\nğŸ“• {Path(STATE.pdf_path).name}"
    else:
        status += "\nâš ï¸ PDF è½¬æ¢éœ€è¦ libreoffice"
    
    # è¿”å›æ–‡ä»¶è·¯å¾„ä¾›ä¸‹è½½
    return status, STATE.report_path, STATE.pdf_path


def export_json():
    """å¯¼å‡º JSON"""
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆè¿è¡Œåˆ†æ", None
    
    data = {
        "timestamp": datetime.now().isoformat(),
        "video_path": str(STATE.video_path),
        "visual": STATE.visual_output.to_dict() if STATE.visual_output else None,
        "audio": STATE.audio_output.to_dict() if STATE.audio_output else None,
        "asr": STATE.asr_output.to_dict() if STATE.asr_output else None,
        "yolo": STATE.yolo_output.to_dict() if STATE.yolo_output else None,
        "consensus": STATE.consensus_output.to_dict() if STATE.consensus_output else None,
    }
    
    json_path = STATE.work_dir / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False, default=str)
    
    return f"âœ… å·²å¯¼å‡º: {json_path.name}", str(json_path)


# =============================================================================
# Gradio ç•Œé¢
# =============================================================================
def create_ui():
    with gr.Blocks(
        title="Video Style Analysis",
        theme=gr.themes.Soft(primary_hue="blue", secondary_hue="slate")
    ) as demo:
        
        gr.Markdown("""
# ğŸ¬ è§†é¢‘é£æ ¼åˆ†æç³»ç»Ÿ
**SOTA 2025/2026** | CLIP (åœºæ™¯) | CLAP (éŸ³é¢‘) | HuBERT (æƒ…æ„Ÿ) | Whisper (ASR) | YOLO11 (æ£€æµ‹)
        """)
        
        with gr.Row():
            # ========== å·¦ä¾§: ä¸Šä¼ å’Œè®¾ç½® ==========
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ ä¸Šä¼ è§†é¢‘")
                video_input = gr.Video(label="é€‰æ‹©è§†é¢‘", height=250)
                upload_status = gr.Textbox(label="çŠ¶æ€", lines=4, interactive=False)
                
                gr.Markdown("### âš™ï¸ è®¾ç½®")
                language_select = gr.Dropdown(
                    choices=[("English", "en"), ("ä¸­æ–‡", "zh"), ("æ—¥æœ¬èª", "ja"), ("Auto", "auto")],
                    value="en",
                    label="ASR è¯­è¨€"
                )
                
                gr.Markdown("### ğŸµ éŸ³é¢‘")
                audio_player = gr.Audio(label="æå–çš„éŸ³é¢‘", type="filepath")
                
                gr.Markdown("### ğŸ–¼ï¸ å…³é”®å¸§")
                frame_gallery = gr.Gallery(label="å…³é”®å¸§", columns=3, height=200, object_fit="contain")
            
            # ========== ä¸­é—´: åˆ†æç»“æœ ==========
            with gr.Column(scale=2):
                gr.Markdown("### ğŸš€ åˆ†ææ§åˆ¶")
                with gr.Row():
                    run_all_btn = gr.Button("ğŸ¯ ä¸€é”®åˆ†æå…¨éƒ¨", variant="primary", size="lg")
                
                with gr.Row():
                    run_visual_btn = gr.Button("ğŸ“¹ è§†è§‰")
                    run_audio_btn = gr.Button("ğŸµ éŸ³é¢‘")
                    run_asr_btn = gr.Button("ğŸ¤ è¯­éŸ³")
                    run_yolo_btn = gr.Button("ğŸ” æ£€æµ‹")
                    run_consensus_btn = gr.Button("ğŸ¯ å…±è¯†")
                
                with gr.Tabs():
                    with gr.Tab("ğŸ“¹ è§†è§‰"):
                        visual_result = gr.Markdown("*è¯·å…ˆä¸Šä¼ è§†é¢‘*")
                        contact_img = gr.Image(label="Contact Sheet", height=200)
                    
                    with gr.Tab("ğŸµ éŸ³é¢‘"):
                        audio_result = gr.Markdown("*è¯·å…ˆä¸Šä¼ è§†é¢‘*")
                    
                    with gr.Tab("ğŸ¤ è¯­éŸ³"):
                        asr_result = gr.Markdown("*è¯·å…ˆä¸Šä¼ è§†é¢‘*")
                    
                    with gr.Tab("ğŸ” æ£€æµ‹"):
                        yolo_result = gr.Markdown("*è¯·å…ˆä¸Šä¼ è§†é¢‘*")
                    
                    with gr.Tab("ğŸ¯ å…±è¯†"):
                        consensus_result = gr.Markdown("*è¯·å…ˆè¿è¡Œåˆ†æ*")
            
            # ========== å³ä¾§: æŠ¥å‘Šå’Œå¯¼å‡º ==========
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š æŠ¥å‘Šä¸å¯¼å‡º")
                
                with gr.Row():
                    gen_report_btn = gr.Button("ğŸ“„ ç”ŸæˆæŠ¥å‘Š", variant="secondary")
                    export_json_btn = gr.Button("ğŸ’¾ å¯¼å‡º JSON")
                
                report_status = gr.Textbox(label="æŠ¥å‘ŠçŠ¶æ€", lines=3, interactive=False)
                
                gr.Markdown("### ğŸ“¥ ä¸‹è½½")
                report_file = gr.File(label="Word æŠ¥å‘Š (.docx)")
                pdf_file = gr.File(label="PDF æŠ¥å‘Š (.pdf)")
                json_file = gr.File(label="JSON æ•°æ®")
                
                json_status = gr.Textbox(label="JSON çŠ¶æ€", lines=2, interactive=False)
                
                gr.Markdown("### ğŸ“‹ æ‘˜è¦")
                summary_box = gr.Textbox(label="åˆ†ææ‘˜è¦", lines=12, interactive=False)
        
        gr.Markdown("---\n**Video Style Analysis** | SOTA 2025/2026")
        
        # ========== äº‹ä»¶ç»‘å®š ==========
        video_input.change(fn=upload_video, inputs=[video_input],
                          outputs=[upload_status, audio_player, frame_gallery])
        
        run_visual_btn.click(fn=run_visual, outputs=[visual_result, contact_img])
        run_audio_btn.click(fn=run_audio, outputs=[audio_result])
        run_asr_btn.click(fn=run_asr, inputs=[language_select], outputs=[asr_result])
        run_yolo_btn.click(fn=run_yolo, outputs=[yolo_result])
        run_consensus_btn.click(fn=run_consensus, outputs=[consensus_result])
        
        run_all_btn.click(fn=run_all, inputs=[language_select],
                         outputs=[visual_result, contact_img, audio_result, asr_result, 
                                  yolo_result, consensus_result, summary_box])
        
        gen_report_btn.click(fn=gen_report, outputs=[report_status, report_file, pdf_file])
        export_json_btn.click(fn=export_json, outputs=[json_status, json_file])
    
    return demo


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Video Style Analysis Web UI")
    parser.add_argument("--port", type=int, default=8088, help="Server port")
    parser.add_argument("--share", action="store_true", help="Create public link")
    args = parser.parse_args()
    
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=args.share,
        show_error=True
    )
