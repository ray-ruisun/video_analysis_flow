#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Video Style Analysis - Gradio Web Interface

åŠŸèƒ½:
- ä¸Šä¼ è§†é¢‘æ–‡ä»¶
- é¢„è§ˆ/æ’­æ”¾è§†é¢‘å’ŒéŸ³é¢‘
- ä¸€é”®å¤„ç†æˆ–åˆ†æ­¥å¤„ç†
- å®æ—¶æ˜¾ç¤ºæ¯ä¸€æ­¥åˆ†æç»“æœ
- ç”Ÿæˆ PDF æŠ¥å‘Š (åœ¨çº¿é¢„è§ˆå’Œä¸‹è½½)
- æ¯å¸§åˆ†æç»“æœå¯è§†åŒ–

æŠ€æœ¯æ ˆ:
- CLIP (åœºæ™¯åˆ†ç±»)
- CLAP (éŸ³é¢‘åˆ†ç±»)
- HuBERT (è¯­éŸ³æƒ…æ„Ÿ)
- Whisper large-v3-turbo (ASR)
- YOLO11 (ç›®æ ‡æ£€æµ‹)
"""

import sys
import os
import json
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, Dict, Any, List

import gradio as gr
import numpy as np
import cv2

# å°† src ç›®å½•åŠ å…¥è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from steps import (
    VisualAnalysisStep,
    AudioAnalysisStep,
    ASRAnalysisStep,
    YOLOAnalysisStep,
    ConsensusStep,
    ReportGenerationStep,
    VideoInput,
    AudioInput,
    ASRInput,
    YOLOInput,
    ConsensusInput,
    ReportInput,
    VideoMetrics,
    VisualOutput,
    AudioOutput,
    ASROutput,
    YOLOOutput,
    ConsensusOutput,
)
from report_word import generate_word_report

# ============================================================================
# å…¨å±€çŠ¶æ€
# ============================================================================
class AnalysisState:
    """ç®¡ç†åˆ†æçŠ¶æ€"""
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.video_path: Optional[Path] = None
        self.audio_path: Optional[Path] = None
        self.work_dir: Optional[Path] = None
        self.visual_output: Optional[VisualOutput] = None
        self.audio_output: Optional[AudioOutput] = None
        self.asr_output: Optional[ASROutput] = None
        self.yolo_output: Optional[YOLOOutput] = None
        self.consensus_output: Optional[ConsensusOutput] = None
        self.report_path: Optional[str] = None
        self.pdf_path: Optional[str] = None

STATE = AnalysisState()

# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================
def extract_audio_from_video(video_path: Path, output_dir: Path) -> Optional[Path]:
    """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
    output_path = output_dir / f"{video_path.stem}_audio.wav"
    
    if output_path.exists():
        return output_path
    
    try:
        cmd = [
            "ffmpeg", "-y", "-i", str(video_path),
            "-vn", "-acodec", "pcm_s16le",
            "-ar", "22050", "-ac", "1",
            str(output_path)
        ]
        subprocess.run(cmd, capture_output=True, check=True)
        return output_path
    except Exception as e:
        print(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
        return None


def extract_frames_for_gallery(video_path: Path, output_dir: Path, num_frames: int = 12) -> List[str]:
    """æå–å…³é”®å¸§ç”¨äºç”»å»Šå±•ç¤º"""
    frames_dir = output_dir / "frames"
    frames_dir.mkdir(exist_ok=True)
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    if total_frames == 0:
        return []
    
    # å‡åŒ€é‡‡æ ·å¸§
    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    
    frame_paths = []
    for i, idx in enumerate(frame_indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            timestamp = idx / fps if fps > 0 else 0
            frame_path = frames_dir / f"frame_{i:03d}_{timestamp:.1f}s.jpg"
            cv2.imwrite(str(frame_path), frame)
            frame_paths.append(str(frame_path))
    
    cap.release()
    return frame_paths


def get_frame_info(frame_idx: int, visual_output: Optional[VisualOutput]) -> str:
    """è·å–å•å¸§çš„åˆ†æä¿¡æ¯"""
    if not visual_output:
        return "æœªåˆ†æ"
    
    # ä» per_frame_analysis è·å–ä¿¡æ¯ (å¦‚æœæœ‰)
    per_frame = getattr(visual_output, 'per_frame_analysis', None)
    if per_frame and frame_idx < len(per_frame):
        frame_data = per_frame[frame_idx]
        return f"äº®åº¦: {frame_data.get('brightness', 'N/A')} | è‰²è°ƒ: {frame_data.get('hue', 'N/A')}"
    
    return f"å¸§ {frame_idx + 1}"


def convert_docx_to_pdf(docx_path: str) -> Optional[str]:
    """å°† DOCX è½¬æ¢ä¸º PDF"""
    pdf_path = docx_path.replace('.docx', '.pdf')
    
    try:
        # å°è¯•ä½¿ç”¨ libreoffice
        cmd = [
            "libreoffice", "--headless", "--convert-to", "pdf",
            "--outdir", str(Path(docx_path).parent),
            docx_path
        ]
        subprocess.run(cmd, capture_output=True, check=True, timeout=60)
        if Path(pdf_path).exists():
            return pdf_path
    except:
        pass
    
    try:
        # å°è¯•ä½¿ç”¨ docx2pdf (Windows/Mac)
        from docx2pdf import convert
        convert(docx_path, pdf_path)
        return pdf_path
    except:
        pass
    
    return None


def format_distribution(detail: Dict) -> str:
    """æ ¼å¼åŒ–åˆ†å¸ƒä¿¡æ¯"""
    if not detail or 'distribution' not in detail:
        return "N/A"
    
    lines = []
    for item in detail.get('distribution', [])[:5]:
        value = item.get('value', 'Unknown')
        count = item.get('count', 0)
        pct = item.get('percentage', 0)
        lines.append(f"  â€¢ {value}: {count}æ¬¡ ({pct}%)")
    
    return "\n".join(lines) if lines else "N/A"


def format_visual_output(output: VisualOutput) -> str:
    """æ ¼å¼åŒ–è§†è§‰åˆ†æç»“æœ"""
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    lines = [
        "# ğŸ“¹ è§†è§‰åˆ†æç»“æœ\n",
        f"**æ—¶é•¿**: {output.duration:.2f}s | **FPS**: {output.fps:.1f} | **é‡‡æ ·å¸§æ•°**: {output.sampled_frames}\n",
        
        "## ğŸ“· é•œå¤´åˆ†æ",
        f"**é•œå¤´è§’åº¦**: {output.camera_angle}",
        format_distribution(output.camera_angle_detail),
        f"\n**ç„¦è·å€¾å‘**: {output.focal_length_tendency}",
        
        "\n## ğŸ¨ è‰²å½©åˆ†æ",
        f"**è‰²è°ƒ**: {output.hue_family}",
        format_distribution(output.hue_detail),
        f"\n**é¥±å’Œåº¦**: {output.saturation_band}",
        format_distribution(output.saturation_detail),
        f"\n**äº®åº¦**: {output.brightness_band}",
        format_distribution(output.brightness_detail),
        f"\n**å¯¹æ¯”åº¦**: {output.contrast}",
        f"\n**è‰²æ¸©**: {output.cct_mean:.0f}K" if output.cct_mean else "",
        
        "\n## âœ‚ï¸ å‰ªè¾‘åˆ†æ",
        f"**å‰ªè¾‘æ•°**: {output.cuts}",
        f"**å¹³å‡é•œå¤´æ—¶é•¿**: {output.avg_shot_length:.2f}s",
        f"**è½¬åœºç±»å‹**: {output.transition_type}",
        
        "\n## ğŸ  åœºæ™¯åˆ†ç±» (CLIP)",
    ]
    
    for scene in output.scene_categories[:3]:
        label = scene.get('label', 'Unknown')
        prob = scene.get('probability', 0)
        lines.append(f"  â€¢ {label}: {prob:.1%}")
    
    return "\n".join(lines)


def format_audio_output(output: AudioOutput) -> str:
    """æ ¼å¼åŒ–éŸ³é¢‘åˆ†æç»“æœ"""
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    lines = [
        "# ğŸµ éŸ³é¢‘åˆ†æç»“æœ (CLAP)\n",
        
        "## èŠ‚å¥åˆ†æ",
        f"**BPM**: {output.tempo_bpm:.1f}",
        f"**èŠ‚æ‹æ•°**: {output.num_beats}",
        f"**æ‰“å‡»ä¹æ¯”ä¾‹**: {output.percussive_ratio:.2f}",
        
        "\n## ğŸ¸ BGM é£æ ¼",
        f"**ä¸»è¦é£æ ¼**: {output.bgm_style} ({output.bgm_style_confidence:.1%})",
    ]
    
    if output.bgm_style_detail and output.bgm_style_detail.get('top_3'):
        lines.append("**Top 3 é£æ ¼:**")
        for item in output.bgm_style_detail['top_3'][:3]:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                lines.append(f"  â€¢ {item[0]}: {item[1]:.1%}")
    
    lines.extend([
        "\n## ğŸ˜Š æƒ…ç»ªåˆ†æ",
        f"**ä¸»è¦æƒ…ç»ª**: {output.mood} ({output.mood_confidence:.1%})",
    ])
    
    if output.mood_detail and output.mood_detail.get('top_3'):
        lines.append("**Top 3 æƒ…ç»ª:**")
        for item in output.mood_detail['top_3'][:3]:
            if isinstance(item, (list, tuple)) and len(item) >= 2:
                lines.append(f"  â€¢ {item[0]}: {item[1]:.1%}")
    
    lines.extend([
        f"\n## ğŸ¹ å…¶ä»–",
        f"**è°ƒå¼**: {output.key_signature}",
        f"**è¯­éŸ³æ¯”ä¾‹**: {output.speech_ratio:.2f}",
    ])
    
    instruments = output.instruments.get('detected_instruments', [])
    if instruments:
        lines.append(f"**æ£€æµ‹åˆ°çš„ä¹å™¨**: {', '.join(instruments)}")
    
    return "\n".join(lines)


def format_asr_output(output: ASROutput) -> str:
    """æ ¼å¼åŒ– ASR åˆ†æç»“æœ"""
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    lines = [
        "# ğŸ¤ è¯­éŸ³åˆ†æç»“æœ (Whisper + HuBERT)\n",
        
        "## ğŸ“ è½¬å½•ç»Ÿè®¡",
        f"**è¯æ•°**: {output.num_words}",
        f"**è¯­é€Ÿ**: {output.words_per_second:.2f} w/s ({output.words_per_minute:.1f} wpm)",
        f"**èŠ‚å¥**: {output.pace}",
        f"**åœé¡¿æ•°**: {output.num_pauses}",
        f"**åœé¡¿é£æ ¼**: {output.pause_style}",
    ]
    
    if output.catchphrases:
        lines.append("\n## ğŸ” å£å¤´ç¦… (é«˜é¢‘çŸ­è¯­)")
        for phrase in output.catchphrases[:10]:
            lines.append(f"  â€¢ \"{phrase}\"")
    
    if output.prosody:
        lines.extend([
            "\n## ğŸ¼ éŸµå¾‹åˆ†æ",
            f"**å¹³å‡éŸ³é«˜**: {output.prosody.get('mean_pitch_hz', 0):.1f} Hz",
            f"**éŸ³é«˜å˜åŒ–**: {output.prosody.get('pitch_std', 0):.1f}",
            f"**éŸ³è°ƒ**: {output.prosody.get('tone', 'N/A')}",
            f"**éŸµå¾‹é£æ ¼**: {output.prosody.get('prosody_style', 'N/A')}",
        ])
    
    if output.emotion:
        lines.extend([
            "\n## ğŸ˜Š è¯­éŸ³æƒ…æ„Ÿ (HuBERT)",
            f"**ä¸»è¦æƒ…æ„Ÿ**: {output.emotion.get('dominant_emotion', 'N/A')} ({output.emotion.get('confidence', 0):.1%})",
        ])
        emotion_scores = output.emotion.get('emotion_scores', {})
        if emotion_scores:
            lines.append("**æƒ…æ„Ÿåˆ†å¸ƒ:**")
            for emo, score in list(emotion_scores.items())[:5]:
                lines.append(f"  â€¢ {emo}: {score:.1%}")
    
    if output.text:
        lines.extend([
            "\n## ğŸ“œ è½¬å½•æ–‡æœ¬",
            f"```\n{output.text[:500]}{'...' if len(output.text) > 500 else ''}\n```"
        ])
    
    return "\n".join(lines)


def format_yolo_output(output: YOLOOutput) -> str:
    """æ ¼å¼åŒ– YOLO åˆ†æç»“æœ"""
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    detection = output.detection
    environment = output.environment
    
    lines = [
        "# ğŸ” ç›®æ ‡æ£€æµ‹ç»“æœ (YOLO11)\n",
        
        "## ğŸ  ç¯å¢ƒåˆ†æ",
        f"**ç¯å¢ƒç±»å‹**: {environment.get('environment_type', 'N/A')}",
        f"**çƒ¹é¥ªé£æ ¼**: {environment.get('cooking_style', 'N/A')}",
        f"**è®¾å¤‡æ¡£æ¬¡**: {environment.get('appliance_tier', 'N/A')}",
        
        "\n## ğŸ“Š æ£€æµ‹ç»Ÿè®¡",
        f"**æ£€æµ‹ç‰©ä½“ç±»æ•°**: {detection.get('unique_objects', 0)}",
        f"**æ€»æ£€æµ‹æ¬¡æ•°**: {detection.get('total_detections', 0)}",
        f"**å¤„ç†å¸§æ•°**: {detection.get('frames_processed', 0)}",
        
        "\n## ğŸ¯ æ£€æµ‹åˆ°çš„ç‰©ä½“",
    ]
    
    object_counts = detection.get('object_counts', {})
    avg_conf = detection.get('avg_confidence', {})
    for obj, count in sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:15]:
        conf = avg_conf.get(obj, 0)
        lines.append(f"  â€¢ **{obj}**: {count}æ¬¡ (ç½®ä¿¡åº¦: {conf:.1%})")
    
    # é¢œè‰²åˆ†æ
    colors = output.colors
    if colors and colors.get('detailed_analysis'):
        lines.append("\n## ğŸ¨ ç‰©ä½“é¢œè‰²")
        for obj, analysis in list(colors['detailed_analysis'].items())[:5]:
            dominant = analysis.get('dominant', 'Unknown')
            lines.append(f"  â€¢ **{obj}**: {dominant}")
    
    # æè´¨åˆ†æ
    materials = output.materials
    if materials and materials.get('detailed_analysis'):
        lines.append("\n## ğŸ§± ç‰©ä½“æè´¨")
        for obj, analysis in list(materials['detailed_analysis'].items())[:5]:
            dominant = analysis.get('dominant', 'Unknown')
            lines.append(f"  â€¢ **{obj}**: {dominant}")
    
    return "\n".join(lines)


def format_consensus_output(output: ConsensusOutput) -> str:
    """æ ¼å¼åŒ–å…±è¯†åˆ†æç»“æœ"""
    if not output or not output.success:
        return "âŒ åˆ†æå¤±è´¥"
    
    lines = [
        "# ğŸ¯ è·¨è§†é¢‘å…±è¯†åˆ†æ\n",
        
        "## ğŸ“· é•œå¤´å…±è¯†",
        f"**é•œå¤´è§’åº¦**: {output.camera_angle}",
        format_distribution(output.camera_angle_detail),
        f"\n**ç„¦è·å€¾å‘**: {output.focal_length_tendency}",
        f"**ç›¸æœºè¿åŠ¨**: {output.camera_motion}",
        
        "\n## ğŸ¨ è‰²å½©å…±è¯†",
        f"**è‰²è°ƒ**: {output.hue_family}",
        format_distribution(output.hue_detail),
        f"\n**é¥±å’Œåº¦**: {output.saturation}",
        f"**äº®åº¦**: {output.brightness}",
        f"**å¯¹æ¯”åº¦**: {output.contrast}",
        f"**è‰²æ¸©**: {output.cct:.0f}K" if output.cct else "",
        
        "\n## âœ‚ï¸ å‰ªè¾‘å…±è¯†",
        f"**å‰ªè¾‘/åˆ†é’Ÿ**: {output.cuts_per_minute:.2f}" if output.cuts_per_minute else "",
        f"**å¹³å‡é•œå¤´æ—¶é•¿**: {output.avg_shot_length:.2f}s" if output.avg_shot_length else "",
        f"**è½¬åœºç±»å‹**: {output.transition_type}",
        
        "\n## ğŸµ éŸ³é¢‘å…±è¯†",
        f"**BGM é£æ ¼**: {output.bgm_style}",
        format_distribution(output.bgm_style_detail),
        f"\n**BGM æƒ…ç»ª**: {output.bgm_mood}",
        f"**èŠ‚æ‹å¯¹é½**: {output.beat_alignment:.2f}" if output.beat_alignment else "",
        f"**BPM**: {output.tempo_bpm:.1f}" if output.tempo_bpm else "",
        
        "\n## ğŸ  åœºæ™¯å…±è¯†",
        f"**åœºæ™¯ç±»å‹**: {output.scene_category}",
        format_distribution(output.scene_category_detail),
    ]
    
    if output.yolo_available:
        lines.extend([
            "\n## ğŸ” YOLO å…±è¯†",
            f"**ç¯å¢ƒ**: {output.yolo_environment}",
            f"**é£æ ¼**: {output.yolo_style}",
        ])
    
    return "\n".join(lines)


# ============================================================================
# å¤„ç†å‡½æ•°
# ============================================================================
def upload_video(video_file) -> Tuple[str, str, str, str, List[str]]:
    """å¤„ç†è§†é¢‘ä¸Šä¼ """
    if video_file is None:
        return None, None, "è¯·ä¸Šä¼ è§†é¢‘æ–‡ä»¶", "", []
    
    STATE.reset()
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    STATE.work_dir = Path(tempfile.mkdtemp(prefix="video_analysis_"))
    
    # å¤åˆ¶è§†é¢‘åˆ°å·¥ä½œç›®å½•
    video_path = Path(video_file)
    STATE.video_path = STATE.work_dir / video_path.name
    
    import shutil
    shutil.copy(video_file, STATE.video_path)
    
    # æå–éŸ³é¢‘
    STATE.audio_path = extract_audio_from_video(STATE.video_path, STATE.work_dir)
    
    # æå–å…³é”®å¸§ç”¨äºç”»å»Š
    frame_paths = extract_frames_for_gallery(STATE.video_path, STATE.work_dir, num_frames=12)
    
    status = f"âœ… è§†é¢‘å·²ä¸Šä¼ : {video_path.name}\n"
    status += f"ğŸ“ å·¥ä½œç›®å½•: {STATE.work_dir}\n"
    status += f"ğŸ–¼ï¸ æå–äº† {len(frame_paths)} ä¸ªå…³é”®å¸§\n"
    
    if STATE.audio_path:
        status += f"ğŸµ éŸ³é¢‘å·²æå–: {STATE.audio_path.name}"
    else:
        status += "âš ï¸ éŸ³é¢‘æå–å¤±è´¥"
    
    return str(STATE.video_path), str(STATE.audio_path) if STATE.audio_path else None, status, "", frame_paths


def run_visual_analysis(progress=gr.Progress()) -> Tuple[str, str]:
    """è¿è¡Œè§†è§‰åˆ†æ"""
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘", None
    
    progress(0.1, desc="åˆå§‹åŒ–è§†è§‰åˆ†æ...")
    
    try:
        step = VisualAnalysisStep()
        input_data = VideoInput(
            video_path=STATE.video_path,
            work_dir=STATE.work_dir,
            frame_mode="edge"
        )
        
        progress(0.3, desc="åˆ†æä¸­...")
        STATE.visual_output = step.run(input_data)
        
        progress(1.0, desc="å®Œæˆ!")
        
        result = format_visual_output(STATE.visual_output)
        contact_sheet = STATE.visual_output.contact_sheet if STATE.visual_output else None
        
        return result, contact_sheet
        
    except Exception as e:
        return f"âŒ è§†è§‰åˆ†æå¤±è´¥: {str(e)}", None


def run_audio_analysis(progress=gr.Progress()) -> str:
    """è¿è¡ŒéŸ³é¢‘åˆ†æ"""
    if STATE.audio_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘å¹¶æå–éŸ³é¢‘"
    
    progress(0.1, desc="åˆå§‹åŒ–éŸ³é¢‘åˆ†æ...")
    
    try:
        step = AudioAnalysisStep()
        input_data = AudioInput(audio_path=STATE.audio_path)
        
        progress(0.3, desc="CLAP åˆ†æä¸­...")
        STATE.audio_output = step.run(input_data)
        
        progress(1.0, desc="å®Œæˆ!")
        
        return format_audio_output(STATE.audio_output)
        
    except Exception as e:
        return f"âŒ éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}"


def run_asr_analysis(language: str, progress=gr.Progress()) -> str:
    """è¿è¡Œ ASR åˆ†æ"""
    if STATE.audio_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘å¹¶æå–éŸ³é¢‘"
    
    progress(0.1, desc="åˆå§‹åŒ– ASR...")
    
    try:
        step = ASRAnalysisStep()
        input_data = ASRInput(
            audio_path=STATE.audio_path,
            language=language,
            model_size="large-v3-turbo",
            enable_prosody=True,
            enable_emotion=True
        )
        
        progress(0.3, desc="Whisper è½¬å½•ä¸­...")
        STATE.asr_output = step.run(input_data)
        
        progress(1.0, desc="å®Œæˆ!")
        
        return format_asr_output(STATE.asr_output)
        
    except Exception as e:
        return f"âŒ ASR åˆ†æå¤±è´¥: {str(e)}"


def run_yolo_analysis(progress=gr.Progress()) -> str:
    """è¿è¡Œ YOLO åˆ†æ"""
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆä¸Šä¼ è§†é¢‘"
    
    progress(0.1, desc="åˆå§‹åŒ– YOLO11...")
    
    try:
        step = YOLOAnalysisStep()
        input_data = YOLOInput(
            video_path=STATE.video_path,
            target_frames=36,
            enable_colors=True,
            enable_materials=True
        )
        
        progress(0.3, desc="ç›®æ ‡æ£€æµ‹ä¸­...")
        STATE.yolo_output = step.run(input_data)
        
        progress(1.0, desc="å®Œæˆ!")
        
        return format_yolo_output(STATE.yolo_output)
        
    except Exception as e:
        return f"âŒ YOLO åˆ†æå¤±è´¥: {str(e)}"


def run_all_analysis(language: str, progress=gr.Progress()) -> Tuple[str, str, str, str, str]:
    """ä¸€é”®è¿è¡Œæ‰€æœ‰åˆ†æ"""
    results = []
    contact_sheet = None
    
    # è§†è§‰åˆ†æ
    progress(0.1, desc="è§†è§‰åˆ†æ...")
    visual_result, contact_sheet = run_visual_analysis()
    results.append(visual_result)
    
    # éŸ³é¢‘åˆ†æ
    progress(0.3, desc="éŸ³é¢‘åˆ†æ...")
    audio_result = run_audio_analysis()
    results.append(audio_result)
    
    # ASR åˆ†æ
    progress(0.5, desc="ASR åˆ†æ...")
    asr_result = run_asr_analysis(language)
    results.append(asr_result)
    
    # YOLO åˆ†æ
    progress(0.7, desc="YOLO åˆ†æ...")
    yolo_result = run_yolo_analysis()
    results.append(yolo_result)
    
    # å…±è¯†åˆ†æ
    progress(0.9, desc="ç”Ÿæˆå…±è¯†...")
    consensus_result = run_consensus_analysis()
    results.append(consensus_result)
    
    progress(1.0, desc="å®Œæˆ!")
    
    return results[0], contact_sheet, results[1], results[2], results[3]


def run_consensus_analysis() -> str:
    """è¿è¡Œå…±è¯†åˆ†æ"""
    metrics = VideoMetrics(path=str(STATE.video_path) if STATE.video_path else "")
    metrics.visual = STATE.visual_output
    metrics.audio = STATE.audio_output
    metrics.asr = STATE.asr_output
    metrics.yolo = STATE.yolo_output
    
    try:
        step = ConsensusStep()
        input_data = ConsensusInput(video_metrics=[metrics])
        STATE.consensus_output = step.run(input_data)
        
        return format_consensus_output(STATE.consensus_output)
        
    except Exception as e:
        return f"âŒ å…±è¯†åˆ†æå¤±è´¥: {str(e)}"


def generate_report(progress=gr.Progress()) -> Tuple[str, str, str]:
    """ç”ŸæˆæŠ¥å‘Š"""
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆè¿è¡Œåˆ†æ", None, None
    
    progress(0.2, desc="ç”Ÿæˆ Word æŠ¥å‘Š...")
    
    try:
        # å‡†å¤‡æ•°æ®
        metrics = VideoMetrics(path=str(STATE.video_path))
        metrics.visual = STATE.visual_output
        metrics.audio = STATE.audio_output
        metrics.asr = STATE.asr_output
        metrics.yolo = STATE.yolo_output
        
        if STATE.consensus_output is None:
            run_consensus_analysis()
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        metrics_dict = metrics.to_dict()
        if STATE.visual_output:
            metrics_dict["visual"]["available"] = True
        if STATE.audio_output:
            metrics_dict["audio"]["available"] = True
        if STATE.asr_output:
            metrics_dict["asr"]["available"] = True
        if STATE.yolo_output:
            metrics_dict["yolo"]["available"] = True
        
        consensus_dict = STATE.consensus_output.to_dict() if STATE.consensus_output else {}
        
        # ç”ŸæˆæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = STATE.work_dir / f"report_{timestamp}.docx"
        
        generate_word_report(
            video_metrics=[metrics_dict],
            consensus=consensus_dict,
            output_path=str(report_path),
            show_screenshots=True
        )
        
        STATE.report_path = str(report_path)
        
        progress(0.6, desc="è½¬æ¢ä¸º PDF...")
        
        # å°è¯•è½¬æ¢ä¸º PDF
        STATE.pdf_path = convert_docx_to_pdf(STATE.report_path)
        
        progress(1.0, desc="å®Œæˆ!")
        
        status = f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ\n"
        status += f"ğŸ“„ Word: {report_path.name}\n"
        if STATE.pdf_path:
            status += f"ğŸ“• PDF: {Path(STATE.pdf_path).name}"
        else:
            status += "âš ï¸ PDF è½¬æ¢å¤±è´¥ (éœ€è¦ libreoffice)"
        
        return status, STATE.report_path, STATE.pdf_path
        
    except Exception as e:
        return f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}", None, None


def export_json() -> Tuple[str, str]:
    """å¯¼å‡º JSON æ•°æ®"""
    if STATE.video_path is None:
        return "âŒ è¯·å…ˆè¿è¡Œåˆ†æ", None
    
    try:
        data = {
            "timestamp": datetime.now().isoformat(),
            "video_path": str(STATE.video_path),
            "visual": STATE.visual_output.to_dict() if STATE.visual_output else None,
            "audio": STATE.audio_output.to_dict() if STATE.audio_output else None,
            "asr": STATE.asr_output.to_dict() if STATE.asr_output else None,
            "yolo": STATE.yolo_output.to_dict() if STATE.yolo_output else None,
            "consensus": STATE.consensus_output.to_dict() if STATE.consensus_output else None,
        }
        
        json_path = STATE.work_dir / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        
        return f"âœ… JSON å·²å¯¼å‡º: {json_path.name}", str(json_path)
        
    except Exception as e:
        return f"âŒ JSON å¯¼å‡ºå¤±è´¥: {str(e)}", None


# ============================================================================
# Gradio ç•Œé¢
# ============================================================================
def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="Video Style Analysis",
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="slate",
        ),
        css="""
        .container { max-width: 1600px; margin: auto; }
        .header { text-align: center; padding: 20px; }
        .result-box { min-height: 300px; }
        .step-btn { min-width: 150px; }
        .frame-gallery { max-height: 400px; overflow-y: auto; }
        .status-box { font-family: monospace; }
        """
    ) as demo:
        
        # æ ‡é¢˜
        gr.Markdown("""
        # ğŸ¬ Video Style Analysis
        ### SOTA è§†é¢‘é£æ ¼åˆ†æç³»ç»Ÿ | PyTorch + HuggingFace
        
        **æŠ€æœ¯æ ˆ**: 
        ğŸ–¼ï¸ CLIP ViT-L/14 (åœºæ™¯) | 
        ğŸµ CLAP (éŸ³é¢‘) | 
        ğŸ˜Š HuBERT-large (æƒ…æ„Ÿ) | 
        ğŸ¤ Whisper large-v3-turbo (ASR) | 
        ğŸ” YOLO11 (æ£€æµ‹)
        """)
        
        with gr.Row():
            # ==================== å·¦ä¾§: ä¸Šä¼ å’Œé¢„è§ˆ ====================
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“¤ ä¸Šä¼ è§†é¢‘")
                
                video_input = gr.Video(
                    label="ä¸Šä¼ è§†é¢‘æ–‡ä»¶ (æ”¯æŒ mp4, avi, mov, mkv)",
                    height=280
                )
                
                upload_status = gr.Textbox(
                    label="ä¸Šä¼ çŠ¶æ€",
                    lines=4,
                    interactive=False,
                    elem_classes="status-box"
                )
                
                gr.Markdown("## ğŸµ éŸ³é¢‘é¢„è§ˆ")
                audio_player = gr.Audio(
                    label="æå–çš„éŸ³é¢‘ (è‡ªåŠ¨ä»è§†é¢‘åˆ†ç¦»)",
                    type="filepath"
                )
                
                gr.Markdown("## âš™ï¸ åˆ†æè®¾ç½®")
                with gr.Row():
                    language_select = gr.Dropdown(
                        choices=[
                            ("English", "en"),
                            ("ä¸­æ–‡", "zh"),
                            ("æ—¥æœ¬èª", "ja"),
                            ("í•œêµ­ì–´", "ko"),
                            ("è‡ªåŠ¨æ£€æµ‹", "auto")
                        ],
                        value="en",
                        label="ASR è¯­è¨€"
                    )
                
                # å¸§ç”»å»Š
                gr.Markdown("## ğŸ–¼ï¸ å…³é”®å¸§é¢„è§ˆ")
                frame_gallery = gr.Gallery(
                    label="è§†é¢‘å…³é”®å¸§ (å‡åŒ€é‡‡æ ·)",
                    columns=4,
                    rows=3,
                    height=300,
                    object_fit="contain",
                    elem_classes="frame-gallery"
                )
            
            # ==================== ä¸­é—´: æ§åˆ¶å’Œç»“æœ ====================
            with gr.Column(scale=2):
                gr.Markdown("## ğŸš€ åˆ†ææ§åˆ¶")
                
                with gr.Row():
                    run_all_btn = gr.Button(
                        "ğŸ¯ ä¸€é”®åˆ†æå…¨éƒ¨", 
                        variant="primary", 
                        size="lg",
                        scale=2
                    )
                    generate_report_btn = gr.Button(
                        "ğŸ“„ ç”ŸæˆæŠ¥å‘Š", 
                        variant="secondary", 
                        size="lg"
                    )
                    export_json_btn = gr.Button(
                        "ğŸ’¾ å¯¼å‡º JSON", 
                        size="lg"
                    )
                
                gr.Markdown("### ğŸ”§ åˆ†æ­¥æ‰§è¡Œ (å¯å•ç‹¬è¿è¡Œæ¯ä¸ªæ¨¡å—)")
                with gr.Row():
                    run_visual_btn = gr.Button("ğŸ“¹ è§†è§‰åˆ†æ", elem_classes="step-btn")
                    run_audio_btn = gr.Button("ğŸµ éŸ³é¢‘åˆ†æ", elem_classes="step-btn")
                    run_asr_btn = gr.Button("ğŸ¤ ASR åˆ†æ", elem_classes="step-btn")
                    run_yolo_btn = gr.Button("ğŸ” YOLO åˆ†æ", elem_classes="step-btn")
                    run_consensus_btn = gr.Button("ğŸ¯ å…±è¯†è®¡ç®—", elem_classes="step-btn")
                
                # è¿›åº¦æ¡
                progress_text = gr.Textbox(
                    label="å½“å‰è¿›åº¦",
                    value="ç­‰å¾…å¼€å§‹...",
                    interactive=False,
                    lines=1
                )
                
                # ç»“æœé€‰é¡¹å¡
                with gr.Tabs() as result_tabs:
                    with gr.TabItem("ğŸ“¹ è§†è§‰åˆ†æ", id="visual"):
                        visual_result = gr.Markdown(
                            value="*ä¸Šä¼ è§†é¢‘åç‚¹å‡»ã€Œè§†è§‰åˆ†æã€æˆ–ã€Œä¸€é”®åˆ†æã€å¼€å§‹*",
                            elem_classes="result-box"
                        )
                        contact_sheet_img = gr.Image(
                            label="Contact Sheet (å…³é”®å¸§æ‹¼æ¥)",
                            height=200
                        )
                    
                    with gr.TabItem("ğŸµ éŸ³é¢‘åˆ†æ", id="audio"):
                        audio_result = gr.Markdown(
                            value="*ä¸Šä¼ è§†é¢‘åç‚¹å‡»ã€ŒéŸ³é¢‘åˆ†æã€æˆ–ã€Œä¸€é”®åˆ†æã€å¼€å§‹*",
                            elem_classes="result-box"
                        )
                    
                    with gr.TabItem("ğŸ¤ ASR åˆ†æ", id="asr"):
                        asr_result = gr.Markdown(
                            value="*ä¸Šä¼ è§†é¢‘åç‚¹å‡»ã€ŒASR åˆ†æã€æˆ–ã€Œä¸€é”®åˆ†æã€å¼€å§‹*",
                            elem_classes="result-box"
                        )
                    
                    with gr.TabItem("ğŸ” YOLO åˆ†æ", id="yolo"):
                        yolo_result = gr.Markdown(
                            value="*ä¸Šä¼ è§†é¢‘åç‚¹å‡»ã€ŒYOLO åˆ†æã€æˆ–ã€Œä¸€é”®åˆ†æã€å¼€å§‹*",
                            elem_classes="result-box"
                        )
                    
                    with gr.TabItem("ğŸ¯ å…±è¯†åˆ†æ", id="consensus"):
                        consensus_result = gr.Markdown(
                            value="*è¿è¡Œå®Œæ‰€æœ‰åˆ†æåè‡ªåŠ¨ç”Ÿæˆ*",
                            elem_classes="result-box"
                        )
            
            # ==================== å³ä¾§: æŠ¥å‘Šå’Œå¯¼å‡º ====================
            with gr.Column(scale=1):
                gr.Markdown("## ğŸ“Š æŠ¥å‘Šç”Ÿæˆ")
                
                report_status = gr.Textbox(
                    label="æŠ¥å‘ŠçŠ¶æ€",
                    lines=5,
                    interactive=False,
                    elem_classes="status-box"
                )
                
                gr.Markdown("### ğŸ“¥ ä¸‹è½½")
                
                report_download = gr.File(
                    label="ğŸ“„ Word æŠ¥å‘Š (.docx)"
                )
                
                pdf_download = gr.File(
                    label="ğŸ“• PDF æŠ¥å‘Š (.pdf)"
                )
                
                gr.Markdown("---")
                
                json_status = gr.Textbox(
                    label="JSON å¯¼å‡ºçŠ¶æ€",
                    lines=2,
                    interactive=False
                )
                
                json_download = gr.File(
                    label="ğŸ’¾ JSON æ•°æ®"
                )
                
                # åˆ†ææ‘˜è¦
                gr.Markdown("## ğŸ“‹ åˆ†ææ‘˜è¦")
                summary_box = gr.Textbox(
                    label="å¿«é€Ÿé¢„è§ˆ",
                    lines=10,
                    interactive=False,
                    placeholder="åˆ†æå®Œæˆåæ˜¾ç¤ºæ‘˜è¦..."
                )
        
        # ==================== äº‹ä»¶ç»‘å®š ====================
        
        # ä¸Šä¼ è§†é¢‘
        video_input.change(
            fn=upload_video,
            inputs=[video_input],
            outputs=[video_input, audio_player, upload_status, visual_result, frame_gallery]
        )
        
        # åˆ†æ­¥æ‰§è¡Œ
        run_visual_btn.click(
            fn=run_visual_analysis,
            outputs=[visual_result, contact_sheet_img]
        )
        
        run_audio_btn.click(
            fn=run_audio_analysis,
            outputs=[audio_result]
        )
        
        run_asr_btn.click(
            fn=run_asr_analysis,
            inputs=[language_select],
            outputs=[asr_result]
        )
        
        run_yolo_btn.click(
            fn=run_yolo_analysis,
            outputs=[yolo_result]
        )
        
        run_consensus_btn.click(
            fn=run_consensus_analysis,
            outputs=[consensus_result]
        )
        
        # ä¸€é”®åˆ†æ
        def run_all_with_summary(language, progress=gr.Progress()):
            """ä¸€é”®åˆ†æå¹¶ç”Ÿæˆæ‘˜è¦"""
            results = list(run_all_analysis(language, progress))
            consensus = run_consensus_analysis()
            
            # ç”Ÿæˆæ‘˜è¦
            summary_lines = ["=== åˆ†ææ‘˜è¦ ===\n"]
            if STATE.visual_output:
                summary_lines.append(f"ğŸ“¹ é•œå¤´è§’åº¦: {STATE.visual_output.camera_angle}")
                summary_lines.append(f"ğŸ¨ è‰²è°ƒ: {STATE.visual_output.hue_family}")
                summary_lines.append(f"âœ‚ï¸ å‰ªè¾‘æ•°: {STATE.visual_output.cuts}")
            if STATE.audio_output:
                summary_lines.append(f"ğŸµ BPM: {STATE.audio_output.tempo_bpm:.1f}")
                summary_lines.append(f"ğŸ¸ BGM é£æ ¼: {STATE.audio_output.bgm_style}")
            if STATE.asr_output:
                summary_lines.append(f"ğŸ¤ è¯­é€Ÿ: {STATE.asr_output.words_per_minute:.1f} wpm")
            if STATE.yolo_output:
                obj_count = STATE.yolo_output.detection.get('unique_objects', 0)
                summary_lines.append(f"ğŸ” æ£€æµ‹ç‰©ä½“: {obj_count} ç§")
            
            summary = "\n".join(summary_lines)
            return results + [consensus, summary]
        
        run_all_btn.click(
            fn=run_all_with_summary,
            inputs=[language_select],
            outputs=[visual_result, contact_sheet_img, audio_result, asr_result, yolo_result, consensus_result, summary_box]
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_report_btn.click(
            fn=generate_report,
            outputs=[report_status, report_download, pdf_download]
        )
        
        # å¯¼å‡º JSON
        export_json_btn.click(
            fn=export_json,
            outputs=[json_status, json_download]
        )
        
        # é¡µè„š
        gr.Markdown("""
        ---
        **Video Style Analysis** | SOTA 2025/2026 | 
        [GitHub](https://github.com/your-repo) | 
        Models: CLIP, CLAP, HuBERT, Whisper, YOLO11
        """)
    
    return demo


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
if __name__ == "__main__":
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
