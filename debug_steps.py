#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å—åŒ–æµæ°´çº¿è°ƒè¯•è„šæœ¬

ç”¨äºé€æ­¥æ‰§è¡Œå’Œè°ƒè¯•æ¯ä¸ªåˆ†ææ¨¡å—ã€‚
æ¯ä¸ªæ­¥éª¤æ‰§è¡Œå®Œæ¯•åä¼šæš‚åœï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤åç»§ç»­ã€‚

================================================================================
å‘½ä»¤è¡Œä½¿ç”¨æ–¹æ³•
================================================================================

# å®Œæ•´æµç¨‹ (è‡ªåŠ¨ä»è§†é¢‘æå–éŸ³é¢‘)
python debug_steps.py -v video1.mp4 video2.mp4 video3.mp4 -o report.docx

# ä»…æ‰§è¡ŒæŸäº›æ¨¡å—
python debug_steps.py -v video1.mp4 --modules visual,yolo -o report.docx

# å•ä¸ªè§†é¢‘è°ƒè¯•
python debug_steps.py -v video1.mp4 --modules visual -o report.docx

# ä¸æš‚åœï¼Œè¿ç»­æ‰§è¡Œ
python debug_steps.py -v video1.mp4 video2.mp4 video3.mp4 --no-pause -o report.docx

================================================================================
å•ç‹¬ä½¿ç”¨æ¯ä¸ªæ¨¡å— (Python API)
================================================================================

# 0. å‡†å¤‡å·¥ä½œ - ä»è§†é¢‘æå–éŸ³é¢‘
from debug_steps import extract_audio_from_video
audio_path = extract_audio_from_video(Path("video.mp4"), Path("work"))
# -> work/video_audio.wav

# 1. è§†è§‰åˆ†ææ¨¡å— (VisualAnalysisStep)
# è¾“å…¥: VideoInput (video_path, work_dir, frame_mode)
# è¾“å‡º: VisualOutput (camera_angle, hue_family, cuts, duration, etc.)
from steps import VisualAnalysisStep, VideoInput
step = VisualAnalysisStep()
input_data = VideoInput(video_path=Path("video.mp4"), work_dir=Path("work"))
output = step.run(input_data)
print(f"é•œå¤´è§’åº¦: {output.camera_angle}")
print(f"è‰²è°ƒ: {output.hue_family}")
print(f"å‰ªè¾‘æ•°: {output.cuts}")

# 2. éŸ³é¢‘åˆ†ææ¨¡å— (AudioAnalysisStep)
# è¾“å…¥: AudioInput (audio_path)
# è¾“å‡º: AudioOutput (tempo_bpm, bgm_style, mood, key_signature, etc.)
from steps import AudioAnalysisStep, AudioInput
step = AudioAnalysisStep()
input_data = AudioInput(audio_path=Path("audio.wav"))  # æˆ–ä»è§†é¢‘æå–çš„éŸ³é¢‘
output = step.run(input_data)
print(f"BPM: {output.tempo_bpm}")
print(f"BGMé£æ ¼: {output.bgm_style}")
print(f"æƒ…ç»ª: {output.mood}")

# 3. ASR è¯­éŸ³è¯†åˆ«æ¨¡å— (ASRAnalysisStep)
# è¾“å…¥: ASRInput (audio_path, language, model_size, enable_prosody, enable_emotion)
# è¾“å‡º: ASROutput (text, words_per_minute, pace, catchphrases, prosody, emotion)
from steps import ASRAnalysisStep, ASRInput
step = ASRAnalysisStep()
input_data = ASRInput(
    audio_path=Path("audio.wav"),
    language="en",            # è¯­è¨€: en, zh, ja, etc.
    model_size="small",       # æ¨¡å‹: tiny, base, small, medium, large
    enable_prosody=True,      # éŸµå¾‹åˆ†æ
    enable_emotion=True       # æƒ…æ„Ÿåˆ†æ
)
output = step.run(input_data)
print(f"è½¬å½•: {output.text[:100]}...")
print(f"è¯­é€Ÿ: {output.words_per_minute:.1f} wpm")
print(f"å£å¤´ç¦…: {output.catchphrases}")

# 4. YOLO ç›®æ ‡æ£€æµ‹æ¨¡å— (YOLOAnalysisStep)
# è¾“å…¥: YOLOInput (video_path, target_frames, enable_colors, enable_materials)
# è¾“å‡º: YOLOOutput (detection, environment, colors, materials)
from steps import YOLOAnalysisStep, YOLOInput
step = YOLOAnalysisStep()
input_data = YOLOInput(
    video_path=Path("video.mp4"),
    target_frames=36,         # é‡‡æ ·å¸§æ•°
    enable_colors=True,       # é¢œè‰²åˆ†æ
    enable_materials=True     # æè´¨åˆ†æ
)
output = step.run(input_data)
print(f"ç¯å¢ƒç±»å‹: {output.environment.get('environment_type')}")
print(f"æ£€æµ‹ç‰©ä½“: {output.detection.get('unique_objects')} ç±»")

# 5. å…±è¯†è®¡ç®—æ¨¡å— (ConsensusStep)
# è¾“å…¥: ConsensusInput (video_metrics: List[VideoMetrics])
# è¾“å‡º: ConsensusOutput (camera_angle, hue_family, bgm_style, etc. çš„å¤šæ•°ç¥¨/ä¸­ä½æ•°)
from steps import ConsensusStep, ConsensusInput, VideoMetrics
step = ConsensusStep()
input_data = ConsensusInput(video_metrics=[vm1, vm2, vm3])
output = step.run(input_data)
print(f"å…±è¯†é•œå¤´: {output.camera_angle}")
print(f"å…±è¯†BGM: {output.bgm_style}")

# 6. æŠ¥å‘Šç”Ÿæˆæ¨¡å— (ReportGenerationStep)
# è¾“å…¥: ReportInput (video_metrics, consensus, output_path, show_screenshots)
# è¾“å‡º: ReportOutput (report_path)
from steps import ReportGenerationStep, ReportInput
step = ReportGenerationStep()
input_data = ReportInput(
    video_metrics=[vm1, vm2, vm3],
    consensus=consensus_output,
    output_path="report.docx",
    show_screenshots=True
)
output = step.run(input_data)
print(f"æŠ¥å‘Š: {output.report_path}")

================================================================================
"""

import sys
import subprocess
import argparse
import json
import datetime
from pathlib import Path
from typing import List, Optional, Any

# å°† src ç›®å½•åŠ å…¥è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

# ============================================================================
# æ—¥å¿—æ–‡ä»¶é…ç½®
# ============================================================================
LOG_FILE = None
LOG_DATA = {"runs": [], "timestamp": None}

from steps import (
    # æ­¥éª¤ç±»
    VisualAnalysisStep,
    AudioAnalysisStep,
    ASRAnalysisStep,
    YOLOAnalysisStep,
    ConsensusStep,
    ReportGenerationStep,
    # è¾“å…¥ç±»å‹
    VideoInput,
    AudioInput,
    ASRInput,
    YOLOInput,
    ConsensusInput,
    ReportInput,
    # æ•°æ®ç±»å‹
    VideoMetrics,
    VisualOutput,
    AudioOutput,
    ASROutput,
    YOLOOutput,
    ConsensusOutput,
)
from utils import setup_logger

# åˆå§‹åŒ–æ—¥å¿—å™¨
logger = setup_logger()


def extract_audio_from_video(video_path: Path, work_dir: Path) -> Path:
    """
    ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘
    
    ä½¿ç”¨ ffmpeg å°†è§†é¢‘ä¸­çš„éŸ³é¢‘æå–ä¸º 22.05kHz mono wav æ–‡ä»¶ã€‚
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        work_dir: å·¥ä½œç›®å½• (éŸ³é¢‘æ–‡ä»¶å°†ä¿å­˜åœ¨è¿™é‡Œ)
        
    Returns:
        Path: æå–çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
    Raises:
        RuntimeError: å¦‚æœ ffmpeg ä¸å¯ç”¨æˆ–æå–å¤±è´¥
    """
    work_dir.mkdir(parents=True, exist_ok=True)
    
    # è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    audio_path = work_dir / f"{video_path.stem}_audio.wav"
    
    # å¦‚æœå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if audio_path.exists():
        logger.info(f"éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æå–: {audio_path}")
        return audio_path
    
    logger.info(f"ä»è§†é¢‘æå–éŸ³é¢‘: {video_path} -> {audio_path}")
    
    # ä½¿ç”¨ ffmpeg æå–éŸ³é¢‘
    # -y: è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
    # -i: è¾“å…¥æ–‡ä»¶
    # -vn: ä¸å¤„ç†è§†é¢‘
    # -acodec pcm_s16le: 16-bit PCM ç¼–ç 
    # -ar 22050: é‡‡æ ·ç‡ 22.05kHz
    # -ac 1: å•å£°é“
    cmd = [
        "ffmpeg", "-y",
        "-i", str(video_path),
        "-vn",
        "-acodec", "pcm_s16le",
        "-ar", "22050",
        "-ac", "1",
        str(audio_path)
    ]
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        logger.info(f"éŸ³é¢‘æå–æˆåŠŸ: {audio_path}")
        return audio_path
    except FileNotFoundError:
        logger.error("ffmpeg æœªå®‰è£…æˆ–ä¸åœ¨ PATH ä¸­")
        logger.error("è¯·å®‰è£… ffmpeg: https://ffmpeg.org/download.html")
        logger.error("  Ubuntu: sudo apt install ffmpeg")
        logger.error("  macOS: brew install ffmpeg")
        raise RuntimeError("ffmpeg not found")
    except subprocess.CalledProcessError as e:
        logger.error(f"ffmpeg æå–éŸ³é¢‘å¤±è´¥: {e.stderr}")
        raise RuntimeError(f"Audio extraction failed: {e.stderr}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ¨¡å—åŒ–æµæ°´çº¿è°ƒè¯•è„šæœ¬ - é€æ­¥æ‰§è¡Œæ¯ä¸ªåˆ†ææ¨¡å—",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å®Œæ•´è°ƒè¯• (è‡ªåŠ¨ä»è§†é¢‘æå–éŸ³é¢‘)
  python debug_steps.py -v video1.mp4 video2.mp4 video3.mp4 -o report.docx

  # ä»…è§†è§‰åˆ†æ
  python debug_steps.py -v video1.mp4 --modules visual -o report.docx

  # è§†è§‰ + YOLO (ä¸éœ€è¦éŸ³é¢‘)
  python debug_steps.py -v video1.mp4 video2.mp4 video3.mp4 --modules visual,yolo -o report.docx

  # è¿ç»­æ‰§è¡Œï¼Œä¸æš‚åœ
  python debug_steps.py -v video1.mp4 --no-pause -o report.docx
        """
    )
    
    parser.add_argument(
        "-v", "--videos",
        nargs="+",
        required=True,
        help="è§†é¢‘æ–‡ä»¶è·¯å¾„ (1-3ä¸ªï¼ŒéŸ³é¢‘å°†è‡ªåŠ¨ä»è§†é¢‘ä¸­æå–)"
    )
    
    parser.add_argument(
        "-o", "--output",
        default="debug_report.docx",
        help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„ (default: debug_report.docx)"
    )
    
    parser.add_argument(
        "--modules",
        default="visual,audio,asr,yolo",
        help="è¦æ‰§è¡Œçš„æ¨¡å—ï¼Œé€—å·åˆ†éš” (default: visual,audio,asr,yolo)"
    )
    
    parser.add_argument(
        "--work-dir",
        default="work",
        help="å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜æ”¾æå–çš„éŸ³é¢‘å’Œä¸­é—´æ–‡ä»¶ (default: work)"
    )
    
    parser.add_argument(
        "--no-pause",
        action="store_true",
        help="ä¸æš‚åœï¼Œè¿ç»­æ‰§è¡Œæ‰€æœ‰æ­¥éª¤"
    )
    
    parser.add_argument(
        "--skip-audio-extract",
        action="store_true",
        help="è·³è¿‡éŸ³é¢‘æå– (å¦‚æœå·¥ä½œç›®å½•å·²æœ‰éŸ³é¢‘æ–‡ä»¶)"
    )
    
    return parser.parse_args()


def pause(message: str = "æŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€æ­¥..."):
    """æš‚åœç­‰å¾…ç”¨æˆ·ç¡®è®¤"""
    try:
        input(f"\n{message}")
    except EOFError:
        pass


def init_log_file(work_dir: Path) -> Path:
    """åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶"""
    global LOG_FILE, LOG_DATA
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    LOG_FILE = work_dir / f"debug_output_{timestamp}.json"
    LOG_DATA = {
        "timestamp": timestamp,
        "start_time": datetime.datetime.now().isoformat(),
        "runs": []
    }
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    work_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")
    return LOG_FILE


def log_step_output(step_name: str, input_data: Any, output_data: Any):
    """è®°å½•æ­¥éª¤è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶"""
    global LOG_DATA, LOG_FILE
    
    if LOG_FILE is None:
        return
    
    import numpy as np
    
    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    def to_serializable(obj):
        # å¤„ç† numpy ç±»å‹
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        # å¤„ç† dataclass å’Œæ™®é€šå¯¹è±¡
        elif hasattr(obj, 'to_dict'):
            return to_serializable(obj.to_dict())
        elif hasattr(obj, '__dict__') and not isinstance(obj, type):
            return {k: to_serializable(v) for k, v in obj.__dict__.items() if not k.startswith('_')}
        elif isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, (list, tuple)):
            return [to_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        else:
            return str(obj)
    
    log_entry = {
        "step": step_name,
        "timestamp": datetime.datetime.now().isoformat(),
        "input": to_serializable(input_data),
        "output": to_serializable(output_data)
    }
    
    LOG_DATA["runs"].append(log_entry)
    
    # å®æ—¶å†™å…¥æ–‡ä»¶
    with open(LOG_FILE, 'w', encoding='utf-8') as f:
        json.dump(LOG_DATA, f, indent=2, ensure_ascii=False)


def print_separator(title: str):
    """æ‰“å°åˆ†éš”çº¿"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def print_dict_detailed(data: dict, indent: int = 2, max_list_items: int = 20):
    """é€’å½’æ‰“å°å­—å…¸çš„è¯¦ç»†å†…å®¹"""
    prefix = " " * indent
    
    for key, value in data.items():
        if isinstance(value, dict):
            print(f"{prefix}{key}:")
            print_dict_detailed(value, indent + 2)
        elif isinstance(value, list):
            if len(value) == 0:
                print(f"{prefix}{key}: []")
            elif len(value) <= max_list_items:
                if all(isinstance(item, (int, float, str)) for item in value):
                    # ç®€å•ç±»å‹åˆ—è¡¨ï¼Œæ˜¾ç¤ºæ‰€æœ‰
                    print(f"{prefix}{key}: {value}")
                else:
                    # å¤æ‚ç±»å‹åˆ—è¡¨ï¼Œé€ä¸ªæ˜¾ç¤º
                    print(f"{prefix}{key}: [{len(value)} items]")
                    for i, item in enumerate(value[:max_list_items]):
                        if isinstance(item, dict):
                            print(f"{prefix}  [{i}]:")
                            print_dict_detailed(item, indent + 4)
                        else:
                            print(f"{prefix}  [{i}]: {item}")
            else:
                print(f"{prefix}{key}: [{len(value)} items, showing first {max_list_items}]")
                for i, item in enumerate(value[:max_list_items]):
                    if isinstance(item, dict):
                        print(f"{prefix}  [{i}]:")
                        print_dict_detailed(item, indent + 4)
                    else:
                        print(f"{prefix}  [{i}]: {item}")
        elif isinstance(value, str) and len(str(value)) > 200:
            print(f"{prefix}{key}: {str(value)[:200]}... ({len(value)} chars)")
        else:
            print(f"{prefix}{key}: {value}")


def print_output_summary(name: str, output, show_full: bool = True):
    """æ‰“å°è¾“å‡ºæ‘˜è¦ (å®Œæ•´è¯¦ç»†ç‰ˆ)"""
    print(f"\n[{name}] å®Œæ•´è¾“å‡º:")
    print("-" * 60)
    
    if hasattr(output, 'to_dict'):
        data = output.to_dict()
        print_dict_detailed(data)
    elif isinstance(output, dict):
        print_dict_detailed(output)
    else:
        print(f"  {output}")
    
    print("-" * 60)


def run_visual_step(
    video_path: Path, 
    work_dir: Path, 
    should_pause: bool = True
) -> Optional[VisualOutput]:
    """
    æ‰§è¡Œè§†è§‰åˆ†ææ­¥éª¤
    
    è¾“å…¥: VideoInput (video_path, work_dir, frame_mode)
    è¾“å‡º: VisualOutput (camera_angle, hue_family, cuts, duration, etc.)
    """
    print_separator("Step: è§†è§‰åˆ†æ (VisualAnalysisStep)")
    print(f"è¾“å…¥: VideoInput(video_path={video_path})")
    
    step = VisualAnalysisStep()
    input_data = VideoInput(
        video_path=video_path,
        work_dir=work_dir,
        frame_mode="edge"
    )
    
    print(f"\næ‰§è¡Œä¸­...")
    output = step.run(input_data)
    
    # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    log_step_output("visual", {"video_path": str(video_path)}, output)
    
    print_output_summary("VisualOutput", output)
    
    # æ˜¾ç¤ºè¯¦ç»†åˆ†å¸ƒ
    print(f"\nğŸ“Š è¯¦ç»†åˆ†å¸ƒ:")
    print(f"\n  é•œå¤´è§’åº¦åˆ†å¸ƒ:")
    if hasattr(output, 'camera_angle_detail') and output.camera_angle_detail:
        for item in output.camera_angle_detail.get('distribution', []):
            print(f"    - {item['value']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    print(f"\n  è‰²è°ƒåˆ†å¸ƒ:")
    if hasattr(output, 'hue_detail') and output.hue_detail:
        for item in output.hue_detail.get('distribution', []):
            print(f"    - {item['value']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    print(f"\n  é¥±å’Œåº¦åˆ†å¸ƒ:")
    if hasattr(output, 'saturation_detail') and output.saturation_detail:
        for item in output.saturation_detail.get('distribution', []):
            print(f"    - {item['value']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    print(f"\n  äº®åº¦åˆ†å¸ƒ:")
    if hasattr(output, 'brightness_detail') and output.brightness_detail:
        for item in output.brightness_detail.get('distribution', []):
            print(f"    - {item['value']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    print(f"\n  å¯¹æ¯”åº¦åˆ†å¸ƒ:")
    if hasattr(output, 'contrast_detail') and output.contrast_detail:
        for item in output.contrast_detail.get('distribution', []):
            print(f"    - {item['value']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    print(f"\n  å…‰çº¿ç±»å‹:")
    if output.lighting and output.lighting.get('type_detail'):
        for item in output.lighting['type_detail'].get('distribution', []):
            print(f"    - {item['value']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    print(f"\nğŸ“ˆ å…³é”®æ•°å€¼:")
    print(f"  - æ€»æ—¶é•¿: {output.duration:.2f}s")
    print(f"  - é‡‡æ ·å¸§æ•°: {output.sampled_frames}")
    print(f"  - å‰ªè¾‘æ•°: {output.cuts}")
    print(f"  - å¹³å‡é•œå¤´æ—¶é•¿: {output.avg_shot_length:.2f}s")
    if output.cct_mean:
        print(f"  - è‰²æ¸©: {output.cct_mean:.0f}K (Â±{output.cct_std:.0f})")
    
    if should_pause:
        pause()
    
    return output


def run_audio_step(
    audio_path: Path, 
    should_pause: bool = True
) -> Optional[AudioOutput]:
    """
    æ‰§è¡ŒéŸ³é¢‘åˆ†ææ­¥éª¤
    
    è¾“å…¥: AudioInput (audio_path)
    è¾“å‡º: AudioOutput (tempo_bpm, bgm_style, mood, key_signature, etc.)
    """
    print_separator("Step: éŸ³é¢‘åˆ†æ (AudioAnalysisStep)")
    print(f"è¾“å…¥: AudioInput(audio_path={audio_path})")
    
    step = AudioAnalysisStep()
    input_data = AudioInput(audio_path=audio_path)
    
    print(f"\næ‰§è¡Œä¸­...")
    output = step.run(input_data)
    
    # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    log_step_output("audio", {"audio_path": str(audio_path)}, output)
    
    print_output_summary("AudioOutput", output)
    
    # æ˜¾ç¤ºè¯¦ç»†åˆ†ç±»ç»“æœ
    print(f"\nğŸ“Š CLAP åˆ†ç±»è¯¦æƒ…:")
    print(f"\n  BGM é£æ ¼:")
    print(f"    - ä¸»è¦é£æ ¼: {output.bgm_style}")
    if hasattr(output, 'bgm_style_detail') and output.bgm_style_detail:
        top3 = output.bgm_style_detail.get('top_3', [])
        if top3:
            print(f"    - Top 3 é£æ ¼:")
            for item in top3:
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    print(f"        {item[0]}: {item[1]:.1%}")
    
    print(f"\n  æƒ…ç»ªåˆ†æ:")
    print(f"    - ä¸»è¦æƒ…ç»ª: {output.mood}")
    if hasattr(output, 'mood_detail') and output.mood_detail:
        top3 = output.mood_detail.get('top_3', [])
        if top3:
            print(f"    - Top 3 æƒ…ç»ª:")
            for item in top3:
                if isinstance(item, (list, tuple)) and len(item) >= 2:
                    print(f"        {item[0]}: {item[1]:.1%}")
    
    print(f"\nğŸ“ˆ åŸºç¡€æŒ‡æ ‡:")
    print(f"  - BPM: {output.tempo_bpm:.1f}")
    print(f"  - èŠ‚æ‹æ•°: {output.num_beats}")
    print(f"  - æ‰“å‡»ä¹æ¯”ä¾‹: {output.percussive_ratio:.2f}")
    print(f"  - è°ƒå¼: {output.key_signature}")
    print(f"  - è¯­éŸ³æ¯”ä¾‹: {output.speech_ratio:.2f}")
    
    if should_pause:
        pause()
    
    return output


def run_asr_step(
    audio_path: Path, 
    should_pause: bool = True
) -> Optional[ASROutput]:
    """
    æ‰§è¡Œ ASR åˆ†ææ­¥éª¤
    
    è¾“å…¥: ASRInput (audio_path, language, model_size, enable_prosody, enable_emotion)
    è¾“å‡º: ASROutput (text, words_per_minute, pace, catchphrases, prosody, emotion)
    """
    print_separator("Step: ASR è¯­éŸ³è¯†åˆ« (ASRAnalysisStep)")
    print(f"è¾“å…¥: ASRInput(audio_path={audio_path}, language='en', model_size='large-v3-turbo')")
    
    step = ASRAnalysisStep()
    input_data = ASRInput(
        audio_path=audio_path,
        language="en",
        model_size="large-v3-turbo",  # ä½¿ç”¨æœ€æ–°æœ€å¼ºæ¨¡å‹
        enable_prosody=True,
        enable_emotion=True
    )
    
    print(f"\næ‰§è¡Œä¸­ (Whisper large-v3-turbo è½¬å½•ä¸­)...")
    output = step.run(input_data)
    
    # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    log_step_output("asr", {"audio_path": str(audio_path)}, output)
    
    print_output_summary("ASROutput", output)
    
    print(f"\nğŸ“Š ASR è¯¦æƒ…:")
    print(f"\n  è½¬å½•ç»Ÿè®¡:")
    print(f"    - è¯æ•°: {output.num_words}")
    print(f"    - è¯­é€Ÿ: {output.words_per_second:.2f} w/s ({output.words_per_minute:.1f} wpm)")
    print(f"    - èŠ‚å¥: {output.pace}")
    print(f"    - åœé¡¿æ•°: {output.num_pauses}")
    print(f"    - åœé¡¿é£æ ¼: {output.pause_style}")
    
    print(f"\n  å£å¤´ç¦… (é«˜é¢‘çŸ­è¯­):")
    if output.catchphrases:
        for phrase in output.catchphrases[:10]:
            print(f"    - {phrase}")
    else:
        print(f"    - æ— ")
    
    if output.prosody:
        print(f"\n  éŸµå¾‹åˆ†æ:")
        print(f"    - å¹³å‡éŸ³é«˜: {output.prosody.get('mean_pitch_hz', 0):.1f} Hz")
        print(f"    - éŸ³é«˜å˜åŒ–: {output.prosody.get('pitch_std', 0):.1f}")
        print(f"    - éŸ³è°ƒ: {output.prosody.get('tone', 'N/A')}")
        print(f"    - éŸµå¾‹é£æ ¼: {output.prosody.get('prosody_style', 'N/A')}")
    
    if output.emotion:
        print(f"\n  æƒ…æ„Ÿåˆ†æ (HuBERT):")
        print(f"    - ä¸»è¦æƒ…æ„Ÿ: {output.emotion.get('dominant_emotion', 'N/A')}")
        print(f"    - ç½®ä¿¡åº¦: {output.emotion.get('confidence', 0):.1%}")
        emotion_scores = output.emotion.get('emotion_scores', {})
        if emotion_scores:
            print(f"    - æƒ…æ„Ÿåˆ†å¸ƒ:")
            for emotion, score in list(emotion_scores.items())[:5]:
                print(f"        {emotion}: {score:.1%}")
    
    if output.text:
        print(f"\n  è½¬å½•æ–‡æœ¬ (å‰500å­—):")
        preview = output.text[:500] + "..." if len(output.text) > 500 else output.text
        print(f"    {preview}")
    
    if should_pause:
        pause()
    
    return output


def run_yolo_step(
    video_path: Path, 
    should_pause: bool = True
) -> Optional[YOLOOutput]:
    """
    æ‰§è¡Œ YOLO æ£€æµ‹æ­¥éª¤ (YOLO11)
    
    è¾“å…¥: YOLOInput (video_path, target_frames, enable_colors, enable_materials)
    è¾“å‡º: YOLOOutput (detection, environment, colors, materials)
    """
    print_separator("Step: YOLO11 ç›®æ ‡æ£€æµ‹ (YOLOAnalysisStep)")
    print(f"è¾“å…¥: YOLOInput(video_path={video_path}, target_frames=36, model=yolo11s.pt)")
    
    step = YOLOAnalysisStep()
    input_data = YOLOInput(
        video_path=video_path,
        target_frames=36,
        enable_colors=True,
        enable_materials=True
    )
    
    print(f"\næ‰§è¡Œä¸­ (YOLO11 æ£€æµ‹ä¸­)...")
    output = step.run(input_data)
    
    # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    log_step_output("yolo", {"video_path": str(video_path)}, output)
    
    print_output_summary("YOLOOutput", output)
    
    print(f"\nğŸ“Š YOLO11 æ£€æµ‹è¯¦æƒ…:")
    detection = output.detection
    environment = output.environment
    
    print(f"\n  ç¯å¢ƒåˆ†æ:")
    print(f"    - ç¯å¢ƒç±»å‹: {environment.get('environment_type', 'N/A')}")
    print(f"    - çƒ¹é¥ªé£æ ¼: {environment.get('cooking_style', 'N/A')}")
    print(f"    - è®¾å¤‡æ¡£æ¬¡: {environment.get('appliance_tier', 'N/A')}")
    
    print(f"\n  æ£€æµ‹ç»Ÿè®¡:")
    print(f"    - æ£€æµ‹ç‰©ä½“ç±»æ•°: {detection.get('unique_objects', 0)}")
    print(f"    - æ€»æ£€æµ‹æ¬¡æ•°: {detection.get('total_detections', 0)}")
    print(f"    - å¤„ç†å¸§æ•°: {detection.get('frames_processed', 0)}")
    
    print(f"\n  æ£€æµ‹åˆ°çš„ç‰©ä½“:")
    object_counts = detection.get('object_counts', {})
    for obj, count in sorted(object_counts.items(), key=lambda x: x[1], reverse=True):
        avg_conf = detection.get('avg_confidence', {}).get(obj, 0)
        print(f"    - {obj}: {count}æ¬¡ (ç½®ä¿¡åº¦: {avg_conf:.1%})")
    
    # é¢œè‰²åˆ†æ
    colors = output.colors
    if colors and colors.get('detailed_analysis'):
        print(f"\n  ç‰©ä½“é¢œè‰²åˆ†æ:")
        for obj, analysis in colors.get('detailed_analysis', {}).items():
            print(f"    {obj}:")
            for item in analysis.get('distribution', []):
                print(f"      - {item['color']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    # æè´¨åˆ†æ
    materials = output.materials
    if materials and materials.get('detailed_analysis'):
        print(f"\n  ç‰©ä½“æè´¨åˆ†æ:")
        for obj, analysis in materials.get('detailed_analysis', {}).items():
            print(f"    {obj}:")
            for item in analysis.get('distribution', []):
                print(f"      - {item['material']}: {item['count']}æ¬¡ ({item['percentage']}%)")
    
    if should_pause:
        pause()
    
    return output


def run_consensus_step(
    video_metrics: List[VideoMetrics], 
    should_pause: bool = True
) -> Optional[ConsensusOutput]:
    """
    æ‰§è¡Œå…±è¯†è®¡ç®—æ­¥éª¤
    
    è¾“å…¥: ConsensusInput (video_metrics: List[VideoMetrics])
    è¾“å‡º: ConsensusOutput (camera_angle, hue_family, bgm_style çš„å¤šæ•°ç¥¨/ä¸­ä½æ•°)
    """
    print_separator("Step: å…±è¯†è®¡ç®— (ConsensusStep)")
    print(f"è¾“å…¥: ConsensusInput(video_metrics=[{len(video_metrics)} ä¸ªè§†é¢‘])")
    
    step = ConsensusStep()
    input_data = ConsensusInput(video_metrics=video_metrics)
    
    print(f"\næ‰§è¡Œä¸­...")
    output = step.run(input_data)
    
    print_output_summary("ConsensusOutput", output)
    print(f"\nå…³é”®ç»“æœ:")
    print(f"  - å…±è¯†é•œå¤´è§’åº¦: {output.camera_angle}")
    print(f"  - å…±è¯†è‰²è°ƒ: {output.hue_family}")
    print(f"  - å…±è¯†åœºæ™¯: {output.scene_category}")
    print(f"  - å…±è¯†BGMé£æ ¼: {output.bgm_style}")
    print(f"  - å…±è¯†æƒ…ç»ª: {output.bgm_mood}")
    if output.cuts_per_minute:
        print(f"  - æ¯åˆ†é’Ÿå‰ªè¾‘æ•°: {output.cuts_per_minute:.2f}")
    
    if should_pause:
        pause()
    
    return output


def run_report_step(
    video_metrics: List[VideoMetrics],
    consensus: ConsensusOutput,
    output_path: str,
    should_pause: bool = True
):
    """
    æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆæ­¥éª¤
    
    è¾“å…¥: ReportInput (video_metrics, consensus, output_path, show_screenshots)
    è¾“å‡º: ReportOutput (report_path)
    """
    print_separator("Step: æŠ¥å‘Šç”Ÿæˆ (ReportGenerationStep)")
    print(f"è¾“å…¥: ReportInput(video_metrics=[{len(video_metrics)} ä¸ª], output_path={output_path})")
    
    step = ReportGenerationStep()
    input_data = ReportInput(
        video_metrics=video_metrics,
        consensus=consensus,
        output_path=output_path,
        show_screenshots=True
    )
    
    print(f"\næ‰§è¡Œä¸­...")
    output = step.run(input_data)
    
    print(f"\nå…³é”®ç»“æœ:")
    print(f"  - æŠ¥å‘Šå·²ä¿å­˜: {output.report_path}")
    
    if should_pause:
        pause()
    
    return output


def main():
    """ä¸»å‡½æ•° - é€æ­¥æ‰§è¡Œæ‰€æœ‰æ¨¡å—"""
    args = parse_args()
    
    # è§£æå‚æ•°
    video_paths = [Path(p) for p in args.videos]
    modules = [m.strip().lower() for m in args.modules.split(",")]
    work_dir = Path(args.work_dir)
    should_pause = not args.no_pause
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦éŸ³é¢‘
    needs_audio = any(m in modules for m in ("audio", "asr"))
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    work_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
    log_file = init_log_file(work_dir)
    
    print("\n" + "=" * 70)
    print("  æ¨¡å—åŒ–æµæ°´çº¿è°ƒè¯•è„šæœ¬ (SOTA 2025/2026)")
    print("=" * 70)
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"  - è§†é¢‘: {[str(p) for p in video_paths]}")
    print(f"  - æ¨¡å—: {modules}")
    print(f"  - è¾“å‡ºæŠ¥å‘Š: {args.output}")
    print(f"  - å·¥ä½œç›®å½•: {work_dir}")
    print(f"  - æ—¥å¿—æ–‡ä»¶: {log_file}")
    print(f"  - æš‚åœæ¨¡å¼: {'å¦' if args.no_pause else 'æ˜¯'}")
    print(f"  - éœ€è¦éŸ³é¢‘: {'æ˜¯' if needs_audio else 'å¦'}")
    print(f"\nğŸ”§ ä½¿ç”¨çš„æ¨¡å‹:")
    print(f"  - åœºæ™¯åˆ†ç±»: CLIP (openai/clip-vit-large-patch14)")
    print(f"  - éŸ³é¢‘åˆ†ç±»: CLAP (laion/larger_clap_music_and_speech)")
    print(f"  - è¯­éŸ³æƒ…æ„Ÿ: HuBERT (superb/hubert-large-superb-er)")
    print(f"  - ASR: Whisper large-v3-turbo")
    print(f"  - ç›®æ ‡æ£€æµ‹: YOLO11 (yolo11s.pt)")
    
    # æå–éŸ³é¢‘ (å¦‚æœéœ€è¦)
    audio_paths: List[Optional[Path]] = []
    if needs_audio:
        print_separator("é¢„å¤„ç†: ä»è§†é¢‘æå–éŸ³é¢‘")
        for video_path in video_paths:
            try:
                audio_path = extract_audio_from_video(video_path, work_dir)
                audio_paths.append(audio_path)
            except Exception as e:
                logger.error(f"æ— æ³•ä» {video_path} æå–éŸ³é¢‘: {e}")
                audio_paths.append(None)
    else:
        audio_paths = [None] * len(video_paths)
    
    if should_pause:
        pause("æŒ‰å›è½¦å¼€å§‹æ‰§è¡Œæ¨¡å—...")
    
    # å­˜å‚¨æ‰€æœ‰è§†é¢‘çš„åˆ†æç»“æœ
    all_video_metrics: List[VideoMetrics] = []
    
    # é€è§†é¢‘åˆ†æ
    for i, video_path in enumerate(video_paths):
        print_separator(f"å¤„ç†è§†é¢‘ {i+1}/{len(video_paths)}: {video_path.name}")
        
        audio_path = audio_paths[i]
        
        # åˆ›å»º VideoMetrics å®ä¾‹
        metrics = VideoMetrics(path=str(video_path))
        
        # æ‰§è¡Œå„æ¨¡å—
        if "visual" in modules:
            metrics.visual = run_visual_step(video_path, work_dir, should_pause)
        
        if "audio" in modules:
            if audio_path and audio_path.exists():
                metrics.audio = run_audio_step(audio_path, should_pause)
            else:
                print(f"\n[è·³è¿‡] éŸ³é¢‘åˆ†æ - éŸ³é¢‘æå–å¤±è´¥æˆ–ä¸å­˜åœ¨")
        
        if "asr" in modules:
            if audio_path and audio_path.exists():
                metrics.asr = run_asr_step(audio_path, should_pause)
            else:
                print(f"\n[è·³è¿‡] ASRåˆ†æ - éŸ³é¢‘æå–å¤±è´¥æˆ–ä¸å­˜åœ¨")
        
        if "yolo" in modules:
            metrics.yolo = run_yolo_step(video_path, should_pause)
        
        all_video_metrics.append(metrics)
        print(f"\nâœ“ è§†é¢‘ {video_path.name} åˆ†æå®Œæˆ")
    
    # å…±è¯†è®¡ç®—
    if len(all_video_metrics) > 0:
        consensus = run_consensus_step(all_video_metrics, should_pause)
    else:
        print("\n[è·³è¿‡] å…±è¯†è®¡ç®— - æ— åˆ†æç»“æœ")
        consensus = ConsensusOutput()
    
    # æŠ¥å‘Šç”Ÿæˆ
    run_report_step(all_video_metrics, consensus, args.output, should_pause)
    
    # å®Œæˆ
    print_separator("å…¨éƒ¨å®Œæˆ!")
    print(f"\næŠ¥å‘Šå·²ç”Ÿæˆ: {args.output}")
    print(f"åˆ†æäº† {len(all_video_metrics)} ä¸ªè§†é¢‘")
    print(f"æ‰§è¡Œçš„æ¨¡å—: {', '.join(modules)}")
    if needs_audio:
        print(f"æå–çš„éŸ³é¢‘æ–‡ä»¶ä¿å­˜åœ¨: {work_dir}/")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
